{
  "_investigation_summary": {
    "timestamp": "2025-09-18",
    "comparison_source": "docs/architecture/diagrams/pratikoai_rag.mmd",
    "total_blueprint_nodes": 135,
    "total_step_files": 154,
    "major_findings": {
      "architectural_mismatch": "Blueprint expects explicit decision nodes and orchestrator steps, but implementation uses black-box factory patterns that collapse multiple nodes into single method calls",
      "provider_selection_collapse": "Steps 48-58 (provider selection flow) are collapsed into LLMFactory.get_optimal_provider() with internal routing logic not exposed as separate orchestrator nodes",
      "logging_vs_functional": "Most ragsteps/* files are logging-only wrappers that return static payloads rather than implementing actual business logic",
      "hidden_orchestration": "Real orchestration logic is embedded in LangGraphAgent methods (_prepare_messages_with_system_prompt, _get_optimal_provider) rather than explicit step functions"
    },
    "critical_gaps": {
      "missing_decision_nodes": [
        "Steps 48-58: Provider selection decision tree implemented as factory method",
        "Steps 12, 31-38: Classification logic embedded in agent methods",
        "Steps 19-30: Document processing flow not explicitly orchestrated"
      ],
      "factory_vs_explicit": {
        "LLMFactory.get_optimal_provider": "Collapses steps 48-58 routing decisions",
        "LangGraphAgent._prepare_messages_with_system_prompt": "Handles steps 46-47 without explicit orchestration",
        "DomainActionClassifier.classify": "Handles steps 31-38 classification flow"
      }
    },
    "alignment_types": {
      "logging_only": "Steps that only log but delegate to existing methods (majority)",
      "missing_orchestrator": "Steps that need new orchestrator wrappers to expose factory logic",
      "needs_implementation": "Steps with no corresponding implementation",
      "wiring_needed": "Steps implemented but not connected to workflow"
    },
    "next_actions": {
      "extract_factory_logic": "Extract LLMFactory routing decisions into explicit orchestrator nodes",
      "create_decision_wrappers": "Create orchestrator wrappers that expose internal agent decision logic",
      "wire_explicit_flow": "Replace factory method calls with explicit step-by-step orchestration",
      "align_with_blueprint": "Ensure workflow matches the 135-node Mermaid decision tree exactly"
    }
  },
  "1": {
    "step": 1,
    "id": "RAG.platform.chatbotcontroller.chat.validate.request.and.authenticate",
    "node_id": "ValidateRequest",
    "node_label": "ChatbotController.chat Validate request and authenticate",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ValidCheck"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/api/v1/chat.py:36-85 - chat_completion() endpoint",
        "TARGET: app/orchestrators/platform.py - step_1__validate_request()",
        "ACTION: Extract validation logic from chat endpoint into explicit orchestrator",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 1: Extract request validation from ChatbotController",
      "# Current: Embedded in chat_completion() endpoint",
      "# Target: Explicit orchestrator for validation",
      "",
      "# 1. Examine current validation logic:",
      "cat app/api/v1/chat.py | grep -A 20 'def chat_completion'",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/platform_step1.py",
      "from app.observability.rag_logging import rag_step_log",
      "from fastapi import HTTPException",
      "",
      "def step_1__validate_request(request_data: dict, auth_token: str = None):",
      "    \"\"\"RAG STEP 1: Validate request and authenticate.\"\"\"",
      "    ",
      "    # Extract validation from chat.py:40-60",
      "    if not request_data.get('messages'):",
      "        raise HTTPException(status_code=400, detail='Messages required')",
      "    ",
      "    # TODO: Add authentication check if needed",
      "    ",
      "    rag_step_log(",
      "        step=1,",
      "        step_id=\"RAG.platform.chatbotcontroller.chat.validate.request.and.authenticate\",",
      "        action=\"validating_request\",",
      "        has_messages=bool(request_data.get('messages')),",
      "        authenticated=True",
      "    )",
      "    ",
      "    return {\"valid\": True, \"next_step\": \"GDPRLog\"}",
      "EOF"
    ],
    "transformation_notes": "Extract validation logic from chat endpoint into explicit orchestrator",
    "current_implementation": "app/api/v1/chat.py:36-85 - chat_completion() endpoint",
    "target_implementation": "app/orchestrators/platform.py - step_1__validate_request()",
    "test_requirements": [
      "Valid request passes validation",
      "Missing messages returns 400",
      "Authentication check if implemented"
    ]
  },
  "2": {
    "step": 2,
    "id": "RAG.platform.user.submits.query.via.post.api.v1.chat",
    "node_id": "Start",
    "node_label": "User submits query via POST /api/v1/chat",
    "category": "platform",
    "type": "startEnd",
    "neighbors": {
      "incoming": [],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/api/v1/chat.py:36 - @router.post('/chat') decorator",
        "TARGET: N/A - This is the entry point marker",
        "ACTION: Entry point node - no transformation needed, just ensure logging",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "entry_point",
    "claude_code_instructions": [
      "# STEP 2: Entry point marker - ensure it's logged",
      "# This is the POST endpoint itself, just needs logging",
      "",
      "# Add logging at start of endpoint:",
      "# In app/api/v1/chat.py after line 36:",
      "rag_step_log(",
      "    step=2,",
      "    step_id=\"RAG.platform.user.submits.query.via.post.api.v1.chat\",",
      "    action=\"request_received\",",
      "    method='POST',",
      "    endpoint='/api/v1/chat'",
      ")"
    ],
    "transformation_notes": "Entry point node - no transformation needed, just ensure logging",
    "current_implementation": "app/api/v1/chat.py:36 - @router.post('/chat') decorator",
    "target_implementation": "N/A - This is the entry point marker",
    "test_requirements": [
      "Endpoint exists",
      "Logging occurs on request"
    ]
  },
  "3": {
    "step": 3,
    "id": "RAG.platform.request.valid",
    "node_id": "ValidCheck",
    "node_label": "Request valid?",
    "category": "platform",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "ValidateRequest"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/api/v1/chat.py:45-55 - Inline validation checks",
        "TARGET: app/orchestrators/platform.py - step_3__request_valid_decision()",
        "ACTION: Extract validation decision into explicit decision node",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 3: Decision node for request validation",
      "# Current: Inline checks in endpoint",
      "# Target: Explicit decision orchestrator",
      "",
      "cat <<'EOF' > app/orchestrators/platform_step3.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_3__request_valid_decision(validation_result: dict):",
      "    \"\"\"RAG STEP 3: Decision - is request valid?\"\"\"",
      "    ",
      "    is_valid = validation_result.get(\"valid\", False)",
      "    ",
      "    next_step = \"GDPRLog\" if is_valid else \"Error400\"",
      "    ",
      "    rag_step_log(",
      "        step=3,",
      "        step_id=\"RAG.platform.request.valid\",",
      "        decision=f\"request_{\"valid\" if is_valid else \"invalid\"}\",",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Extract validation decision into explicit decision node",
    "current_implementation": "app/api/v1/chat.py:45-55 - Inline validation checks",
    "target_implementation": "app/orchestrators/platform.py - step_3__request_valid_decision()",
    "test_requirements": [
      "Routes valid requests to GDPRLog",
      "Routes invalid requests to Error400"
    ]
  },
  "4": {
    "step": 4,
    "id": "RAG.privacy.gdprcompliance.record.processing.log.data.processing",
    "node_id": "GDPRLog",
    "node_label": "GDPRCompliance.record_processing Log data processing",
    "category": "privacy",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "PrivacyCheck"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/services/privacy/gdpr_compliance.py - If exists, or missing",
        "TARGET: app/orchestrators/privacy.py - step_4__gdpr_log()",
        "ACTION: Implement or extract GDPR logging into orchestrator",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 4: GDPR compliance logging",
      "# May be missing - create if needed",
      "",
      "# 1. Check if GDPR service exists:",
      "ls -la app/services/privacy/gdpr_compliance.py 2>/dev/null || echo 'Not found'",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/privacy_step4.py",
      "from app.observability.rag_logging import rag_step_log",
      "import datetime",
      "",
      "def step_4__gdpr_log(request_data: dict, user_id: str = None):",
      "    \"\"\"RAG STEP 4: Log data processing for GDPR compliance.\"\"\"",
      "    ",
      "    # Record processing activity",
      "    processing_record = {",
      "        \"timestamp\": datetime.datetime.utcnow().isoformat(),",
      "        \"purpose\": \"chat_completion\",",
      "        \"data_categories\": [\"user_messages\"],",
      "        \"user_id\": user_id or \"anonymous\"",
      "    }",
      "    ",
      "    # TODO: Store in GDPR audit log if service exists",
      "    ",
      "    rag_step_log(",
      "        step=4,",
      "        step_id=\"RAG.privacy.gdprcompliance.record.processing.log.data.processing\",",
      "        action=\"gdpr_logged\",",
      "        **processing_record",
      "    )",
      "    ",
      "    return {\"logged\": True, \"next_step\": \"PrivacyCheck\"}",
      "EOF"
    ],
    "transformation_notes": "Implement or extract GDPR logging into orchestrator",
    "current_implementation": "app/services/privacy/gdpr_compliance.py - If exists, or missing",
    "target_implementation": "app/orchestrators/privacy.py - step_4__gdpr_log()",
    "test_requirements": [
      "GDPR record created with timestamp",
      "Processing purpose recorded",
      "Routes to PrivacyCheck"
    ]
  },
  "5": {
    "step": 5,
    "id": "RAG.platform.return.400.bad.request",
    "node_id": "Error400",
    "node_label": "Return 400 Bad Request",
    "category": "platform",
    "type": "error",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "End"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/api/v1/chat.py - HTTPException(status_code=400)",
        "TARGET: app/orchestrators/platform.py - step_5__return_400()",
        "ACTION: Error handling node - standardize error response",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 5: Error response for invalid requests",
      "",
      "cat <<'EOF' > app/orchestrators/platform_step5.py",
      "from app.observability.rag_logging import rag_step_log",
      "from fastapi import HTTPException",
      "",
      "def step_5__return_400(validation_errors: list = None):",
      "    \"\"\"RAG STEP 5: Return 400 Bad Request.\"\"\"",
      "    ",
      "    rag_step_log(",
      "        step=5,",
      "        step_id=\"RAG.platform.return.400.bad.request\",",
      "        action=\"returning_error\",",
      "        status_code=400,",
      "        errors=validation_errors",
      "    )",
      "    ",
      "    raise HTTPException(",
      "        status_code=400,",
      "        detail={\"error\": \"Bad Request\", \"details\": validation_errors}",
      "    )",
      "EOF"
    ],
    "transformation_notes": "Error handling node - standardize error response",
    "current_implementation": "app/api/v1/chat.py - HTTPException(status_code=400)",
    "target_implementation": "app/orchestrators/platform.py - step_5__return_400()",
    "test_requirements": [
      "Returns 400 status code",
      "Includes error details",
      "Logs error response"
    ]
  },
  "6": {
    "step": 6,
    "id": "RAG.privacy.privacy.anonymize.requests.enabled",
    "node_id": "PrivacyCheck",
    "node_label": "PRIVACY_ANONYMIZE_REQUESTS enabled?",
    "category": "privacy",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "GDPRLog"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/config.py - Settings.PRIVACY_ANONYMIZE_REQUESTS",
        "TARGET: app/orchestrators/privacy.py - step_6__privacy_check_decision()",
        "ACTION: Decision node checking privacy configuration",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 6: Decision node for privacy anonymization",
      "",
      "cat <<'EOF' > app/orchestrators/privacy_step6.py",
      "from app.core.config import settings",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_6__privacy_check_decision():",
      "    \"\"\"RAG STEP 6: Check if PII anonymization is enabled.\"\"\"",
      "    ",
      "    enabled = settings.PRIVACY_ANONYMIZE_REQUESTS",
      "    next_step = \"AnonymizeText\" if enabled else \"InitAgent\"",
      "    ",
      "    rag_step_log(",
      "        step=6,",
      "        step_id=\"RAG.privacy.privacy.anonymize.requests.enabled\",",
      "        decision=f\"anonymization_{\"enabled\" if enabled else \"disabled\"}\",",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Decision node checking privacy configuration",
    "current_implementation": "app/core/config.py - Settings.PRIVACY_ANONYMIZE_REQUESTS",
    "target_implementation": "app/orchestrators/privacy.py - step_6__privacy_check_decision()",
    "test_requirements": [
      "Reads PRIVACY_ANONYMIZE_REQUESTS setting",
      "Routes to AnonymizeText if enabled",
      "Routes to InitAgent if disabled"
    ]
  },
  "7": {
    "step": 7,
    "id": "RAG.privacy.anonymizer.anonymize.text.anonymize.pii",
    "node_id": "AnonymizeText",
    "node_label": "Anonymizer.anonymize_text Anonymize PII",
    "category": "privacy",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "PIICheck"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/services/privacy/anonymizer.py - If exists",
        "TARGET: app/orchestrators/privacy.py - step_7__anonymize_pii()",
        "ACTION: Extract PII anonymization into orchestrator",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 7: Anonymize PII in text",
      "",
      "# 1. Check for existing anonymizer:",
      "ls -la app/services/privacy/anonymizer.py 2>/dev/null || echo 'Not found'",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/privacy_step7.py",
      "from app.observability.rag_logging import rag_step_log",
      "import re",
      "",
      "def step_7__anonymize_pii(messages: list):",
      "    \"\"\"RAG STEP 7: Anonymize PII in messages.\"\"\"",
      "    ",
      "    # Simple PII patterns (extend as needed)",
      "    patterns = {",
      "        \"email\": (r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"[EMAIL]\"),",
      "        \"phone\": (r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\", \"[PHONE]\"),",
      "        \"ssn\": (r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\", \"[SSN]\")",
      "    }",
      "    ",
      "    pii_detected = False",
      "    anonymized_messages = []",
      "    ",
      "    for msg in messages:",
      "        content = msg.get(\"content\", \"\")",
      "        for pii_type, (pattern, replacement) in patterns.items():",
      "            if re.search(pattern, content):",
      "                pii_detected = True",
      "                content = re.sub(pattern, replacement, content)",
      "        anonymized_messages.append({**msg, \"content\": content})",
      "    ",
      "    rag_step_log(",
      "        step=7,",
      "        step_id=\"RAG.privacy.anonymizer.anonymize.text.anonymize.pii\",",
      "        action=\"anonymized\",",
      "        pii_detected=pii_detected,",
      "        messages_count=len(messages)",
      "    )",
      "    ",
      "    return {",
      "        \"messages\": anonymized_messages,",
      "        \"pii_detected\": pii_detected,",
      "        \"next_step\": \"PIICheck\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Extract PII anonymization into orchestrator",
    "current_implementation": "app/services/privacy/anonymizer.py - If exists",
    "target_implementation": "app/orchestrators/privacy.py - step_7__anonymize_pii()",
    "test_requirements": [
      "Detects and replaces emails",
      "Detects and replaces phone numbers",
      "Returns anonymized messages",
      "Sets pii_detected flag"
    ]
  },
  "8": {
    "step": 8,
    "id": "RAG.response.langgraphagent.get.response.initialize.workflow",
    "node_id": "InitAgent",
    "node_label": "LangGraphAgent.get_response Initialize workflow",
    "category": "response",
    "type": "process",
    "neighbors": {
      "incoming": [
        "LogPII"
      ],
      "outgoing": [
        "ConvertMessages"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/langgraph/graph.py:85 - LangGraphAgent.get_response()",
        "TARGET: app/orchestrators/platform.py - step_8__init_workflow()",
        "ACTION: Wrap agent initialization with orchestrator",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 8: Initialize LangGraph workflow",
      "# This wraps the existing agent initialization",
      "",
      "cat <<'EOF' > app/orchestrators/platform_step8.py",
      "from app.core.langgraph.graph import LangGraphAgent",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "",
      "def step_8__init_workflow(messages: list, model: str = None):",
      "    \"\"\"RAG STEP 8: Initialize LangGraph workflow.\"\"\"",
      "    ",
      "    with rag_step_timer(step=8) as timer:",
      "        # Initialize the agent",
      "        agent = LangGraphAgent(model=model)",
      "        ",
      "        rag_step_log(",
      "            step=8,",
      "            step_id=\"RAG.response.langgraphagent.get.response.initialize.workflow\",",
      "            action=\"workflow_initialized\",",
      "            model=model or 'default',",
      "            messages_count=len(messages)",
      "        )",
      "        ",
      "        return {",
      "            \"agent\": agent,",
      "            \"messages\": messages,",
      "            \"next_step\": \"ConvertMessages\"",
      "        }",
      "EOF"
    ],
    "transformation_notes": "Wrap agent initialization with orchestrator",
    "current_implementation": "app/core/langgraph/graph.py:85 - LangGraphAgent.get_response()",
    "target_implementation": "app/orchestrators/platform.py - step_8__init_workflow()",
    "test_requirements": [
      "Agent instance created",
      "Workflow initialized",
      "Routes to ConvertMessages"
    ]
  },
  "9": {
    "step": 9,
    "id": "RAG.platform.pii.detected",
    "node_id": "PIICheck",
    "node_label": "PII detected?",
    "category": "platform",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "AnonymizeText"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Inline check after anonymization",
        "TARGET: app/orchestrators/privacy.py - step_9__pii_detected_decision()",
        "ACTION: Decision node for PII detection result",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 9: Decision node - was PII detected?",
      "",
      "cat <<'EOF' > app/orchestrators/privacy_step9.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_9__pii_detected_decision(anonymization_result: dict):",
      "    \"\"\"RAG STEP 9: Decision - was PII detected?\"\"\"",
      "    ",
      "    pii_detected = anonymization_result.get(\"pii_detected\", False)",
      "    next_step = \"LogPII\" if pii_detected else \"InitAgent\"",
      "    ",
      "    rag_step_log(",
      "        step=9,",
      "        step_id=\"RAG.platform.pii.detected\",",
      "        decision=f\"pii_{\"detected\" if pii_detected else \"not_detected\"}\",",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Decision node for PII detection result",
    "current_implementation": "Inline check after anonymization",
    "target_implementation": "app/orchestrators/privacy.py - step_9__pii_detected_decision()",
    "test_requirements": [
      "Routes to LogPII if PII detected",
      "Routes to InitAgent if no PII"
    ]
  },
  "10": {
    "step": 10,
    "id": "RAG.platform.logger.info.log.pii.anonymization",
    "node_id": "LogPII",
    "node_label": "Logger.info Log PII anonymization",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "InitAgent"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Standard logging calls",
        "TARGET: app/orchestrators/platform.py - step_10__log_pii_anonymization()",
        "ACTION: Structured logging for PII anonymization",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 10: Log PII anonymization event",
      "",
      "cat <<'EOF' > app/orchestrators/platform_step10.py",
      "from app.observability.rag_logging import rag_step_log",
      "import logging",
      "",
      "logger = logging.getLogger(__name__)",
      "",
      "def step_10__log_pii_anonymization(anonymization_result: dict):",
      "    \"\"\"RAG STEP 10: Log PII anonymization.\"\"\"",
      "    ",
      "    # Standard logger",
      "    logger.info(",
      "        \"PII anonymization completed\",",
      "        extra={",
      "            \"pii_detected\": anonymization_result.get(\"pii_detected\"),",
      "            \"messages_count\": len(anonymization_result.get(\"messages\", []))",
      "        }",
      "    )",
      "    ",
      "    # RAG step log",
      "    rag_step_log(",
      "        step=10,",
      "        step_id=\"RAG.platform.logger.info.log.pii.anonymization\",",
      "        action=\"pii_anonymization_logged\",",
      "        pii_detected=anonymization_result.get(\"pii_detected\")",
      "    )",
      "    ",
      "    return {\"next_step\": \"InitAgent\"}",
      "EOF"
    ],
    "transformation_notes": "Structured logging for PII anonymization",
    "current_implementation": "Standard logging calls",
    "target_implementation": "app/orchestrators/platform.py - step_10__log_pii_anonymization()",
    "test_requirements": [
      "Logs to standard logger",
      "Creates RAG step log",
      "Routes to InitAgent"
    ]
  },
  "11": {
    "step": 11,
    "id": "RAG.platform.langgraphagent.chat.convert.to.message.objects",
    "node_id": "ConvertMessages",
    "node_label": "LangGraphAgent._chat Convert to Message objects",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [
        "InitAgent"
      ],
      "outgoing": [
        "ExtractQuery"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/langgraph/graph.py:200-220 - _chat() method converts dicts to Message objects",
        "TARGET: app/orchestrators/platform.py - step_11__convert_messages()",
        "ACTION: Extract message conversion logic into orchestrator",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 11: Convert raw dicts to Message objects",
      "# Current: Inside LangGraphAgent._chat()",
      "# Target: Explicit message conversion orchestrator",
      "",
      "# 1. Check current conversion logic:",
      "cat app/core/langgraph/graph.py | grep -A 20 'def _chat'",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/platform_step11.py",
      "from app.observability.rag_logging import rag_step_log",
      "from app.schemas.chat import Message",
      "from typing import List, Dict, Union",
      "",
      "def step_11__convert_messages(raw_messages: List[Union[Dict, Message]]) -> dict:",
      "    \"\"\"RAG STEP 11: Convert to Message objects.\"\"\"",
      "    ",
      "    converted = []",
      "    for msg in raw_messages:",
      "        if isinstance(msg, dict):",
      "            converted.append(Message(**msg))",
      "        elif isinstance(msg, Message):",
      "            converted.append(msg)",
      "        else:",
      "            raise ValueError(f\"Invalid message type: {type(msg)}\")",
      "    ",
      "    rag_step_log(",
      "        step=11,",
      "        step_id=\"RAG.platform.langgraphagent.chat.convert.to.message.objects\",",
      "        action=\"messages_converted\",",
      "        input_count=len(raw_messages),",
      "        output_count=len(converted)",
      "    )",
      "    ",
      "    return {",
      "        \"messages\": converted,",
      "        \"next_step\": \"ExtractQuery\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Extract message conversion logic into orchestrator",
    "current_implementation": "app/core/langgraph/graph.py:200-220 - _chat() method converts dicts to Message objects",
    "target_implementation": "app/orchestrators/platform.py - step_11__convert_messages()",
    "test_requirements": [
      "Converts dicts to Message objects",
      "Preserves existing Message objects",
      "Handles invalid types with error"
    ]
  },
  "12": {
    "step": 12,
    "id": "RAG.classify.langgraphagent.classify.user.query.extract.user.message",
    "node_id": "ExtractQuery",
    "node_label": "LangGraphAgent._classify_user_query Extract user message",
    "category": "classify",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ConvertMessages"
      ],
      "outgoing": [
        "MessageExists"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/langgraph/graph.py:98 - _classify_user_query() extracts last user message",
        "TARGET: app/orchestrators/classify.py - step_12__extract_user_message()",
        "ACTION: Extract user message extraction from classification method",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 12: Extract user message from conversation",
      "# Current: Part of _classify_user_query()",
      "# Target: Dedicated extraction orchestrator",
      "",
      "# 1. Examine current extraction:",
      "cat app/core/langgraph/graph.py | grep -A 15 '_classify_user_query'",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/classify_step12.py",
      "from app.observability.rag_logging import rag_step_log",
      "from app.schemas.chat import Message",
      "from typing import List, Optional",
      "",
      "def step_12__extract_user_message(messages: List[Message]) -> dict:",
      "    \"\"\"RAG STEP 12: Extract user message for classification.\"\"\"",
      "    ",
      "    user_message = None",
      "    # Find last user message",
      "    for msg in reversed(messages):",
      "        if msg.role == \"user\":",
      "            user_message = msg.content",
      "            break",
      "    ",
      "    rag_step_log(",
      "        step=12,",
      "        step_id=\"RAG.classify.langgraphagent.classify.user.query.extract.user.message\",",
      "        action=\"user_message_extracted\",",
      "        has_user_message=user_message is not None,",
      "        message_length=len(user_message) if user_message else 0",
      "    )",
      "    ",
      "    return {",
      "        \"user_message\": user_message,",
      "        \"messages\": messages,",
      "        \"next_step\": \"MessageExists\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Extract user message extraction from classification method",
    "current_implementation": "app/core/langgraph/graph.py:98 - _classify_user_query() extracts last user message",
    "target_implementation": "app/orchestrators/classify.py - step_12__extract_user_message()",
    "test_requirements": [
      "Extracts last user message",
      "Handles missing user message",
      "Preserves full message list"
    ]
  },
  "13": {
    "step": 13,
    "id": "RAG.platform.user.message.exists",
    "node_id": "MessageExists",
    "node_label": "User message exists?",
    "category": "platform",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "ExtractQuery"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Inline check in classification flow",
        "TARGET: app/orchestrators/platform.py - step_13__message_exists_decision()",
        "ACTION: Decision node for user message existence",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "inline_logic",
    "claude_code_instructions": [
      "# STEP 13: Decision - does user message exist?",
      "",
      "cat <<'EOF' > app/orchestrators/platform_step13.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_13__message_exists_decision(extraction_result: dict) -> str:",
      "    \"\"\"RAG STEP 13: Decision - user message exists?\"\"\"",
      "    ",
      "    user_message = extraction_result.get(\"user_message\")",
      "    exists = user_message is not None and len(user_message.strip()) > 0",
      "    ",
      "    next_step = \"ExtractFacts\" if exists else \"DefaultPrompt\"",
      "    ",
      "    rag_step_log(",
      "        step=13,",
      "        step_id=\"RAG.platform.user.message.exists\",",
      "        decision=f\"message_{\"exists\" if exists else \"missing\"}\",",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Decision node for user message existence",
    "current_implementation": "Inline check in classification flow",
    "target_implementation": "app/orchestrators/platform.py - step_13__message_exists_decision()",
    "test_requirements": [
      "Routes to ExtractFacts if message exists",
      "Routes to DefaultPrompt if missing/empty"
    ]
  },
  "14": {
    "step": 14,
    "id": "RAG.facts.atomicfactsextractor.extract.extract.atomic.facts",
    "node_id": "ExtractFacts",
    "node_label": "AtomicFactsExtractor.extract Extract atomic facts",
    "category": "facts",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "CanonicalizeFacts"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/services/facts/atomic_facts_extractor.py - extract() method if exists",
        "TARGET: app/orchestrators/facts.py - step_14__extract_atomic_facts()",
        "ACTION: Wrap atomic facts extraction service",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 14: Extract atomic facts from user message",
      "",
      "# 1. Check if service exists:",
      "ls -la app/services/facts/atomic_facts_extractor.py 2>/dev/null || echo 'Not found'",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/facts_step14.py",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "from typing import List, Dict",
      "",
      "def step_14__extract_atomic_facts(user_message: str) -> dict:",
      "    \"\"\"RAG STEP 14: Extract atomic facts from message.\"\"\"",
      "    ",
      "    with rag_step_timer(step=14) as timer:",
      "        # Simple fact extraction (enhance as needed)",
      "        facts = []",
      "        ",
      "        # Extract dates",
      "        import re",
      "        dates = re.findall(r\"\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b\", user_message)",
      "        for date in dates:",
      "            facts.append({\"type\": \"date\", \"value\": date})",
      "        ",
      "        # Extract amounts",
      "        amounts = re.findall(r\"\\$[\\d,]+\\.?\\d*|\\d+\\.\\d+\\s*(?:EUR|USD)\", user_message)",
      "        for amount in amounts:",
      "            facts.append({\"type\": \"amount\", \"value\": amount})",
      "        ",
      "        # Extract key terms",
      "        terms = [\"tax\", \"invoice\", \"payment\", \"contract\", \"salary\"]",
      "        for term in terms:",
      "            if term.lower() in user_message.lower():",
      "                facts.append({\"type\": \"term\", \"value\": term})",
      "        ",
      "        rag_step_log(",
      "            step=14,",
      "            step_id=\"RAG.facts.atomicfactsextractor.extract.extract.atomic.facts\",",
      "            action=\"facts_extracted\",",
      "            facts_count=len(facts),",
      "            fact_types=list(set(f['type'] for f in facts))",
      "        )",
      "        ",
      "        return {",
      "            \"facts\": facts,",
      "            \"user_message\": user_message,",
      "            \"next_step\": \"CanonicalizeFacts\"",
      "        }",
      "EOF"
    ],
    "transformation_notes": "Wrap atomic facts extraction service",
    "current_implementation": "app/services/facts/atomic_facts_extractor.py - extract() method if exists",
    "target_implementation": "app/orchestrators/facts.py - step_14__extract_atomic_facts()",
    "test_requirements": [
      "Extracts dates from text",
      "Extracts monetary amounts",
      "Extracts key terms",
      "Returns structured facts list"
    ]
  },
  "15": {
    "step": 15,
    "id": "RAG.prompting.continue.without.classification",
    "node_id": "DefaultPrompt",
    "node_label": "Continue without classification",
    "category": "prompting",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "SelectPrompt"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Fallback path when no user message",
        "TARGET: app/orchestrators/prompting.py - step_15__continue_without_classification()",
        "ACTION: Handle flow when classification is skipped",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 15: Continue without classification (fallback)",
      "",
      "cat <<'EOF' > app/orchestrators/prompting_step15.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_15__continue_without_classification(messages: list) -> dict:",
      "    \"\"\"RAG STEP 15: Continue without classification.\"\"\"",
      "    ",
      "    rag_step_log(",
      "        step=15,",
      "        step_id=\"RAG.prompting.continue.without.classification\",",
      "        action=\"skipping_classification\",",
      "        reason=\"no_user_message\",",
      "        messages_count=len(messages)",
      "    )",
      "    ",
      "    # Skip directly to default prompt selection",
      "    return {",
      "        \"messages\": messages,",
      "        \"classification\": None,",
      "        \"next_step\": \"DefaultPrompt\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Handle flow when classification is skipped",
    "current_implementation": "Fallback path when no user message",
    "target_implementation": "app/orchestrators/prompting.py - step_15__continue_without_classification()",
    "test_requirements": [
      "Handles missing classification gracefully",
      "Routes to DefaultPrompt"
    ]
  },
  "16": {
    "step": 16,
    "id": "RAG.facts.atomicfactsextractor.canonicalize.normalize.dates.amounts.rates",
    "node_id": "CanonicalizeFacts",
    "node_label": "AtomicFactsExtractor.canonicalize Normalize dates amounts rates",
    "category": "facts",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ExtractFacts"
      ],
      "outgoing": [
        "AttachmentFingerprint"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/services/facts/atomic_facts_extractor.py - canonicalize() if exists",
        "TARGET: app/orchestrators/facts.py - step_16__canonicalize_facts()",
        "ACTION: Normalize extracted facts into canonical form",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 16: Canonicalize facts (normalize formats)",
      "",
      "cat <<'EOF' > app/orchestrators/facts_step16.py",
      "from app.observability.rag_logging import rag_step_log",
      "from datetime import datetime",
      "import re",
      "",
      "def step_16__canonicalize_facts(facts: list) -> dict:",
      "    \"\"\"RAG STEP 16: Normalize dates, amounts, rates to canonical form.\"\"\"",
      "    ",
      "    canonical_facts = []",
      "    ",
      "    for fact in facts:",
      "        if fact[\"type\"] == \"date\":",
      "            # Normalize dates to ISO format",
      "            try:",
      "                date_str = fact[\"value\"]",
      "                # Parse various formats",
      "                if \"/\" in date_str:",
      "                    dt = datetime.strptime(date_str, \"%m/%d/%Y\")",
      "                canonical_facts.append({\"type\": \"date\", \"value\": dt.isoformat()[:10]})",
      "            except:",
      "                canonical_facts.append(fact)",
      "                ",
      "        elif fact[\"type\"] == \"amount\":",
      "            # Normalize amounts to decimal",
      "            value = fact[\"value\"]",
      "            value = re.sub(r\"[^\\d.]\", \"\", value)",
      "            try:",
      "                canonical_facts.append({\"type\": \"amount\", \"value\": float(value)})",
      "            except:",
      "                canonical_facts.append(fact)",
      "                ",
      "        else:",
      "            canonical_facts.append(fact)",
      "    ",
      "    rag_step_log(",
      "        step=16,",
      "        step_id=\"RAG.facts.atomicfactsextractor.canonicalize.normalize.dates.amounts.rates\",",
      "        action=\"facts_canonicalized\",",
      "        input_count=len(facts),",
      "        output_count=len(canonical_facts)",
      "    )",
      "    ",
      "    return {",
      "        \"canonical_facts\": canonical_facts,",
      "        \"next_step\": \"AttachmentFingerprint\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Normalize extracted facts into canonical form",
    "current_implementation": "app/services/facts/atomic_facts_extractor.py - canonicalize() if exists",
    "target_implementation": "app/orchestrators/facts.py - step_16__canonicalize_facts()",
    "test_requirements": [
      "Normalizes dates to ISO format",
      "Normalizes amounts to floats",
      "Preserves other fact types"
    ]
  },
  "17": {
    "step": 17,
    "id": "RAG.preflight.attachmentfingerprint.compute.sha.256.per.attachment",
    "node_id": "AttachmentFingerprint",
    "node_label": "AttachmentFingerprint.compute SHA-256 per attachment",
    "category": "preflight",
    "type": "process",
    "neighbors": {
      "incoming": [
        "CanonicalizeFacts"
      ],
      "outgoing": [
        "QuerySig"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing or in document processing",
        "TARGET: app/orchestrators/preflight.py - step_17__compute_attachment_hashes()",
        "ACTION: Compute SHA-256 hashes for attachments",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 17: Compute SHA-256 for each attachment",
      "",
      "cat <<'EOF' > app/orchestrators/preflight_step17.py",
      "from app.observability.rag_logging import rag_step_log",
      "import hashlib",
      "from typing import List, Dict",
      "",
      "def step_17__compute_attachment_hashes(attachments: List[Dict] = None) -> dict:",
      "    \"\"\"RAG STEP 17: Compute SHA-256 per attachment.\"\"\"",
      "    ",
      "    attachment_hashes = []",
      "    ",
      "    if attachments:",
      "        for attachment in attachments:",
      "            file_content = attachment.get(\"content\", b\"\")",
      "            if isinstance(file_content, str):",
      "                file_content = file_content.encode('utf-8')",
      "            ",
      "            sha256 = hashlib.sha256(file_content).hexdigest()",
      "            attachment_hashes.append({",
      "                \"filename\": attachment.get(\"filename\", \"unknown\"),",
      "                \"sha256\": sha256,",
      "                \"size\": len(file_content)",
      "            })",
      "    ",
      "    rag_step_log(",
      "        step=17,",
      "        step_id=\"RAG.preflight.attachmentfingerprint.compute.sha.256.per.attachment\",",
      "        action=\"attachment_hashes_computed\",",
      "        attachment_count=len(attachments) if attachments else 0,",
      "        has_attachments=bool(attachments)",
      "    )",
      "    ",
      "    return {",
      "        \"attachment_hashes\": attachment_hashes,",
      "        \"attachments\": attachments,",
      "        \"next_step\": \"QuerySig\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Compute SHA-256 hashes for attachments",
    "current_implementation": "Missing or in document processing",
    "target_implementation": "app/orchestrators/preflight.py - step_17__compute_attachment_hashes()",
    "test_requirements": [
      "Computes SHA-256 for each attachment",
      "Handles missing attachments",
      "Returns hash list with metadata"
    ]
  },
  "18": {
    "step": 18,
    "id": "RAG.facts.querysignature.compute.hash.from.canonical.facts",
    "node_id": "QuerySig",
    "node_label": "QuerySignature.compute Hash from canonical facts",
    "category": "facts",
    "type": "process",
    "neighbors": {
      "incoming": [
        "AttachmentFingerprint"
      ],
      "outgoing": [
        "AttachCheck"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing or in cache key generation",
        "TARGET: app/orchestrators/facts.py - step_18__compute_query_signature()",
        "ACTION: Generate deterministic hash from canonical facts",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 18: Compute query signature from canonical facts",
      "",
      "cat <<'EOF' > app/orchestrators/facts_step18.py",
      "from app.observability.rag_logging import rag_step_log",
      "import hashlib",
      "import json",
      "",
      "def step_18__compute_query_signature(canonical_facts: list, attachment_hashes: list = None) -> dict:",
      "    \"\"\"RAG STEP 18: Hash from canonical facts for caching.\"\"\"",
      "    ",
      "    # Create deterministic signature",
      "    signature_data = {",
      "        \"facts\": sorted(canonical_facts, key=lambda x: json.dumps(x, sort_keys=True)),",
      "        \"attachments\": sorted(attachment_hashes or [], key=lambda x: x.get(\"sha256\", \"\"))",
      "    }",
      "    ",
      "    # Generate hash",
      "    signature_str = json.dumps(signature_data, sort_keys=True)",
      "    query_signature = hashlib.sha256(signature_str.encode()).hexdigest()[:16]",
      "    ",
      "    rag_step_log(",
      "        step=18,",
      "        step_id=\"RAG.facts.querysignature.compute.hash.from.canonical.facts\",",
      "        action=\"query_signature_computed\",",
      "        signature=query_signature,",
      "        facts_count=len(canonical_facts),",
      "        attachments_count=len(attachment_hashes) if attachment_hashes else 0",
      "    )",
      "    ",
      "    return {",
      "        \"query_signature\": query_signature,",
      "        \"canonical_facts\": canonical_facts,",
      "        \"attachment_hashes\": attachment_hashes,",
      "        \"next_step\": \"AttachCheck\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Generate deterministic hash from canonical facts",
    "current_implementation": "Missing or in cache key generation",
    "target_implementation": "app/orchestrators/facts.py - step_18__compute_query_signature()",
    "test_requirements": [
      "Generates deterministic hash",
      "Includes facts and attachments",
      "Returns stable signature"
    ]
  },
  "19": {
    "step": 19,
    "id": "RAG.preflight.attachments.present",
    "node_id": "AttachCheck",
    "node_label": "Attachments present?",
    "category": "preflight",
    "type": "process",
    "neighbors": {
      "incoming": [
        "QuerySig"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Inline check in processing flow",
        "TARGET: app/orchestrators/preflight.py - step_19__attachments_present_decision()",
        "ACTION: Decision node for attachment presence",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "inline_logic",
    "claude_code_instructions": [
      "# STEP 19: Decision - are attachments present?",
      "",
      "cat <<'EOF' > app/orchestrators/preflight_step19.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_19__attachments_present_decision(processing_context: dict) -> str:",
      "    \"\"\"RAG STEP 19: Decision - attachments present?\"\"\"",
      "    ",
      "    attachments = processing_context.get(\"attachments\", [])",
      "    attachment_hashes = processing_context.get(\"attachment_hashes\", [])",
      "    has_attachments = bool(attachments or attachment_hashes)",
      "    ",
      "    next_step = \"QuickPreIngest\" if has_attachments else \"GoldenFastGate\"",
      "    ",
      "    rag_step_log(",
      "        step=19,",
      "        step_id=\"RAG.preflight.attachments.present\",",
      "        decision=f\"attachments_{\"present\" if has_attachments else \"absent\"}\",",
      "        attachment_count=len(attachments),",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Decision node for attachment presence",
    "current_implementation": "Inline check in processing flow",
    "target_implementation": "app/orchestrators/preflight.py - step_19__attachments_present_decision()",
    "test_requirements": [
      "Routes to QuickPreIngest if attachments",
      "Routes to GoldenFastGate if no attachments"
    ]
  },
  "20": {
    "step": 20,
    "id": "RAG.golden.golden.fast.path.eligible.no.doc.or.quick.check.safe",
    "node_id": "GoldenFastGate",
    "node_label": "Golden fast-path eligible? no doc or quick check safe",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing or in golden set matching logic",
        "TARGET: app/orchestrators/golden.py - step_20__golden_fast_path_check()",
        "ACTION: Check eligibility for golden answer fast path",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 20: Check golden fast-path eligibility",
      "",
      "cat <<'EOF' > app/orchestrators/golden_step20.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_20__golden_fast_path_check(query_signature: str, has_attachments: bool = False) -> dict:",
      "    \"\"\"RAG STEP 20: Check if eligible for golden fast-path.\"\"\"",
      "    ",
      "    # Fast path eligible if:",
      "    # 1. No attachments (or safe doc types)",
      "    # 2. Query signature exists",
      "    # 3. Not a complex multi-turn conversation",
      "    ",
      "    eligible = not has_attachments and bool(query_signature)",
      "    ",
      "    next_step = \"GoldenLookup\" if eligible else \"ClassifyDomain\"",
      "    ",
      "    rag_step_log(",
      "        step=20,",
      "        step_id=\"RAG.golden.goldenfastgate.golden.fast.path.eligible.no.doc.or.quick.check.safe\",",
      "        decision=f\"fast_path_{\"eligible\" if eligible else \"ineligible\"}\",",
      "        has_attachments=has_attachments,",
      "        has_signature=bool(query_signature),",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return {",
      "        \"fast_path_eligible\": eligible,",
      "        \"next_step\": next_step",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Check eligibility for golden answer fast path",
    "current_implementation": "Missing or in golden set matching logic",
    "target_implementation": "app/orchestrators/golden.py - step_20__golden_fast_path_check()",
    "test_requirements": [
      "Checks attachment presence",
      "Validates query signature",
      "Routes to GoldenLookup or ClassifyDomain"
    ]
  },
  "21": {
    "step": 21,
    "id": "RAG.preflight.docpreingest.quick.extract.type.sniff.and.key.fields",
    "node_id": "QuickPreIngest",
    "node_label": "DocPreIngest.quick_extract type sniff and key fields",
    "category": "preflight",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "DocDependent"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing or in document processing pipeline",
        "TARGET: app/orchestrators/docs.py - step_21__quick_pre_ingest()",
        "ACTION: Implement quick document type detection and key field extraction",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 21: Quick document pre-ingestion",
      "# Current: Missing or embedded in doc processing",
      "# Target: Explicit pre-ingestion orchestrator",
      "",
      "# 1. Check for existing doc processing:",
      "find app -name '*doc*' -o -name '*ingest*' | head -10",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/docs_step21.py",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "from typing import List, Dict",
      "import magic  # python-magic for file type detection",
      "",
      "def step_21__quick_pre_ingest(attachments: List[Dict]) -> dict:",
      "    \"\"\"RAG STEP 21: Quick extract document type and key fields.\"\"\"",
      "    ",
      "    with rag_step_timer(step=21) as timer:",
      "        doc_metadata = []",
      "        ",
      "        for attachment in attachments:",
      "            filename = attachment.get(\"filename\", \"unknown\")",
      "            content = attachment.get(\"content\", b\"\")",
      "            ",
      "            # Quick type detection",
      "            if filename.endswith('.pdf'):",
      "                doc_type = \"pdf\"",
      "            elif filename.endswith(('.xlsx', '.xls')):",
      "                doc_type = \"spreadsheet\"",
      "            elif filename.endswith('.xml'):",
      "                doc_type = \"xml\"",
      "            else:",
      "                doc_type = \"unknown\"",
      "            ",
      "            # Extract key fields (basic)",
      "            key_fields = {}",
      "            if doc_type == \"xml\" and b\"fattura\" in content.lower():",
      "                key_fields[\"document_type\"] = \"fattura\"",
      "            elif doc_type == \"pdf\" and b\"contratto\" in content.lower():",
      "                key_fields[\"document_type\"] = \"contract\"",
      "            ",
      "            doc_metadata.append({",
      "                \"filename\": filename,",
      "                \"type\": doc_type,",
      "                \"key_fields\": key_fields,",
      "                \"size\": len(content)",
      "            })",
      "        ",
      "        rag_step_log(",
      "            step=21,",
      "            step_id=\"RAG.preflight.docpreingest.quick.extract.type.sniff.and.key.fields\",",
      "            action=\"quick_extract_completed\",",
      "            documents_processed=len(attachments),",
      "            document_types=[m['type'] for m in doc_metadata]",
      "        )",
      "        ",
      "        return {",
      "            \"doc_metadata\": doc_metadata,",
      "            \"attachments\": attachments,",
      "            \"next_step\": \"DocDependent\"",
      "        }",
      "EOF"
    ],
    "transformation_notes": "Implement quick document type detection and key field extraction",
    "current_implementation": "Missing or in document processing pipeline",
    "target_implementation": "app/orchestrators/docs.py - step_21__quick_pre_ingest()",
    "test_requirements": [
      "Detects PDF documents",
      "Detects XML/Excel documents",
      "Extracts basic key fields",
      "Returns structured metadata"
    ]
  },
  "22": {
    "step": 22,
    "id": "RAG.docs.doc.dependent.or.refers.to.doc",
    "node_id": "DocDependent",
    "node_label": "Doc-dependent or refers to doc?",
    "category": "docs",
    "type": "process",
    "neighbors": {
      "incoming": [
        "QuickPreIngest"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing decision logic",
        "TARGET: app/orchestrators/docs.py - step_22__doc_dependent_decision()",
        "ACTION: Decision node to determine if query depends on attached documents",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 22: Decision - is query document-dependent?",
      "",
      "cat <<'EOF' > app/orchestrators/docs_step22.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_22__doc_dependent_decision(doc_metadata: list, user_message: str = None) -> str:",
      "    \"\"\"RAG STEP 22: Decision - query depends on documents?\"\"\"",
      "    ",
      "    # Check if query explicitly refers to documents",
      "    doc_dependent = False",
      "    ",
      "    if user_message:",
      "        doc_keywords = [",
      "            \"this document\", \"attached file\", \"in the pdf\",",
      "            \"from the document\", \"analyze this\", \"what does this say\"",
      "        ]",
      "        doc_dependent = any(keyword in user_message.lower() for keyword in doc_keywords)",
      "    ",
      "    # Or if we have important document types",
      "    if doc_metadata:",
      "        important_types = ['fattura', 'contract', 'f24', 'payslip']",
      "        for doc in doc_metadata:",
      "            if doc.get(\"key_fields\", {}).get(\"document_type\") in important_types:",
      "                doc_dependent = True",
      "                break",
      "    ",
      "    next_step = \"RequireDocIngest\" if doc_dependent else \"GoldenFastGate\"",
      "    ",
      "    rag_step_log(",
      "        step=22,",
      "        step_id=\"RAG.docs.doc.dependent.or.refers.to.doc\",",
      "        decision=f\"doc_{\"dependent\" if doc_dependent else \"independent\"}\",",
      "        has_documents=bool(doc_metadata),",
      "        user_references_docs=doc_dependent,",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Decision node to determine if query depends on attached documents",
    "current_implementation": "Missing decision logic",
    "target_implementation": "app/orchestrators/docs.py - step_22__doc_dependent_decision()",
    "test_requirements": [
      "Detects explicit document references in query",
      "Identifies important document types",
      "Routes to RequireDocIngest or GoldenFastGate"
    ]
  },
  "23": {
    "step": 23,
    "id": "RAG.golden.plannerhint.require.doc.ingest.first.ingest.then.golden.and.kb",
    "node_id": "RequireDocIngest",
    "node_label": "PlannerHint.require_doc_ingest_first ingest then Golden and KB",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ClassifyDomain"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing planner hint system",
        "TARGET: app/orchestrators/golden.py - step_23__require_doc_ingest_hint()",
        "ACTION: Set planner hint to prioritize document ingestion before golden/KB lookup",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 23: Set planner hint for document ingestion priority",
      "",
      "cat <<'EOF' > app/orchestrators/golden_step23.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_23__require_doc_ingest_hint(doc_metadata: list) -> dict:",
      "    \"\"\"RAG STEP 23: Set hint to require document ingestion first.\"\"\"",
      "    ",
      "    # Create planner hint for workflow orchestration",
      "    hint = {",
      "        \"require_doc_ingest\": True,",
      "        \"priority\": \"documents_first\",",
      "        \"reason\": \"query_depends_on_attached_documents\",",
      "        \"document_count\": len(doc_metadata)",
      "    }",
      "    ",
      "    rag_step_log(",
      "        step=23,",
      "        step_id=\"RAG.golden.plannerhint.require.doc.ingest.first.ingest.then.golden.and.kb\",",
      "        action=\"planner_hint_set\",",
      "        hint_type=\"require_doc_ingest\",",
      "        document_count=len(doc_metadata)",
      "    )",
      "    ",
      "    # Continue to classification with hint set",
      "    return {",
      "        \"planner_hint\": hint,",
      "        \"doc_metadata\": doc_metadata,",
      "        \"next_step\": \"ClassifyDomain\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Set planner hint to prioritize document ingestion before golden/KB lookup",
    "current_implementation": "Missing planner hint system",
    "target_implementation": "app/orchestrators/golden.py - step_23__require_doc_ingest_hint()",
    "test_requirements": [
      "Creates planner hint object",
      "Sets document priority flag",
      "Routes to ClassifyDomain"
    ]
  },
  "24": {
    "step": 24,
    "id": "RAG.preflight.goldenset.match.by.signature.or.semantic",
    "node_id": "GoldenLookup",
    "node_label": "GoldenSet.match_by_signature_or_semantic",
    "category": "preflight",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "GoldenHit"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing golden set matching service",
        "TARGET: app/orchestrators/golden.py - step_24__golden_lookup()",
        "ACTION: Implement golden set lookup by signature and semantic matching",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 24: Golden set lookup and matching",
      "",
      "# 1. Check for existing golden set service:",
      "find app -name '*golden*' | head -5",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/golden_step24.py",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "from typing import Dict, Optional",
      "",
      "def step_24__golden_lookup(query_signature: str, canonical_facts: list) -> dict:",
      "    \"\"\"RAG STEP 24: Match by signature or semantic similarity.\"\"\"",
      "    ",
      "    with rag_step_timer(step=24) as timer:",
      "        match_result = None",
      "        match_score = 0.0",
      "        match_type = None",
      "        ",
      "        # 1. Exact signature match (fastest)",
      "        if query_signature:",
      "            # TODO: Query golden set database by signature",
      "            # exact_match = golden_set_db.get_by_signature(query_signature)",
      "            exact_match = None  # Placeholder",
      "            if exact_match:",
      "                match_result = exact_match",
      "                match_score = 1.0",
      "                match_type = \"signature\"",
      "        ",
      "        # 2. Semantic matching if no exact match",
      "        if not match_result and canonical_facts:",
      "            # TODO: Semantic search in golden set",
      "            # semantic_matches = golden_set_db.semantic_search(canonical_facts)",
      "            # Best match with score > 0.7",
      "            semantic_matches = []  # Placeholder",
      "            if semantic_matches:",
      "                best_match = max(semantic_matches, key=lambda x: x['score'])",
      "                if best_match['score'] > 0.7:",
      "                    match_result = best_match['content']",
      "                    match_score = best_match['score']",
      "                    match_type = \"semantic\"",
      "        ",
      "        rag_step_log(",
      "            step=24,",
      "            step_id=\"RAG.preflight.goldenset.match.by.signature.or.semantic\",",
      "            action=\"golden_lookup_completed\",",
      "            query_signature=query_signature,",
      "            match_found=match_result is not None,",
      "            match_score=match_score,",
      "            match_type=match_type",
      "        )",
      "        ",
      "        return {",
      "            \"match_result\": match_result,",
      "            \"match_score\": match_score,",
      "            \"match_type\": match_type,",
      "            \"next_step\": \"GoldenHit\"",
      "        }",
      "EOF"
    ],
    "transformation_notes": "Implement golden set lookup by signature and semantic matching",
    "current_implementation": "Missing golden set matching service",
    "target_implementation": "app/orchestrators/golden.py - step_24__golden_lookup()",
    "test_requirements": [
      "Performs signature-based lookup",
      "Falls back to semantic matching",
      "Returns match with confidence score"
    ]
  },
  "25": {
    "step": 25,
    "id": "RAG.golden.high.confidence.match.score.at.least.0.90",
    "node_id": "GoldenHit",
    "node_label": "High confidence match? score at least 0.90",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [
        "GoldenLookup"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Inline confidence check",
        "TARGET: app/orchestrators/golden.py - step_25__golden_confidence_decision()",
        "ACTION: Decision node for golden set match confidence threshold",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "inline_logic",
    "claude_code_instructions": [
      "# STEP 25: Decision - high confidence golden match?",
      "",
      "cat <<'EOF' > app/orchestrators/golden_step25.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_25__golden_confidence_decision(match_result: dict) -> str:",
      "    \"\"\"RAG STEP 25: Check if golden match meets confidence threshold.\"\"\"",
      "    ",
      "    match_score = match_result.get(\"match_score\", 0.0)",
      "    has_match = match_result.get(\"match_result\") is not None",
      "    ",
      "    # High confidence threshold",
      "    high_confidence = has_match and match_score >= 0.90",
      "    ",
      "    next_step = \"KBContextCheck\" if high_confidence else \"ClassifyDomain\"",
      "    ",
      "    rag_step_log(",
      "        step=25,",
      "        step_id=\"RAG.golden.high.confidence.match.score.at.least.0.90\",",
      "        decision=f\"confidence_{\"high\" if high_confidence else \"low\"}\",",
      "        match_score=match_score,",
      "        threshold=0.90,",
      "        has_match=has_match,",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Decision node for golden set match confidence threshold",
    "current_implementation": "Inline confidence check",
    "target_implementation": "app/orchestrators/golden.py - step_25__golden_confidence_decision()",
    "test_requirements": [
      "Checks 0.90 confidence threshold",
      "Routes to KBContextCheck if high confidence",
      "Routes to ClassifyDomain if low confidence"
    ]
  },
  "26": {
    "step": 26,
    "id": "RAG.kb.knowledgesearch.context.topk.fetch.recent.kb.for.changes",
    "node_id": "KBContextCheck",
    "node_label": "KnowledgeSearch.context_topk fetch recent KB for changes",
    "category": "kb",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "KBDelta"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/services/knowledge/knowledge_search.py - If exists",
        "TARGET: app/orchestrators/kb.py - step_26__kb_context_check()",
        "ACTION: Fetch recent KB changes to validate golden answer freshness",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 26: Knowledge base context check",
      "",
      "# 1. Check for KB service:",
      "find app -name '*knowledge*' | head -5",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/kb_step26.py",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "from datetime import datetime, timedelta",
      "",
      "def step_26__kb_context_check(golden_match: dict, canonical_facts: list) -> dict:",
      "    \"\"\"RAG STEP 26: Fetch recent KB for golden answer validation.\"\"\"",
      "    ",
      "    with rag_step_timer(step=26) as timer:",
      "        # Get recent KB updates (last 30 days)",
      "        cutoff_date = datetime.now() - timedelta(days=30)",
      "        ",
      "        # TODO: Query knowledge base for recent changes",
      "        # recent_kb_docs = knowledge_search.get_recent(cutoff_date, limit=10)",
      "        recent_kb_docs = []  # Placeholder",
      "        ",
      "        # Check if any recent docs conflict with golden answer",
      "        potential_conflicts = []",
      "        for doc in recent_kb_docs:",
      "            # Simple keyword overlap check",
      "            # TODO: More sophisticated conflict detection",
      "            if any(fact.get(\"value\", \"\") in doc.get(\"content\", \"\") for fact in canonical_facts):",
      "                potential_conflicts.append(doc)",
      "        ",
      "        rag_step_log(",
      "            step=26,",
      "            step_id=\"RAG.kb.knowledgesearch.context.topk.fetch.recent.kb.for.changes\",",
      "            action=\"kb_context_fetched\",",
      "            recent_docs_count=len(recent_kb_docs),",
      "            potential_conflicts=len(potential_conflicts),",
      "            cutoff_date=cutoff_date.isoformat()",
      "        )",
      "        ",
      "        return {",
      "            \"golden_match\": golden_match,",
      "            \"recent_kb_docs\": recent_kb_docs,",
      "            \"potential_conflicts\": potential_conflicts,",
      "            \"next_step\": \"KBDelta\"",
      "        }",
      "EOF"
    ],
    "transformation_notes": "Fetch recent KB changes to validate golden answer freshness",
    "current_implementation": "app/services/knowledge/knowledge_search.py - If exists",
    "target_implementation": "app/orchestrators/kb.py - step_26__kb_context_check()",
    "test_requirements": [
      "Fetches recent KB documents",
      "Identifies potential conflicts",
      "Returns context for delta analysis"
    ]
  },
  "27": {
    "step": 27,
    "id": "RAG.golden.kb.newer.than.golden.as.of.or.conflicting.tags",
    "node_id": "KBDelta",
    "node_label": "KB newer than Golden as of or conflicting tags?",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [
        "KBContextCheck"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing KB delta analysis",
        "TARGET: app/orchestrators/golden.py - step_27__kb_delta_decision()",
        "ACTION: Decision node to check if KB has newer/conflicting information than golden set",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 27: Decision - KB newer or conflicting with golden?",
      "",
      "cat <<'EOF' > app/orchestrators/golden_step27.py",
      "from app.observability.rag_logging import rag_step_log",
      "from datetime import datetime",
      "",
      "def step_27__kb_delta_decision(context_check: dict) -> str:",
      "    \"\"\"RAG STEP 27: Check if KB is newer or has conflicts.\"\"\"",
      "    ",
      "    golden_match = context_check.get(\"golden_match\", {})",
      "    potential_conflicts = context_check.get(\"potential_conflicts\", [])",
      "    recent_kb_docs = context_check.get(\"recent_kb_docs\", [])",
      "    ",
      "    # Check for conflicts or newer information",
      "    has_conflicts = len(potential_conflicts) > 0",
      "    has_recent_updates = len(recent_kb_docs) > 0",
      "    ",
      "    # Golden answer timestamp (if available)",
      "    golden_timestamp = golden_match.get(\"created_at\")",
      "    kb_is_newer = False",
      "    ",
      "    if golden_timestamp and recent_kb_docs:",
      "        # Check if any KB doc is newer than golden answer",
      "        for doc in recent_kb_docs:",
      "            doc_timestamp = doc.get(\"updated_at\")",
      "            if doc_timestamp and doc_timestamp > golden_timestamp:",
      "                kb_is_newer = True",
      "                break",
      "    ",
      "    # Use LLM path if there are conflicts or newer info",
      "    use_llm_path = has_conflicts or kb_is_newer",
      "    ",
      "    next_step = \"PreContextFromGolden\" if use_llm_path else \"ServeGolden\"",
      "    ",
      "    rag_step_log(",
      "        step=27,",
      "        step_id=\"RAG.golden.kb.newer.than.golden.as.of.or.conflicting.tags\",",
      "        decision=f\"kb_{\"newer_or_conflicting\" if use_llm_path else \"stable\"}\",",
      "        has_conflicts=has_conflicts,",
      "        kb_is_newer=kb_is_newer,",
      "        conflicts_count=len(potential_conflicts),",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Decision node to check if KB has newer/conflicting information than golden set",
    "current_implementation": "Missing KB delta analysis",
    "target_implementation": "app/orchestrators/golden.py - step_27__kb_delta_decision()",
    "test_requirements": [
      "Detects KB conflicts with golden answer",
      "Compares timestamps for freshness",
      "Routes to LLM path or direct golden serve"
    ]
  },
  "28": {
    "step": 28,
    "id": "RAG.golden.serve.golden.answer.with.citations",
    "node_id": "ServeGolden",
    "node_label": "Serve Golden answer with citations",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ReturnComplete"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing golden answer serving",
        "TARGET: app/orchestrators/golden.py - step_28__serve_golden_answer()",
        "ACTION: Format and serve golden set answer with proper citations",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 28: Serve golden answer with citations",
      "",
      "cat <<'EOF' > app/orchestrators/golden_step28.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_28__serve_golden_answer(golden_match: dict) -> dict:",
      "    \"\"\"RAG STEP 28: Format and serve golden answer.\"\"\"",
      "    ",
      "    answer_content = golden_match.get(\"content\", \"\")",
      "    answer_id = golden_match.get(\"id\")",
      "    sources = golden_match.get(\"sources\", [])",
      "    ",
      "    # Format response with citations",
      "    formatted_response = {",
      "        \"content\": answer_content,",
      "        \"type\": \"golden_answer\",",
      "        \"confidence\": golden_match.get(\"match_score\", 1.0),",
      "        \"sources\": sources,",
      "        \"golden_id\": answer_id",
      "    }",
      "    ",
      "    # Add citation markers if sources exist",
      "    if sources:",
      "        citation_text = \"\\n\\n**Sources:**\\n\"",
      "        for i, source in enumerate(sources, 1):",
      "            citation_text += f\"[{i}] {source.get(\\\"title\\\", \\\"Unknown\\\")}\"",
      "            if source.get(\"url\"):",
      "                citation_text += f\" ({source[\\\"url\\\"]})\"",
      "            citation_text += \"\\n\"",
      "        formatted_response[\"content\"] += citation_text",
      "    ",
      "    rag_step_log(",
      "        step=28,",
      "        step_id=\"RAG.golden.serve.golden.answer.with.citations\",",
      "        action=\"golden_answer_served\",",
      "        answer_id=answer_id,",
      "        sources_count=len(sources),",
      "        confidence=golden_match.get(\"match_score\", 1.0)",
      "    )",
      "    ",
      "    return {",
      "        \"response\": formatted_response,",
      "        \"next_step\": \"ReturnComplete\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Format and serve golden set answer with proper citations",
    "current_implementation": "Missing golden answer serving",
    "target_implementation": "app/orchestrators/golden.py - step_28__serve_golden_answer()",
    "test_requirements": [
      "Formats golden answer content",
      "Adds citations and sources",
      "Returns structured response"
    ]
  },
  "29": {
    "step": 29,
    "id": "RAG.facts.contextbuilder.merge.facts.and.kb.docs.and.doc.facts.if.present",
    "node_id": "PreContextFromGolden",
    "node_label": "ContextBuilder.merge facts and KB docs and doc facts if present",
    "category": "facts",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "KBPreFetch"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing context builder service",
        "TARGET: app/orchestrators/facts.py - step_29__merge_context_from_golden()",
        "ACTION: Merge facts, KB docs, and golden context for LLM processing",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 29: Merge context from golden path",
      "",
      "cat <<'EOF' > app/orchestrators/facts_step29.py",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "",
      "def step_29__merge_context_from_golden(context_data: dict) -> dict:",
      "    \"\"\"RAG STEP 29: Merge facts, KB docs, and golden context.\"\"\"",
      "    ",
      "    with rag_step_timer(step=29) as timer:",
      "        canonical_facts = context_data.get(\"canonical_facts\", [])",
      "        recent_kb_docs = context_data.get(\"recent_kb_docs\", [])",
      "        golden_match = context_data.get(\"golden_match\", {})",
      "        doc_facts = context_data.get(\"doc_facts\", [])",
      "        ",
      "        # Build comprehensive context",
      "        merged_context = {",
      "            \"facts\": canonical_facts + doc_facts,",
      "            \"golden_context\": golden_match.get(\"content\", \"\"),",
      "            \"kb_updates\": [doc.get(\"content\", \"\") for doc in recent_kb_docs[:5]],",
      "            \"source_priority\": \"golden_with_kb_validation\"",
      "        }",
      "        ",
      "        # Create context string for LLM",
      "        context_str = f\"\"\"",
      "Golden Answer Context:",
      "{merged_context['golden_context']}",
      "",
      "Recent Knowledge Base Updates:",
      "{chr(10).join(merged_context['kb_updates'])}",
      "",
      "Extracted Facts:",
      "{', '.join([f\"{f.get('type', '')}: {f.get('value', '')}\" for f in merged_context['facts']])}",
      "        \"\"\"",
      "        ",
      "        rag_step_log(",
      "            step=29,",
      "            step_id=\"RAG.facts.contextbuilder.merge.facts.and.kb.docs.and.doc.facts.if.present\",",
      "            action=\"context_merged_from_golden\",",
      "            facts_count=len(merged_context['facts']),",
      "            kb_docs_count=len(recent_kb_docs),",
      "            has_golden_context=bool(merged_context['golden_context'])",
      "        )",
      "        ",
      "        return {",
      "            \"merged_context\": merged_context,",
      "            \"context_string\": context_str,",
      "            \"next_step\": \"KBPreFetch\"",
      "        }",
      "EOF"
    ],
    "transformation_notes": "Merge facts, KB docs, and golden context for LLM processing",
    "current_implementation": "Missing context builder service",
    "target_implementation": "app/orchestrators/facts.py - step_29__merge_context_from_golden()",
    "test_requirements": [
      "Merges facts from multiple sources",
      "Combines golden and KB context",
      "Creates formatted context string"
    ]
  },
  "30": {
    "step": 30,
    "id": "RAG.response.return.chatresponse",
    "node_id": "ReturnComplete",
    "node_label": "Return ChatResponse",
    "category": "response",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ServeGolden"
      ],
      "outgoing": [
        "CollectMetrics"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/api/v1/chat.py - Return statement in endpoint",
        "TARGET: app/orchestrators/response.py - step_30__return_chat_response()",
        "ACTION: Format final chat response with proper structure",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 30: Return final chat response",
      "",
      "cat <<'EOF' > app/orchestrators/response_step30.py",
      "from app.observability.rag_logging import rag_step_log",
      "from app.schemas.chat import ChatResponse",
      "",
      "def step_30__return_chat_response(response_data: dict) -> ChatResponse:",
      "    \"\"\"RAG STEP 30: Format and return final chat response.\"\"\"",
      "    ",
      "    response_content = response_data.get(\"response\", {})",
      "    response_type = response_content.get(\"type\", \"standard\")",
      "    ",
      "    # Create structured response",
      "    chat_response = ChatResponse(",
      "        content=response_content.get(\"content\", \"\"),",
      "        type=response_type,",
      "        sources=response_content.get(\"sources\", []),",
      "        confidence=response_content.get(\"confidence\", 1.0),",
      "        processing_time=response_data.get(\"processing_time\")",
      "    )",
      "    ",
      "    rag_step_log(",
      "        step=30,",
      "        step_id=\"RAG.response.return.chatresponse\",",
      "        action=\"chat_response_returned\",",
      "        response_type=response_type,",
      "        content_length=len(response_content.get(\"content\", \"\")),",
      "        sources_count=len(response_content.get(\"sources\", []))",
      "    )",
      "    ",
      "    return chat_response",
      "EOF"
    ],
    "transformation_notes": "Format final chat response with proper structure",
    "current_implementation": "app/api/v1/chat.py - Return statement in endpoint",
    "target_implementation": "app/orchestrators/response.py - step_30__return_chat_response()",
    "test_requirements": [
      "Creates ChatResponse object",
      "Includes all response metadata",
      "Logs final response details"
    ]
  },
  "31": {
    "step": 31,
    "id": "RAG.classify.domainactionclassifier.classify.rule.based.classification",
    "node_id": "ClassifyDomain",
    "node_label": "DomainActionClassifier.classify Rule-based classification",
    "category": "classify",
    "type": "process",
    "neighbors": {
      "incoming": [
        "RequireDocIngest"
      ],
      "outgoing": [
        "CalcScores"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/services/domain_action_classifier.py:40-120 - classify() method with internal routing",
        "TARGET: app/orchestrators/classify.py - step_31__rule_based_classification()",
        "ACTION: Extract rule-based classification logic from DomainActionClassifier service",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "collapsed_into_service",
    "claude_code_instructions": [
      "# STEP 31: Extract rule-based classification",
      "# Current: Embedded in DomainActionClassifier.classify() method",
      "# Target: Explicit rule-based classification orchestrator",
      "",
      "# 1. Examine current classifier:",
      "cat app/services/domain_action_classifier.py | grep -A 30 'def classify'",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/classify_step31.py",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "from app.services.domain_action_classifier import DomainActionClassifier",
      "",
      "def step_31__rule_based_classification(user_message: str) -> dict:",
      "    \"\"\"RAG STEP 31: Rule-based domain/action classification.\"\"\"",
      "    ",
      "    with rag_step_timer(step=31) as timer:",
      "        classifier = DomainActionClassifier()",
      "        ",
      "        # Extract rule-based logic from classifier.classify()",
      "        # TODO: This should call ONLY the rule-based part",
      "        classification_result = classifier._rule_based_classify(user_message)",
      "        ",
      "        # Extract domain and action",
      "        domain = classification_result.get(\"domain\")",
      "        action = classification_result.get(\"action\")",
      "        confidence = classification_result.get(\"confidence\", 0.0)",
      "        ",
      "        rag_step_log(",
      "            step=31,",
      "            step_id=\"RAG.classify.domainactionclassifier.classify.rule.based.classification\",",
      "            action=\"rule_based_classification\",",
      "            domain=domain,",
      "            action_type=action,",
      "            confidence=confidence,",
      "            method=\"rule_based\"",
      "        )",
      "        ",
      "        return {",
      "            \"classification\": classification_result,",
      "            \"user_message\": user_message,",
      "            \"next_step\": \"CalcScores\"",
      "        }",
      "",
      "# TODO: Extract _rule_based_classify from DomainActionClassifier",
      "# Or create wrapper that only calls rule-based logic",
      "EOF"
    ],
    "transformation_notes": "Extract rule-based classification logic from DomainActionClassifier service",
    "current_implementation": "app/services/domain_action_classifier.py:40-120 - classify() method with internal routing",
    "target_implementation": "app/orchestrators/classify.py - step_31__rule_based_classification()",
    "test_requirements": [
      "Calls only rule-based classification logic",
      "Returns domain and action with confidence",
      "Does not use LLM fallback"
    ]
  },
  "32": {
    "step": 32,
    "id": "RAG.classify.calculate.domain.and.action.scores.match.italian.keywords",
    "node_id": "CalcScores",
    "node_label": "Calculate domain and action scores Match Italian keywords",
    "category": "classify",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ClassifyDomain"
      ],
      "outgoing": [
        "ConfidenceCheck"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/services/domain_action_classifier.py:60-90 - Internal scoring logic",
        "TARGET: app/orchestrators/classify.py - step_32__calculate_scores()",
        "ACTION: Extract keyword matching and scoring logic into explicit orchestrator",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "collapsed_into_service",
    "claude_code_instructions": [
      "# STEP 32: Calculate domain/action scores with Italian keywords",
      "# Current: Internal scoring in DomainActionClassifier",
      "# Target: Explicit scoring orchestrator",
      "",
      "cat <<'EOF' > app/orchestrators/classify_step32.py",
      "from app.observability.rag_logging import rag_step_log",
      "import re",
      "",
      "def step_32__calculate_scores(classification_data: dict) -> dict:",
      "    \"\"\"RAG STEP 32: Calculate domain/action scores with Italian keywords.\"\"\"",
      "    ",
      "    user_message = classification_data.get(\"user_message\", \"\")",
      "    classification = classification_data.get(\"classification\", {})",
      "    ",
      "    # Italian keyword mappings (extract from classifier)",
      "    domain_keywords = {",
      "        \"fiscal\": [\"fattura\", \"iva\", \"tasse\", \"codice fiscale\", \"partita iva\"],",
      "        \"legal\": [\"contratto\", \"clausola\", \"accordo\", \"diritto\", \"legge\"],",
      "        \"hr\": [\"stipendio\", \"busta paga\", \"ferie\", \"permessi\", \"tfr\"],",
      "        \"accounting\": [\"bilancio\", \"contabilit\u00e0\", \"registro\", \"movimento\"]",
      "    }",
      "    ",
      "    action_keywords = {",
      "        \"calculate\": [\"calcola\", \"quanto\", \"importo\", \"somma\"],",
      "        \"verify\": [\"verifica\", \"controlla\", \"\u00e8 corretto\", \"giusto\"],",
      "        \"explain\": [\"spiega\", \"cosa significa\", \"come funziona\"],",
      "        \"extract\": [\"estrai\", \"trova\", \"cerca\", \"mostra\"]",
      "    }",
      "    ",
      "    # Score calculation",
      "    domain_scores = {}",
      "    for domain, keywords in domain_keywords.items():",
      "        score = sum(1 for kw in keywords if kw.lower() in user_message.lower())",
      "        domain_scores[domain] = score / len(keywords) if keywords else 0",
      "    ",
      "    action_scores = {}",
      "    for action, keywords in action_keywords.items():",
      "        score = sum(1 for kw in keywords if kw.lower() in user_message.lower())",
      "        action_scores[action] = score / len(keywords) if keywords else 0",
      "    ",
      "    # Best matches",
      "    best_domain = max(domain_scores.items(), key=lambda x: x[1]) if domain_scores else (None, 0)",
      "    best_action = max(action_scores.items(), key=lambda x: x[1]) if action_scores else (None, 0)",
      "    ",
      "    # Overall confidence",
      "    overall_confidence = (best_domain[1] + best_action[1]) / 2",
      "    ",
      "    enhanced_classification = {",
      "        **classification,",
      "        \"domain\": best_domain[0],",
      "        \"action\": best_action[0],",
      "        \"confidence\": overall_confidence,",
      "        \"domain_scores\": domain_scores,",
      "        \"action_scores\": action_scores",
      "    }",
      "    ",
      "    rag_step_log(",
      "        step=32,",
      "        step_id=\"RAG.classify.calculate.domain.and.action.scores.match.italian.keywords\",",
      "        action=\"scores_calculated\",",
      "        best_domain=best_domain[0],",
      "        best_action=best_action[0],",
      "        overall_confidence=overall_confidence,",
      "        domain_scores=domain_scores",
      "    )",
      "    ",
      "    return {",
      "        \"classification\": enhanced_classification,",
      "        \"user_message\": user_message,",
      "        \"next_step\": \"ConfidenceCheck\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Extract keyword matching and scoring logic into explicit orchestrator",
    "current_implementation": "app/services/domain_action_classifier.py:60-90 - Internal scoring logic",
    "target_implementation": "app/orchestrators/classify.py - step_32__calculate_scores()",
    "test_requirements": [
      "Matches Italian keywords for domains",
      "Calculates confidence scores",
      "Returns best domain/action with scores"
    ]
  },
  "33": {
    "step": 33,
    "id": "RAG.classify.confidence.at.least.threshold",
    "node_id": "ConfidenceCheck",
    "node_label": "Confidence at least threshold?",
    "category": "classify",
    "type": "process",
    "neighbors": {
      "incoming": [
        "CalcScores"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/services/domain_action_classifier.py:100-110 - Inline confidence check",
        "TARGET: app/orchestrators/classify.py - step_33__confidence_threshold_decision()",
        "ACTION: Extract confidence threshold decision into explicit decision node",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "collapsed_into_service",
    "claude_code_instructions": [
      "# STEP 33: Decision - confidence meets threshold?",
      "",
      "cat <<'EOF' > app/orchestrators/classify_step33.py",
      "from app.observability.rag_logging import rag_step_log",
      "from app.core.config import settings",
      "",
      "def step_33__confidence_threshold_decision(classification_data: dict) -> str:",
      "    \"\"\"RAG STEP 33: Check if classification confidence meets threshold.\"\"\"",
      "    ",
      "    classification = classification_data.get(\"classification\", {})",
      "    confidence = classification.get(\"confidence\", 0.0)",
      "    ",
      "    # Get threshold from config (default 0.6)",
      "    threshold = getattr(settings, 'CLASSIFICATION_CONFIDENCE_THRESHOLD', 0.6)",
      "    meets_threshold = confidence >= threshold",
      "    ",
      "    next_step = \"TrackMetrics\" if meets_threshold else \"LLMFallback\"",
      "    ",
      "    rag_step_log(",
      "        step=33,",
      "        step_id=\"RAG.classify.confidence.at.least.threshold\",",
      "        decision=f\"confidence_{\"sufficient\" if meets_threshold else \"insufficient\"}\",",
      "        confidence=confidence,",
      "        threshold=threshold,",
      "        meets_threshold=meets_threshold,",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Extract confidence threshold decision into explicit decision node",
    "current_implementation": "app/services/domain_action_classifier.py:100-110 - Inline confidence check",
    "target_implementation": "app/orchestrators/classify.py - step_33__confidence_threshold_decision()",
    "test_requirements": [
      "Checks configurable confidence threshold",
      "Routes to TrackMetrics if sufficient",
      "Routes to LLMFallback if insufficient"
    ]
  },
  "34": {
    "step": 34,
    "id": "RAG.metrics.classificationmetrics.track.record.metrics",
    "node_id": "TrackMetrics",
    "node_label": "ClassificationMetrics.track Record metrics",
    "category": "metrics",
    "type": "process",
    "neighbors": {
      "incoming": [
        "UseLLM",
        "UseRuleBased"
      ],
      "outgoing": [
        "KBPreFetch"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing classification metrics service",
        "TARGET: app/orchestrators/metrics.py - step_34__track_classification_metrics()",
        "ACTION: Implement classification metrics tracking orchestrator",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 34: Track classification metrics",
      "",
      "cat <<'EOF' > app/orchestrators/metrics_step34.py",
      "from app.observability.rag_logging import rag_step_log",
      "from datetime import datetime",
      "",
      "def step_34__track_classification_metrics(classification_data: dict) -> dict:",
      "    \"\"\"RAG STEP 34: Record classification metrics.\"\"\"",
      "    ",
      "    classification = classification_data.get(\"classification\", {})",
      "    user_message = classification_data.get(\"user_message\", \"\")",
      "    ",
      "    # Create metrics record",
      "    metrics = {",
      "        \"timestamp\": datetime.utcnow().isoformat(),",
      "        \"domain\": classification.get(\"domain\"),",
      "        \"action\": classification.get(\"action\"),",
      "        \"confidence\": classification.get(\"confidence\", 0.0),",
      "        \"method\": \"rule_based\",",
      "        \"message_length\": len(user_message),",
      "        \"domain_scores\": classification.get(\"domain_scores\", {}),",
      "        \"action_scores\": classification.get(\"action_scores\", {})",
      "    }",
      "    ",
      "    # TODO: Store in metrics database/service",
      "    # metrics_service.record_classification(metrics)",
      "    ",
      "    rag_step_log(",
      "        step=34,",
      "        step_id=\"RAG.metrics.classificationmetrics.track.record.metrics\",",
      "        action=\"metrics_recorded\",",
      "        **metrics",
      "    )",
      "    ",
      "    return {",
      "        \"classification\": classification,",
      "        \"metrics\": metrics,",
      "        \"next_step\": \"KBPreFetch\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Implement classification metrics tracking orchestrator",
    "current_implementation": "Missing classification metrics service",
    "target_implementation": "app/orchestrators/metrics.py - step_34__track_classification_metrics()",
    "test_requirements": [
      "Records classification metrics",
      "Includes domain/action scores",
      "Tracks confidence and method"
    ]
  },
  "35": {
    "step": 35,
    "id": "RAG.classify.domainactionclassifier.llm.fallback.use.llm.classification",
    "node_id": "LLMFallback",
    "node_label": "DomainActionClassifier._llm_fallback Use LLM classification",
    "category": "classify",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "LLMBetter"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/services/domain_action_classifier.py:150-200 - _llm_fallback() method",
        "TARGET: app/orchestrators/classify.py - step_35__llm_fallback_classification()",
        "ACTION: Extract LLM fallback classification logic from DomainActionClassifier",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "collapsed_into_service",
    "claude_code_instructions": [
      "# STEP 35: Extract LLM fallback classification",
      "# Current: _llm_fallback() method in DomainActionClassifier",
      "# Target: Explicit LLM classification orchestrator",
      "",
      "# 1. Examine current LLM fallback:",
      "cat app/services/domain_action_classifier.py | grep -A 20 '_llm_fallback'",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/classify_step35.py",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "from app.services.domain_action_classifier import DomainActionClassifier",
      "",
      "def step_35__llm_fallback_classification(classification_data: dict) -> dict:",
      "    \"\"\"RAG STEP 35: LLM-based classification fallback.\"\"\"",
      "    ",
      "    with rag_step_timer(step=35) as timer:",
      "        user_message = classification_data.get(\"user_message\", \"\")",
      "        rule_based_result = classification_data.get(\"classification\", {})",
      "        ",
      "        classifier = DomainActionClassifier()",
      "        ",
      "        # Extract LLM logic from _llm_fallback method",
      "        # TODO: This should call ONLY the LLM part",
      "        llm_result = classifier._llm_only_classify(user_message)",
      "        ",
      "        rag_step_log(",
      "            step=35,",
      "            step_id=\"RAG.classify.domainactionclassifier.llm.fallback.use.llm.classification\",",
      "            action=\"llm_fallback_classification\",",
      "            domain=llm_result.get(\"domain\"),",
      "            action_type=llm_result.get(\"action\"),",
      "            confidence=llm_result.get(\"confidence\", 0.0),",
      "            method=\"llm_fallback\",",
      "            rule_based_confidence=rule_based_result.get(\"confidence\", 0.0)",
      "        )",
      "        ",
      "        return {",
      "            \"rule_based_classification\": rule_based_result,",
      "            \"llm_classification\": llm_result,",
      "            \"user_message\": user_message,",
      "            \"next_step\": \"LLMBetter\"",
      "        }",
      "",
      "# TODO: Extract _llm_only_classify from DomainActionClassifier",
      "# Or create wrapper that only calls LLM logic",
      "EOF"
    ],
    "transformation_notes": "Extract LLM fallback classification logic from DomainActionClassifier",
    "current_implementation": "app/services/domain_action_classifier.py:150-200 - _llm_fallback() method",
    "target_implementation": "app/orchestrators/classify.py - step_35__llm_fallback_classification()",
    "test_requirements": [
      "Uses LLM for classification",
      "Preserves rule-based result for comparison",
      "Returns both classifications for decision"
    ]
  },
  "36": {
    "step": 36,
    "id": "RAG.llm.llm.better.than.rule.based",
    "node_id": "LLMBetter",
    "node_label": "LLM better than rule-based?",
    "category": "llm",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "LLMFallback"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/services/domain_action_classifier.py:200-220 - Internal comparison logic",
        "TARGET: app/orchestrators/classify.py - step_36__llm_better_decision()",
        "ACTION: Extract LLM vs rule-based comparison decision from classifier service",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "collapsed_into_service",
    "claude_code_instructions": [
      "# STEP 36: Decision - LLM better than rule-based?",
      "# Current: Internal comparison in DomainActionClassifier",
      "# Target: Explicit decision orchestrator",
      "",
      "cat <<'EOF' > app/orchestrators/classify_step36.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_36__llm_better_decision(classification_comparison: dict) -> str:",
      "    \"\"\"RAG STEP 36: Decision - is LLM classification better?\"\"\"",
      "    ",
      "    rule_based = classification_comparison.get(\"rule_based_classification\", {})",
      "    llm_based = classification_comparison.get(\"llm_classification\", {})",
      "    ",
      "    rule_confidence = rule_based.get(\"confidence\", 0.0)",
      "    llm_confidence = llm_based.get(\"confidence\", 0.0)",
      "    ",
      "    # Decision criteria (extract from classifier logic):",
      "    # 1. LLM confidence significantly higher (>0.2 difference)",
      "    # 2. Rule-based confidence very low (<0.3)",
      "    confidence_diff = llm_confidence - rule_confidence",
      "    llm_significantly_better = confidence_diff > 0.2",
      "    rule_very_low = rule_confidence < 0.3",
      "    ",
      "    use_llm = llm_significantly_better or rule_very_low",
      "    ",
      "    next_step = \"UseLLM\" if use_llm else \"UseRuleBased\"",
      "    ",
      "    rag_step_log(",
      "        step=36,",
      "        step_id=\"RAG.llm.llm.better.than.rule.based\",",
      "        decision=f\"llm_{\"better\" if use_llm else \"worse\"}\",",
      "        rule_confidence=rule_confidence,",
      "        llm_confidence=llm_confidence,",
      "        confidence_diff=confidence_diff,",
      "        llm_significantly_better=llm_significantly_better,",
      "        rule_very_low=rule_very_low,",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Extract LLM vs rule-based comparison decision from classifier service",
    "current_implementation": "app/services/domain_action_classifier.py:200-220 - Internal comparison logic",
    "target_implementation": "app/orchestrators/classify.py - step_36__llm_better_decision()",
    "test_requirements": [
      "Compares rule-based vs LLM confidence",
      "Uses configurable decision criteria",
      "Routes to UseLLM or UseRuleBased"
    ]
  },
  "37": {
    "step": 37,
    "id": "RAG.llm.use.llm.classification",
    "node_id": "UseLLM",
    "node_label": "Use LLM classification",
    "category": "llm",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "TrackMetrics"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Inline selection of LLM result",
        "TARGET: app/orchestrators/classify.py - step_37__use_llm_classification()",
        "ACTION: Finalize LLM classification as the chosen result",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "inline_logic",
    "claude_code_instructions": [
      "# STEP 37: Use LLM classification result",
      "",
      "cat <<'EOF' > app/orchestrators/classify_step37.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_37__use_llm_classification(classification_comparison: dict) -> dict:",
      "    \"\"\"RAG STEP 37: Finalize LLM classification as chosen result.\"\"\"",
      "    ",
      "    llm_classification = classification_comparison.get(\"llm_classification\", {})",
      "    user_message = classification_comparison.get(\"user_message\", \"\")",
      "    ",
      "    # Select LLM result as final classification",
      "    final_classification = {",
      "        **llm_classification,",
      "        \"method\": \"llm\",",
      "        \"chosen_over\": \"rule_based\"",
      "    }",
      "    ",
      "    rag_step_log(",
      "        step=37,",
      "        step_id=\"RAG.llm.use.llm.classification\",",
      "        action=\"llm_classification_selected\",",
      "        domain=final_classification.get(\"domain\"),",
      "        action_type=final_classification.get(\"action\"),",
      "        confidence=final_classification.get(\"confidence\", 0.0),",
      "        method=\"llm\"",
      "    )",
      "    ",
      "    return {",
      "        \"classification\": final_classification,",
      "        \"user_message\": user_message,",
      "        \"next_step\": \"TrackMetrics\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Finalize LLM classification as the chosen result",
    "current_implementation": "Inline selection of LLM result",
    "target_implementation": "app/orchestrators/classify.py - step_37__use_llm_classification()",
    "test_requirements": [
      "Selects LLM classification result",
      "Marks classification method as LLM",
      "Routes to TrackMetrics"
    ]
  },
  "38": {
    "step": 38,
    "id": "RAG.platform.use.rule.based.classification",
    "node_id": "UseRuleBased",
    "node_label": "Use rule-based classification",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "TrackMetrics"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Inline selection of rule-based result",
        "TARGET: app/orchestrators/classify.py - step_38__use_rule_based_classification()",
        "ACTION: Finalize rule-based classification as the chosen result",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "inline_logic",
    "claude_code_instructions": [
      "# STEP 38: Use rule-based classification result",
      "",
      "cat <<'EOF' > app/orchestrators/classify_step38.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_38__use_rule_based_classification(classification_comparison: dict) -> dict:",
      "    \"\"\"RAG STEP 38: Finalize rule-based classification as chosen result.\"\"\"",
      "    ",
      "    rule_based_classification = classification_comparison.get(\"rule_based_classification\", {})",
      "    user_message = classification_comparison.get(\"user_message\", \"\")",
      "    ",
      "    # Select rule-based result as final classification",
      "    final_classification = {",
      "        **rule_based_classification,",
      "        \"method\": \"rule_based\",",
      "        \"chosen_over\": \"llm\"",
      "    }",
      "    ",
      "    rag_step_log(",
      "        step=38,",
      "        step_id=\"RAG.platform.use.rule.based.classification\",",
      "        action=\"rule_based_classification_selected\",",
      "        domain=final_classification.get(\"domain\"),",
      "        action_type=final_classification.get(\"action\"),",
      "        confidence=final_classification.get(\"confidence\", 0.0),",
      "        method=\"rule_based\"",
      "    )",
      "    ",
      "    return {",
      "        \"classification\": final_classification,",
      "        \"user_message\": user_message,",
      "        \"next_step\": \"TrackMetrics\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Finalize rule-based classification as the chosen result",
    "current_implementation": "Inline selection of rule-based result",
    "target_implementation": "app/orchestrators/classify.py - step_38__use_rule_based_classification()",
    "test_requirements": [
      "Selects rule-based classification result",
      "Marks classification method as rule-based",
      "Routes to TrackMetrics"
    ]
  },
  "39": {
    "step": 39,
    "id": "RAG.preflight.knowledgesearch.retrieve.topk.bm25.and.vectors.and.recency.boost",
    "node_id": "KBPreFetch",
    "node_label": "KnowledgeSearch.retrieve_topk BM25 and vectors and recency boost",
    "category": "preflight",
    "type": "process",
    "neighbors": {
      "incoming": [
        "TrackMetrics",
        "PreContextFromGolden"
      ],
      "outgoing": [
        "BuildContext"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/services/knowledge/knowledge_search.py - retrieve_topk() method if exists",
        "TARGET: app/orchestrators/kb.py - step_39__kb_retrieve_topk()",
        "ACTION: Wrap knowledge search service with explicit orchestrator",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 39: Knowledge base retrieval with BM25 and vectors",
      "",
      "# 1. Check for KB search service:",
      "find app -name '*knowledge*' -o -name '*search*' | head -5",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/kb_step39.py",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "from typing import List, Dict",
      "",
      "def step_39__kb_retrieve_topk(classification_data: dict) -> dict:",
      "    \"\"\"RAG STEP 39: Retrieve top-k KB docs with BM25 and vectors.\"\"\"",
      "    ",
      "    with rag_step_timer(step=39) as timer:",
      "        user_message = classification_data.get(\"user_message\", \"\")",
      "        classification = classification_data.get(\"classification\", {})",
      "        canonical_facts = classification_data.get(\"canonical_facts\", [])",
      "        ",
      "        # Prepare search query",
      "        search_terms = [user_message]",
      "        if canonical_facts:",
      "            fact_terms = [f.get(\"value\", \"\") for f in canonical_facts if f.get(\"type\") != \"date\"]",
      "            search_terms.extend(fact_terms)",
      "        ",
      "        query = \" \".join(search_terms)",
      "        ",
      "        # TODO: Call actual knowledge search service",
      "        # from app.services.knowledge.knowledge_search import KnowledgeSearch",
      "        # kb_service = KnowledgeSearch()",
      "        # results = kb_service.retrieve_topk(",
      "        #     query=query,",
      "        #     domain=classification.get('domain'),",
      "        #     k=10,",
      "        #     use_bm25=True,",
      "        #     use_vectors=True,",
      "        #     recency_boost=True",
      "        # )",
      "        ",
      "        # Placeholder results",
      "        results = []",
      "        ",
      "        rag_step_log(",
      "            step=39,",
      "            step_id=\"RAG.preflight.knowledgesearch.retrieve.topk.bm25.and.vectors.and.recency.boost\",",
      "            action=\"kb_retrieval_completed\",",
      "            query=query[:100],",
      "            domain=classification.get(\"domain\"),",
      "            results_count=len(results),",
      "            search_methods=[\"bm25\", \"vectors\", \"recency_boost\"]",
      "        )",
      "        ",
      "        return {",
      "            \"kb_results\": results,",
      "            \"search_query\": query,",
      "            \"classification\": classification,",
      "            \"canonical_facts\": canonical_facts,",
      "            \"next_step\": \"BuildContext\"",
      "        }",
      "EOF"
    ],
    "transformation_notes": "Wrap knowledge search service with explicit orchestrator",
    "current_implementation": "app/services/knowledge/knowledge_search.py - retrieve_topk() method if exists",
    "target_implementation": "app/orchestrators/kb.py - step_39__kb_retrieve_topk()",
    "test_requirements": [
      "Constructs search query from message and facts",
      "Uses domain-aware knowledge search",
      "Returns ranked KB results"
    ]
  },
  "40": {
    "step": 40,
    "id": "RAG.facts.contextbuilder.merge.facts.and.kb.docs.and.optional.doc.facts",
    "node_id": "BuildContext",
    "node_label": "ContextBuilder.merge facts and KB docs and optional doc facts",
    "category": "facts",
    "type": "process",
    "neighbors": {
      "incoming": [
        "KBPreFetch"
      ],
      "outgoing": [
        "SelectPrompt"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing context builder service",
        "TARGET: app/orchestrators/facts.py - step_40__build_context()",
        "ACTION: Implement context building by merging facts, KB docs, and optional document facts",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 40: Build comprehensive context from all sources",
      "",
      "cat <<'EOF' > app/orchestrators/facts_step40.py",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "",
      "def step_40__build_context(retrieval_data: dict) -> dict:",
      "    \"\"\"RAG STEP 40: Merge facts, KB docs, and optional doc facts.\"\"\"",
      "    ",
      "    with rag_step_timer(step=40) as timer:",
      "        kb_results = retrieval_data.get(\"kb_results\", [])",
      "        canonical_facts = retrieval_data.get(\"canonical_facts\", [])",
      "        classification = retrieval_data.get(\"classification\", {})",
      "        doc_facts = retrieval_data.get(\"doc_facts\", [])",
      "        ",
      "        # Build comprehensive context",
      "        context_sections = []",
      "        ",
      "        # 1. Classification context",
      "        if classification.get(\"domain\"):",
      "            context_sections.append(f\"Domain: {classification[\\\"domain\\\"]}\")",
      "        if classification.get(\"action\"):",
      "            context_sections.append(f\"Action: {classification[\\\"action\\\"]}\")",
      "        ",
      "        # 2. Extracted facts",
      "        if canonical_facts:",
      "            facts_text = \"Extracted Facts:\\n\"",
      "            for fact in canonical_facts:",
      "                facts_text += f\"- {fact.get(\\\"type\\\", \\\"unknown\\\")}: {fact.get(\\\"value\\\", \\\"\\\")}\\n\"",
      "            context_sections.append(facts_text)",
      "        ",
      "        # 3. Document facts (if any)",
      "        if doc_facts:",
      "            doc_text = \"Document Facts:\\n\"",
      "            for fact in doc_facts:",
      "                doc_text += f\"- {fact.get(\\\"type\\\", \\\"unknown\\\")}: {fact.get(\\\"value\\\", \\\"\\\")}\\n\"",
      "            context_sections.append(doc_text)",
      "        ",
      "        # 4. Knowledge base context",
      "        if kb_results:",
      "            kb_text = \"Relevant Knowledge:\\n\"",
      "            for i, result in enumerate(kb_results[:5], 1):",
      "                content = result.get(\"content\", \"\")[:200]",
      "                kb_text += f\"{i}. {content}...\\n\"",
      "            context_sections.append(kb_text)",
      "        ",
      "        # Combine all sections",
      "        full_context = \"\\n\\n\".join(context_sections)",
      "        ",
      "        context_metadata = {",
      "            \"facts_count\": len(canonical_facts),",
      "            \"doc_facts_count\": len(doc_facts),",
      "            \"kb_results_count\": len(kb_results),",
      "            \"domain\": classification.get(\"domain\"),",
      "            \"confidence\": classification.get(\"confidence\", 0.0)",
      "        }",
      "        ",
      "        rag_step_log(",
      "            step=40,",
      "            step_id=\"RAG.facts.contextbuilder.merge.facts.and.kb.docs.and.optional.doc.facts\",",
      "            action=\"context_built\",",
      "            **context_metadata",
      "        )",
      "        ",
      "        return {",
      "            \"context\": full_context,",
      "            \"context_metadata\": context_metadata,",
      "            \"classification\": classification,",
      "            \"next_step\": \"SelectPrompt\"",
      "        }",
      "EOF"
    ],
    "transformation_notes": "Implement context building by merging facts, KB docs, and optional document facts",
    "current_implementation": "Missing context builder service",
    "target_implementation": "app/orchestrators/facts.py - step_40__build_context()",
    "test_requirements": [
      "Merges facts from all sources",
      "Includes classification context",
      "Formats KB results appropriately"
    ]
  },
  "41": {
    "step": 41,
    "id": "RAG.prompting.langgraphagent.get.system.prompt.select.appropriate.prompt",
    "node_id": "SelectPrompt",
    "node_label": "LangGraphAgent._get_system_prompt Select appropriate prompt",
    "category": "prompting",
    "type": "process",
    "neighbors": {
      "incoming": [
        "BuildContext",
        "DefaultPrompt"
      ],
      "outgoing": [
        "ClassConfidence"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/langgraph/graph.py:406-450 - _get_system_prompt() method with internal logic",
        "TARGET: app/orchestrators/prompting.py - step_41__select_system_prompt()",
        "ACTION: Extract system prompt selection logic from LangGraphAgent into explicit orchestrator",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "collapsed_into_agent",
    "claude_code_instructions": [
      "# STEP 41: Extract system prompt selection",
      "# Current: Embedded in LangGraphAgent._get_system_prompt()",
      "# Target: Explicit prompt selection orchestrator",
      "",
      "# 1. Examine current prompt selection:",
      "cat app/core/langgraph/graph.py | grep -A 40 '_get_system_prompt'",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/prompting_step41.py",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "from app.core.langgraph.graph import LangGraphAgent",
      "",
      "def step_41__select_system_prompt(context_data: dict) -> dict:",
      "    \"\"\"RAG STEP 41: Select appropriate system prompt.\"\"\"",
      "    ",
      "    with rag_step_timer(step=41) as timer:",
      "        classification = context_data.get(\"classification\", {})",
      "        context = context_data.get(\"context\", \"\")",
      "        messages = context_data.get(\"messages\", [])",
      "        ",
      "        # Extract prompt selection logic from LangGraphAgent",
      "        # This should be the orchestrator that delegates to the agent",
      "        agent = LangGraphAgent()",
      "        ",
      "        # TODO: Extract ONLY the selection logic, not the full method",
      "        # system_prompt = agent._select_prompt_only(classification, context)",
      "        ",
      "        # For now, delegate to existing method (needs refactoring)",
      "        system_prompt = agent._get_system_prompt(messages, classification)",
      "        ",
      "        rag_step_log(",
      "            step=41,",
      "            step_id=\"RAG.prompting.langgraphagent.get.system.prompt.select.appropriate.prompt\",",
      "            action=\"system_prompt_selected\",",
      "            domain=classification.get(\"domain\"),",
      "            confidence=classification.get(\"confidence\", 0.0),",
      "            prompt_length=len(system_prompt) if system_prompt else 0",
      "        )",
      "        ",
      "        return {",
      "            \"system_prompt\": system_prompt,",
      "            \"classification\": classification,",
      "            \"context\": context,",
      "            \"messages\": messages,",
      "            \"next_step\": \"ClassConfidence\"",
      "        }",
      "",
      "# TODO: Refactor LangGraphAgent._get_system_prompt to separate",
      "# selection logic from message preparation",
      "EOF"
    ],
    "transformation_notes": "Extract system prompt selection logic from LangGraphAgent into explicit orchestrator",
    "current_implementation": "app/core/langgraph/graph.py:406-450 - _get_system_prompt() method with internal logic",
    "target_implementation": "app/orchestrators/prompting.py - step_41__select_system_prompt()",
    "test_requirements": [
      "Selects appropriate system prompt",
      "Uses classification context",
      "Returns prompt for message preparation"
    ]
  },
  "42": {
    "step": 42,
    "id": "RAG.classify.classification.exists.and.confidence.at.least.0.6",
    "node_id": "ClassConfidence",
    "node_label": "Classification exists and confidence at least 0.6?",
    "category": "classify",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "SelectPrompt"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/langgraph/graph.py:420-430 - Inline confidence check in _get_system_prompt",
        "TARGET: app/orchestrators/prompting.py - step_42__classification_confidence_decision()",
        "ACTION: Extract classification confidence decision from prompt selection method",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "collapsed_into_agent",
    "claude_code_instructions": [
      "# STEP 42: Decision - classification confidence sufficient for domain prompt?",
      "# Current: Inline check in _get_system_prompt",
      "# Target: Explicit decision orchestrator",
      "",
      "cat <<'EOF' > app/orchestrators/prompting_step42.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_42__classification_confidence_decision(prompt_data: dict) -> str:",
      "    \"\"\"RAG STEP 42: Check if classification meets confidence for domain prompt.\"\"\"",
      "    ",
      "    classification = prompt_data.get(\"classification\", {})",
      "    confidence = classification.get(\"confidence\", 0.0)",
      "    domain = classification.get(\"domain\")",
      "    ",
      "    # Decision criteria (extract from _get_system_prompt logic)",
      "    has_classification = domain is not None",
      "    confidence_threshold = 0.6  # From step description",
      "    sufficient_confidence = confidence >= confidence_threshold",
      "    ",
      "    use_domain_prompt = has_classification and sufficient_confidence",
      "    ",
      "    next_step = \"DomainPrompt\" if use_domain_prompt else \"DefaultSysPrompt\"",
      "    ",
      "    rag_step_log(",
      "        step=42,",
      "        step_id=\"RAG.classify.classification.exists.and.confidence.at.least.0.6\",",
      "        decision=f\"confidence_{\"sufficient\" if use_domain_prompt else \"insufficient\"}\",",
      "        has_classification=has_classification,",
      "        confidence=confidence,",
      "        threshold=confidence_threshold,",
      "        domain=domain,",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Extract classification confidence decision from prompt selection method",
    "current_implementation": "app/core/langgraph/graph.py:420-430 - Inline confidence check in _get_system_prompt",
    "target_implementation": "app/orchestrators/prompting.py - step_42__classification_confidence_decision()",
    "test_requirements": [
      "Checks 0.6 confidence threshold",
      "Verifies classification exists",
      "Routes to DomainPrompt or DefaultSysPrompt"
    ]
  },
  "43": {
    "step": 43,
    "id": "RAG.classify.prompttemplatemanager.get.prompt.get.domain.specific.prompt",
    "node_id": "DomainPrompt",
    "node_label": "PromptTemplateManager.get_prompt Get domain-specific prompt",
    "category": "classify",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "CheckSysMsg"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: Missing PromptTemplateManager service or embedded in _get_system_prompt",
        "TARGET: app/orchestrators/prompting.py - step_43__get_domain_prompt()",
        "ACTION: Implement or extract domain-specific prompt selection",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing",
    "claude_code_instructions": [
      "# STEP 43: Get domain-specific prompt",
      "",
      "# 1. Check for existing prompt manager:",
      "find app -name '*prompt*' -o -name '*template*' | head -5",
      "",
      "# 2. Create orchestrator:",
      "cat <<'EOF' > app/orchestrators/prompting_step43.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_43__get_domain_prompt(prompt_data: dict) -> dict:",
      "    \"\"\"RAG STEP 43: Get domain-specific system prompt.\"\"\"",
      "    ",
      "    classification = prompt_data.get(\"classification\", {})",
      "    context = prompt_data.get(\"context\", \"\")",
      "    domain = classification.get(\"domain\")",
      "    ",
      "    # Domain-specific prompt templates",
      "    domain_prompts = {",
      "        \"fiscal\": \"\"\"You are an Italian tax and fiscal expert assistant.",
      "You have access to current Italian tax regulations and can help with:",
      "- Invoice analysis and validation",
      "- Tax calculations and deductions",
      "- VAT (IVA) related questions",
      "- Fiscal code verification",
      "",
      "Context: {context}",
      "\"\"\",",
      "        \"legal\": \"\"\"You are an Italian legal assistant specializing in contract law.",
      "You can help with:",
      "- Contract analysis and interpretation",
      "- Legal clause explanations",
      "- Italian civil and commercial law",
      "",
      "Context: {context}",
      "\"\"\",",
      "        \"hr\": \"\"\"You are an Italian HR and employment law expert.",
      "You can assist with:",
      "- Payslip analysis and calculation",
      "- Employment contract questions",
      "- Leave and benefits calculations",
      "- Italian labor law compliance",
      "",
      "Context: {context}",
      "\"\"\"",
      "    }",
      "    ",
      "    # Get domain-specific prompt or fallback",
      "    prompt_template = domain_prompts.get(domain)",
      "    if prompt_template:",
      "        domain_prompt = prompt_template.format(context=context)",
      "    else:",
      "        # Fallback for unknown domains",
      "        domain_prompt = f\"\"\"You are a helpful assistant specializing in {domain}.",
      "",
      "Context: {context}",
      "\"\"\"",
      "    ",
      "    rag_step_log(",
      "        step=43,",
      "        step_id=\"RAG.classify.prompttemplatemanager.get.prompt.get.domain.specific.prompt\",",
      "        action=\"domain_prompt_selected\",",
      "        domain=domain,",
      "        prompt_length=len(domain_prompt),",
      "        has_context=bool(context)",
      "    )",
      "    ",
      "    return {",
      "        \"system_prompt\": domain_prompt,",
      "        \"prompt_type\": \"domain_specific\",",
      "        \"domain\": domain,",
      "        \"next_step\": \"CheckSysMsg\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Implement or extract domain-specific prompt selection",
    "current_implementation": "Missing PromptTemplateManager service or embedded in _get_system_prompt",
    "target_implementation": "app/orchestrators/prompting.py - step_43__get_domain_prompt()",
    "test_requirements": [
      "Returns domain-specific prompt templates",
      "Includes context in prompt",
      "Handles unknown domains gracefully"
    ]
  },
  "44": {
    "step": 44,
    "id": "RAG.prompting.use.default.system.prompt",
    "node_id": "DefaultSysPrompt",
    "node_label": "Use default SYSTEM_PROMPT",
    "category": "prompting",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "CheckSysMsg"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/config.py - SYSTEM_PROMPT setting or embedded in _get_system_prompt",
        "TARGET: app/orchestrators/prompting.py - step_44__use_default_prompt()",
        "ACTION: Use default system prompt when classification is insufficient",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 44: Use default system prompt",
      "",
      "cat <<'EOF' > app/orchestrators/prompting_step44.py",
      "from app.observability.rag_logging import rag_step_log",
      "from app.core.config import settings",
      "",
      "def step_44__use_default_prompt(prompt_data: dict) -> dict:",
      "    \"\"\"RAG STEP 44: Use default system prompt.\"\"\"",
      "    ",
      "    context = prompt_data.get(\"context\", \"\")",
      "    classification = prompt_data.get(\"classification\", {})",
      "    ",
      "    # Get default prompt from settings",
      "    default_prompt = getattr(settings, 'SYSTEM_PROMPT', ",
      "        \"\"\"You are a helpful AI assistant for Italian business and legal questions.",
      "You can help with tax, legal, HR, and accounting queries based on Italian regulations.",
      "",
      "Context: {context}",
      "\"\"\")",
      "    ",
      "    # Include context if available",
      "    if context and '{context}' in default_prompt:",
      "        system_prompt = default_prompt.format(context=context)",
      "    else:",
      "        system_prompt = default_prompt",
      "        if context:",
      "            system_prompt += f\"\\n\\nContext: {context}\"",
      "    ",
      "    rag_step_log(",
      "        step=44,",
      "        step_id=\"RAG.prompting.use.default.system.prompt\",",
      "        action=\"default_prompt_used\",",
      "        reason=\"insufficient_classification\",",
      "        classification_confidence=classification.get(\"confidence\", 0.0),",
      "        prompt_length=len(system_prompt)",
      "    )",
      "    ",
      "    return {",
      "        \"system_prompt\": system_prompt,",
      "        \"prompt_type\": \"default\",",
      "        \"next_step\": \"CheckSysMsg\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Use default system prompt when classification is insufficient",
    "current_implementation": "app/core/config.py - SYSTEM_PROMPT setting or embedded in _get_system_prompt",
    "target_implementation": "app/orchestrators/prompting.py - step_44__use_default_prompt()",
    "test_requirements": [
      "Uses configured default prompt",
      "Includes context if available",
      "Logs reason for default usage"
    ]
  },
  "45": {
    "step": 45,
    "id": "RAG.prompting.system.message.exists",
    "node_id": "CheckSysMsg",
    "node_label": "System message exists?",
    "category": "prompting",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "DomainPrompt",
        "DefaultSysPrompt"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/langgraph/graph.py:599-610 - Inline check in _prepare_messages_with_system_prompt",
        "TARGET: app/orchestrators/prompting.py - step_45__system_message_exists_decision()",
        "ACTION: Extract system message existence check from message preparation method",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "collapsed_into_agent",
    "claude_code_instructions": [
      "# STEP 45: Decision - does system message exist in conversation?",
      "# Current: Inline check in _prepare_messages_with_system_prompt",
      "# Target: Explicit decision orchestrator",
      "",
      "cat <<'EOF' > app/orchestrators/prompting_step45.py",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_45__system_message_exists_decision(prompt_data: dict) -> str:",
      "    \"\"\"RAG STEP 45: Check if system message exists in conversation.\"\"\"",
      "    ",
      "    messages = prompt_data.get(\"messages\", [])",
      "    system_prompt = prompt_data.get(\"system_prompt\", \"\")",
      "    ",
      "    # Check for existing system message",
      "    has_system_message = False",
      "    system_message_index = -1",
      "    ",
      "    for i, message in enumerate(messages):",
      "        if hasattr(message, \"role\") and message.role == \"system\":",
      "            has_system_message = True",
      "            system_message_index = i",
      "            break",
      "        elif isinstance(message, dict) and message.get(\"role\") == \"system\":",
      "            has_system_message = True",
      "            system_message_index = i",
      "            break",
      "    ",
      "    next_step = \"ReplaceMsg\" if has_system_message else \"InsertMsg\"",
      "    ",
      "    rag_step_log(",
      "        step=45,",
      "        step_id=\"RAG.prompting.system.message.exists\",",
      "        decision=f\"system_message_{\"exists\" if has_system_message else \"missing\"}\",",
      "        has_system_message=has_system_message,",
      "        system_message_index=system_message_index,",
      "        messages_count=len(messages),",
      "        next_step=next_step",
      "    )",
      "    ",
      "    return next_step",
      "EOF"
    ],
    "transformation_notes": "Extract system message existence check from message preparation method",
    "current_implementation": "app/core/langgraph/graph.py:599-610 - Inline check in _prepare_messages_with_system_prompt",
    "target_implementation": "app/orchestrators/prompting.py - step_45__system_message_exists_decision()",
    "test_requirements": [
      "Detects existing system messages",
      "Finds system message position",
      "Routes to ReplaceMsg or InsertMsg"
    ]
  },
  "46": {
    "step": 46,
    "id": "RAG.prompting.replace.system.message",
    "node_id": "ReplaceMsg",
    "node_label": "Replace system message",
    "category": "prompting",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "SelectProvider"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/langgraph/graph.py:678-690 - Logic in _prepare_messages_with_system_prompt",
        "TARGET: app/orchestrators/prompting.py - step_46__replace_system_message()",
        "ACTION: Extract system message replacement logic from LangGraphAgent (ALREADY PARTIALLY DONE)",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "collapsed_into_agent",
    "claude_code_instructions": [
      "# STEP 46: Replace existing system message",
      "# Current: Logic in _prepare_messages_with_system_prompt",
      "# Target: Explicit replacement orchestrator",
      "# NOTE: This step was partially implemented in earlier work",
      "",
      "cat <<'EOF' > app/orchestrators/prompting_step46.py",
      "from app.observability.rag_logging import rag_step_log",
      "from app.schemas.chat import Message",
      "",
      "def step_46__replace_system_message(prompt_data: dict) -> dict:",
      "    \"\"\"RAG STEP 46: Replace existing system message with new prompt.\"\"\"",
      "    ",
      "    messages = prompt_data.get(\"messages\", [])",
      "    system_prompt = prompt_data.get(\"system_prompt\", \"\")",
      "    ",
      "    # Find and replace system message",
      "    updated_messages = []",
      "    replaced = False",
      "    ",
      "    for message in messages:",
      "        if ((hasattr(message, \"role\") and message.role == \"system\") or ",
      "            (isinstance(message, dict) and message.get(\"role\") == \"system\")):",
      "            # Replace with new system prompt",
      "            if isinstance(message, dict):",
      "                updated_messages.append({",
      "                    \"role\": \"system\",",
      "                    \"content\": system_prompt",
      "                })",
      "            else:",
      "                updated_messages.append(Message(",
      "                    role=\"system\",",
      "                    content=system_prompt",
      "                ))",
      "            replaced = True",
      "        else:",
      "            updated_messages.append(message)",
      "    ",
      "    rag_step_log(",
      "        step=46,",
      "        step_id=\"RAG.prompting.replace.system.message\",",
      "        action=\"system_message_replaced\",",
      "        replaced=replaced,",
      "        new_prompt_length=len(system_prompt),",
      "        messages_count=len(updated_messages)",
      "    )",
      "    ",
      "    return {",
      "        \"messages\": updated_messages,",
      "        \"system_prompt\": system_prompt,",
      "        \"action\": \"replaced\",",
      "        \"next_step\": \"SelectProvider\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Extract system message replacement logic from LangGraphAgent (ALREADY PARTIALLY DONE)",
    "current_implementation": "app/core/langgraph/graph.py:678-690 - Logic in _prepare_messages_with_system_prompt",
    "target_implementation": "app/orchestrators/prompting.py - step_46__replace_system_message()",
    "test_requirements": [
      "Replaces existing system message",
      "Preserves other messages",
      "Handles both dict and Message objects"
    ]
  },
  "47": {
    "step": 47,
    "id": "RAG.prompting.insert.system.message",
    "node_id": "InsertMsg",
    "node_label": "Insert system message",
    "category": "prompting",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "SelectProvider"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/langgraph/graph.py:657-670 - Logic in _prepare_messages_with_system_prompt",
        "TARGET: app/orchestrators/prompting.py - step_47__insert_system_message()",
        "ACTION: Extract system message insertion logic from LangGraphAgent (ALREADY PARTIALLY DONE)",
        "SEE: claude_code_instructions for commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "collapsed_into_agent",
    "claude_code_instructions": [
      "# STEP 47: Insert new system message",
      "# Current: Logic in _prepare_messages_with_system_prompt",
      "# Target: Explicit insertion orchestrator",
      "# NOTE: This step was partially implemented in earlier work",
      "",
      "cat <<'EOF' > app/orchestrators/prompting_step47.py",
      "from app.observability.rag_logging import rag_step_log",
      "from app.schemas.chat import Message",
      "",
      "def step_47__insert_system_message(prompt_data: dict) -> dict:",
      "    \"\"\"RAG STEP 47: Insert system message at beginning of conversation.\"\"\"",
      "    ",
      "    messages = prompt_data.get(\"messages\", [])",
      "    system_prompt = prompt_data.get(\"system_prompt\", \"\")",
      "    ",
      "    # Create system message",
      "    if messages and isinstance(messages[0], dict):",
      "        # Working with dict format",
      "        system_message = {",
      "            \"role\": \"system\",",
      "            \"content\": system_prompt",
      "        }",
      "    else:",
      "        # Working with Message objects",
      "        system_message = Message(",
      "            role=\"system\",",
      "            content=system_prompt",
      "        )",
      "    ",
      "    # Insert at beginning",
      "    updated_messages = [system_message] + list(messages)",
      "    ",
      "    rag_step_log(",
      "        step=47,",
      "        step_id=\"RAG.prompting.insert.system.message\",",
      "        action=\"system_message_inserted\",",
      "        prompt_length=len(system_prompt),",
      "        original_messages_count=len(messages),",
      "        updated_messages_count=len(updated_messages)",
      "    )",
      "    ",
      "    return {",
      "        \"messages\": updated_messages,",
      "        \"system_prompt\": system_prompt,",
      "        \"action\": \"inserted\",",
      "        \"next_step\": \"SelectProvider\"",
      "    }",
      "EOF"
    ],
    "transformation_notes": "Extract system message insertion logic from LangGraphAgent (ALREADY PARTIALLY DONE)",
    "current_implementation": "app/core/langgraph/graph.py:657-670 - Logic in _prepare_messages_with_system_prompt",
    "target_implementation": "app/orchestrators/prompting.py - step_47__insert_system_message()",
    "test_requirements": [
      "Inserts system message at start",
      "Preserves existing messages order",
      "Handles both dict and Message formats"
    ]
  },
  "48": {
    "step": 48,
    "id": "RAG.providers.langgraphagent.get.optimal.provider.select.llm.provider",
    "node_id": "SelectProvider",
    "node_label": "LangGraphAgent._get_optimal_provider Select LLM provider",
    "category": "providers",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ReplaceMsg",
        "InsertMsg"
      ],
      "outgoing": [
        "RouteStrategy"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/langgraph/graph.py:734 - _get_optimal_provider() method",
        "TARGET: app/orchestrators/providers_step48.py - step_48__select_provider()",
        "ACTION: Extract provider selection initiation from LangGraphAgent._get_optimal_provider into explicit orchestrator",
        "SEE: claude_code_instructions field for copy-paste commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 48: Extract provider selection initiation from LangGraphAgent",
      "# Current: Buried in _get_optimal_provider() method",
      "# Target: Explicit orchestrator node that initiates provider selection",
      "",
      "# 1. Read current implementation:",
      "cat app/core/langgraph/graph.py | grep -A 30 'def _get_optimal_provider'",
      "",
      "# 2. Create orchestrator wrapper:",
      "cat <<'EOF' > app/orchestrators/providers_step48.py",
      "from app.observability.rag_logging import rag_step_log, rag_step_timer",
      "",
      "def step_48__select_provider(messages, classification=None):",
      "    \"\"\"RAG STEP 48: Initiate LLM provider selection.\"\"\"",
      "    with rag_step_timer(step=48) as timer:",
      "        # Extract classification-aware routing from LangGraphAgent",
      "        # This step initiates the provider selection flow",
      "        rag_step_log(",
      "            step=48,",
      "            step_id=\"RAG.providers.langgraphagent.get.optimal.provider.select.llm.provider\",",
      "            action=\"initiating_provider_selection\",",
      "            messages_count=len(messages),",
      "            classification=classification",
      "        )",
      "        # Return context for next step (routing strategy)",
      "        return {",
      "            \"messages\": messages,",
      "            \"classification\": classification,",
      "            \"next_step\": \"RouteStrategy\"",
      "        }",
      "EOF",
      "",
      "# 3. Add test for this orchestrator:",
      "pytest tests/test_rag_step_48_select_provider.py -xvs"
    ],
    "transformation_notes": "Extract provider selection initiation from LangGraphAgent._get_optimal_provider into explicit orchestrator",
    "current_implementation": "app/core/langgraph/graph.py:734 - _get_optimal_provider() method",
    "target_implementation": "app/orchestrators/providers_step48.py - step_48__select_provider()",
    "test_requirements": [
      "Verify orchestrator is called",
      "Check logging occurs",
      "Ensure next step is RouteStrategy"
    ]
  },
  "49": {
    "step": 49,
    "id": "RAG.facts.llmfactory.get.optimal.provider.apply.routing.strategy",
    "node_id": "RouteStrategy",
    "node_label": "LLMFactory.get_optimal_provider Apply routing strategy",
    "category": "facts",
    "type": "process",
    "neighbors": {
      "incoming": [
        "SelectProvider"
      ],
      "outgoing": [
        "StrategyType"
      ]
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/llm/factory.py:127-185 - Inside get_optimal_provider()",
        "TARGET: app/orchestrators/providers_step49.py - step_49__apply_routing_strategy()",
        "ACTION: Extract routing strategy determination from LLMFactory into explicit orchestrator",
        "SEE: claude_code_instructions field for copy-paste commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 49: Extract routing strategy application from LLMFactory",
      "# Current: Inside LLMFactory.get_optimal_provider()",
      "# Target: Explicit orchestrator that applies routing strategy",
      "",
      "# 1. Understand current factory logic:",
      "cat app/core/llm/factory.py | grep -A 50 'def get_optimal_provider'",
      "",
      "# 2. Create orchestrator that applies strategy:",
      "cat <<'EOF' > app/orchestrators/providers_step49.py",
      "from app.core.llm.factory import RoutingStrategy",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_49__apply_routing_strategy(messages, classification=None):",
      "    \"\"\"RAG STEP 49: Apply routing strategy to determine provider selection path.\"\"\"",
      "    # Determine routing strategy based on config/classification",
      "    strategy = _determine_strategy(classification)",
      "    ",
      "    rag_step_log(",
      "        step=49,",
      "        step_id=\"RAG.facts.llmfactory.get.optimal.provider.apply.routing.strategy\",",
      "            action=\"applying_routing_strategy\",",
      "        strategy=strategy.value,",
      "        messages_count=len(messages)",
      "    )",
      "    ",
      "    # Pass to next step (strategy decision)",
      "    return {",
      "        \"messages\": messages,",
      "        \"strategy\": strategy,",
      "        \"next_step\": \"StrategyType\"",
      "    }",
      "",
      "def _determine_strategy(classification):",
      "    \"\"\"Extract from LLMFactory logic.\"\"\"",
      "    # TODO: Extract from factory.py:150-165",
      "    from app.core.config import settings",
      "    return settings.LLM_ROUTING_STRATEGY",
      "EOF",
      "",
      "# 3. Wire into workflow and test:",
      "pytest tests/test_rag_step_49_apply_routing_strategy.py -xvs"
    ],
    "transformation_notes": "Extract routing strategy determination from LLMFactory into explicit orchestrator",
    "current_implementation": "app/core/llm/factory.py:127-185 - Inside get_optimal_provider()",
    "target_implementation": "app/orchestrators/providers_step49.py - step_49__apply_routing_strategy()",
    "test_requirements": [
      "Strategy correctly determined",
      "Logging includes strategy value",
      "Routes to StrategyType node"
    ]
  },
  "50": {
    "step": 50,
    "id": "RAG.platform.routing.strategy",
    "node_id": "StrategyType",
    "node_label": "Routing strategy?",
    "category": "platform",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "RouteStrategy"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [
        "CURRENT: app/core/llm/factory.py:140-160 - Internal switch statement",
        "TARGET: app/orchestrators/providers_step50.py - step_50__routing_strategy_decision()",
        "ACTION: Extract routing strategy switch from factory into explicit decision orchestrator",
        "SEE: claude_code_instructions field for copy-paste commands"
      ],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "needs_extraction",
    "claude_code_instructions": [
      "# STEP 50: Extract routing decision from factory's internal switch",
      "# Current: Switch statement inside LLMFactory",
      "# Target: Explicit decision node that routes to provider selection",
      "",
      "# 1. Extract the routing logic from factory:",
      "cat app/core/llm/factory.py | grep -B5 -A20 'RoutingStrategy.COST_OPTIMIZED'",
      "",
      "# 2. Create decision orchestrator:",
      "cat <<'EOF' > app/orchestrators/providers_step50.py",
      "from app.core.llm.factory import RoutingStrategy",
      "from app.observability.rag_logging import rag_step_log",
      "",
      "def step_50__routing_strategy_decision(messages, strategy, max_cost_eur=None):",
      "    \"\"\"RAG STEP 50: Decision node - route based on strategy type.\"\"\"",
      "    ",
      "    # Map strategy to next step (provider selector)",
      "    routing_map = {",
      "        RoutingStrategy.COST_OPTIMIZED: \"CheapProvider\",",
      "        RoutingStrategy.QUALITY_FIRST: \"BestProvider\",",
      "        RoutingStrategy.BALANCED: \"BalanceProvider\",",
      "        RoutingStrategy.FAILOVER: \"PrimaryProvider\"",
      "    }",
      "    ",
      "    next_step = routing_map.get(strategy, \"BalanceProvider\")",
      "    ",
      "    rag_step_log(",
      "        step=50,",
      "        step_id=\"RAG.platform.routing.strategy\",",
      "        node_label=\"StrategyType\",",
      "        decision=f\"routing_to_{strategy.value}\",",
      "        routing_strategy=strategy.value,",
      "        next_step=next_step,",
      "        max_cost_eur=max_cost_eur",
      "    )",
      "    ",
      "    return next_step",
      "EOF",
      "",
      "# 3. Create parity test proving same provider is selected:",
      "cat <<'EOF' > tests/test_step50_parity.py",
      "def test_routing_parity():",
      "    \"\"\"Prove that orchestrated flow selects same provider as factory.\"\"\"",
      "    from app.core.llm.factory import LLMFactory, RoutingStrategy",
      "    from app.orchestrators.providers_step50 import step_50__routing_strategy_decision",
      "    ",
      "    messages = [{'role': 'user', 'content': 'test'}]",
      "    ",
      "    # Old way (factory)",
      "    factory = LLMFactory()",
      "    old_provider = factory.get_optimal_provider(messages, RoutingStrategy.COST_OPTIMIZED)",
      "    ",
      "    # New way (orchestrated)",
      "    next_step = step_50__routing_strategy_decision(messages, RoutingStrategy.COST_OPTIMIZED)",
      "    assert next_step == \"CheapProvider\"",
      "    # TODO: Verify CheapProvider step returns same provider as old_provider",
      "EOF",
      "",
      "pytest tests/test_step50_parity.py -xvs"
    ],
    "transformation_notes": "Extract routing strategy switch from factory into explicit decision orchestrator",
    "current_implementation": "app/core/llm/factory.py:140-160 - Internal switch statement",
    "target_implementation": "app/orchestrators/providers_step50.py - step_50__routing_strategy_decision()",
    "test_requirements": [
      "Maps each strategy to correct next step",
      "Logs decision with all context",
      "Parity test proves same provider selected"
    ]
  },
  "51": {
    "step": 51,
    "id": "RAG.providers.select.cheapest.provider",
    "node_id": "CheapProvider",
    "node_label": "Select cheapest provider",
    "category": "providers",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "EstimateCost"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "52": {
    "step": 52,
    "id": "RAG.providers.select.best.provider",
    "node_id": "BestProvider",
    "node_label": "Select best provider",
    "category": "providers",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "EstimateCost"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "53": {
    "step": 53,
    "id": "RAG.providers.balance.cost.and.quality",
    "node_id": "BalanceProvider",
    "node_label": "Balance cost and quality",
    "category": "providers",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "EstimateCost"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "54": {
    "step": 54,
    "id": "RAG.providers.use.primary.provider",
    "node_id": "PrimaryProvider",
    "node_label": "Use primary provider",
    "category": "providers",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "EstimateCost"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "55": {
    "step": 55,
    "id": "RAG.providers.costcalculator.estimate.cost.calculate.query.cost",
    "node_id": "EstimateCost",
    "node_label": "CostCalculator.estimate_cost Calculate query cost",
    "category": "providers",
    "type": "process",
    "neighbors": {
      "incoming": [
        "CheapProvider",
        "BestProvider",
        "BalanceProvider",
        "PrimaryProvider"
      ],
      "outgoing": [
        "CostCheck"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "56": {
    "step": 56,
    "id": "RAG.providers.cost.within.budget",
    "node_id": "CostCheck",
    "node_label": "Cost within budget?",
    "category": "providers",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "EstimateCost"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "57": {
    "step": 57,
    "id": "RAG.providers.create.provider.instance",
    "node_id": "CreateProvider",
    "node_label": "Create provider instance",
    "category": "providers",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "CheckCache"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "58": {
    "step": 58,
    "id": "RAG.providers.select.cheaper.provider.or.fail",
    "node_id": "CheaperProvider",
    "node_label": "Select cheaper provider or fail",
    "category": "providers",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "CheckCache"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "59": {
    "step": 59,
    "id": "RAG.cache.langgraphagent.get.cached.llm.response.check.for.cached.response",
    "node_id": "CheckCache",
    "node_label": "LangGraphAgent._get_cached_llm_response Check for cached response",
    "category": "cache",
    "type": "process",
    "neighbors": {
      "incoming": [
        "CreateProvider",
        "CheaperProvider"
      ],
      "outgoing": [
        "ResolveEpochs"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "60": {
    "step": 60,
    "id": "RAG.golden.epochstamps.resolve.kb.epoch.golden.epoch.ccnl.epoch.parser.version",
    "node_id": "ResolveEpochs",
    "node_label": "EpochStamps.resolve kb_epoch golden_epoch ccnl_epoch parser_version",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [
        "CheckCache"
      ],
      "outgoing": [
        "GenHash"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "61": {
    "step": 61,
    "id": "RAG.cache.cacheservice.generate.response.key.sig.and.doc.hashes.and.epochs.and.versions",
    "node_id": "GenHash",
    "node_label": "CacheService._generate_response_key sig and doc_hashes and epochs and versions",
    "category": "cache",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ResolveEpochs"
      ],
      "outgoing": [
        "RedisGet"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "62": {
    "step": 62,
    "id": "RAG.cache.cache.hit",
    "node_id": "CacheHit",
    "node_label": "Cache hit?",
    "category": "cache",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "RedisGet"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "63": {
    "step": 63,
    "id": "RAG.cache.usagetracker.track.track.cache.hit",
    "node_id": "TrackCacheHit",
    "node_label": "UsageTracker.track Track cache hit",
    "category": "cache",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "LogCacheHit"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "64": {
    "step": 64,
    "id": "RAG.providers.llmprovider.chat.completion.make.api.call",
    "node_id": "LLMCall",
    "node_label": "LLMProvider.chat_completion Make API call",
    "category": "providers",
    "type": "process",
    "neighbors": {
      "incoming": [
        "FailoverProvider",
        "RetrySame"
      ],
      "outgoing": [
        "LLMSuccess"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "65": {
    "step": 65,
    "id": "RAG.cache.logger.info.log.cache.hit",
    "node_id": "LogCacheHit",
    "node_label": "Logger.info Log cache hit",
    "category": "cache",
    "type": "process",
    "neighbors": {
      "incoming": [
        "TrackCacheHit"
      ],
      "outgoing": [
        "ReturnCached"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "66": {
    "step": 66,
    "id": "RAG.cache.return.cached.response",
    "node_id": "ReturnCached",
    "node_label": "Return cached response",
    "category": "cache",
    "type": "process",
    "neighbors": {
      "incoming": [
        "LogCacheHit"
      ],
      "outgoing": [
        "ProcessMsg"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "67": {
    "step": 67,
    "id": "RAG.llm.llm.call.successful",
    "node_id": "LLMSuccess",
    "node_label": "LLM call successful?",
    "category": "llm",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "LLMCall"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "68": {
    "step": 68,
    "id": "RAG.cache.cacheservice.cache.response.store.in.redis",
    "node_id": "CacheResponse",
    "node_label": "CacheService.cache_response Store in Redis",
    "category": "cache",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "TrackUsage"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "69": {
    "step": 69,
    "id": "RAG.platform.another.attempt.allowed",
    "node_id": "RetryCheck",
    "node_label": "Another attempt allowed?",
    "category": "platform",
    "type": "decision",
    "neighbors": {
      "incoming": [],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "70": {
    "step": 70,
    "id": "RAG.platform.prod.environment.and.last.retry",
    "node_id": "ProdCheck",
    "node_label": "Prod environment and last retry?",
    "category": "platform",
    "type": "decision",
    "neighbors": {
      "incoming": [],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "71": {
    "step": 71,
    "id": "RAG.platform.return.500.error",
    "node_id": "Error500",
    "node_label": "Return 500 error",
    "category": "platform",
    "type": "error",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "End"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "72": {
    "step": 72,
    "id": "RAG.providers.get.failover.provider",
    "node_id": "FailoverProvider",
    "node_label": "Get FAILOVER provider",
    "category": "providers",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "LLMCall"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "73": {
    "step": 73,
    "id": "RAG.providers.retry.same.provider",
    "node_id": "RetrySame",
    "node_label": "Retry same provider",
    "category": "providers",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "LLMCall"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "74": {
    "step": 74,
    "id": "RAG.metrics.usagetracker.track.track.api.usage",
    "node_id": "TrackUsage",
    "node_label": "UsageTracker.track Track API usage",
    "category": "metrics",
    "type": "process",
    "neighbors": {
      "incoming": [
        "CacheResponse"
      ],
      "outgoing": [
        "ToolCheck"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "75": {
    "step": 75,
    "id": "RAG.response.response.has.tool.calls",
    "node_id": "ToolCheck",
    "node_label": "Response has tool_calls?",
    "category": "response",
    "type": "process",
    "neighbors": {
      "incoming": [
        "TrackUsage"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "76": {
    "step": 76,
    "id": "RAG.platform.convert.to.aimessage.with.tool.calls",
    "node_id": "ConvertAIMsg",
    "node_label": "Convert to AIMessage with tool_calls",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ExecuteTools"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "77": {
    "step": 77,
    "id": "RAG.platform.convert.to.simple.aimessage",
    "node_id": "SimpleAIMsg",
    "node_label": "Convert to simple AIMessage",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "FinalResponse"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "78": {
    "step": 78,
    "id": "RAG.platform.langgraphagent.tool.call.execute.tools",
    "node_id": "ExecuteTools",
    "node_label": "LangGraphAgent._tool_call Execute tools",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ConvertAIMsg"
      ],
      "outgoing": [
        "ToolType"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "79": {
    "step": 79,
    "id": "RAG.routing.tool.type",
    "node_id": "ToolType",
    "node_label": "Tool type?",
    "category": "routing",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "ExecuteTools"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "80": {
    "step": 80,
    "id": "RAG.kb.knowledgesearchtool.search.kb.on.demand",
    "node_id": "KBQueryTool",
    "node_label": "KnowledgeSearchTool.search KB on demand",
    "category": "kb",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ToolResults"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "81": {
    "step": 81,
    "id": "RAG.ccnl.ccnltool.ccnl.query.query.labor.agreements",
    "node_id": "CCNLQuery",
    "node_label": "CCNLTool.ccnl_query Query labor agreements",
    "category": "ccnl",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "PostgresQuery"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "82": {
    "step": 82,
    "id": "RAG.preflight.documentingesttool.process.process.attachments",
    "node_id": "DocIngest",
    "node_label": "DocumentIngestTool.process Process attachments",
    "category": "preflight",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ValidateAttach"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "83": {
    "step": 83,
    "id": "RAG.golden.faqtool.faq.query.query.golden.set",
    "node_id": "FAQQuery",
    "node_label": "FAQTool.faq_query Query Golden Set",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ToolResults"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "84": {
    "step": 84,
    "id": "RAG.preflight.attachmentvalidator.validate.check.files.and.limits",
    "node_id": "ValidateAttach",
    "node_label": "AttachmentValidator.validate Check files and limits",
    "category": "preflight",
    "type": "process",
    "neighbors": {
      "incoming": [
        "DocIngest"
      ],
      "outgoing": [
        "AttachOK"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "85": {
    "step": 85,
    "id": "RAG.preflight.valid.attachments",
    "node_id": "AttachOK",
    "node_label": "Valid attachments?",
    "category": "preflight",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "ValidateAttach"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "86": {
    "step": 86,
    "id": "RAG.platform.return.tool.error.invalid.file",
    "node_id": "ToolErr",
    "node_label": "Return tool error Invalid file",
    "category": "platform",
    "type": "error",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "FinalResponse"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "87": {
    "step": 87,
    "id": "RAG.docs.docsanitizer.sanitize.strip.macros.and.js",
    "node_id": "DocSecurity",
    "node_label": "DocSanitizer.sanitize Strip macros and JS",
    "category": "docs",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "DocClassify"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "88": {
    "step": 88,
    "id": "RAG.classify.docclassifier.classify.detect.document.type",
    "node_id": "DocClassify",
    "node_label": "DocClassifier.classify Detect document type",
    "category": "classify",
    "type": "process",
    "neighbors": {
      "incoming": [
        "DocSecurity"
      ],
      "outgoing": [
        "DocType"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "89": {
    "step": 89,
    "id": "RAG.docs.document.type",
    "node_id": "DocType",
    "node_label": "Document type?",
    "category": "docs",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "DocClassify"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "90": {
    "step": 90,
    "id": "RAG.docs.fatturaparser.parse.xsd.xsd.validation",
    "node_id": "FatturaParser",
    "node_label": "FatturaParser.parse_xsd XSD validation",
    "category": "docs",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ExtractDocFacts"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "91": {
    "step": 91,
    "id": "RAG.docs.f24parser.parse.ocr.layout.aware.ocr",
    "node_id": "F24Parser",
    "node_label": "F24Parser.parse_ocr Layout aware OCR",
    "category": "docs",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ExtractDocFacts"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "92": {
    "step": 92,
    "id": "RAG.docs.contractparser.parse",
    "node_id": "ContractParser",
    "node_label": "ContractParser.parse",
    "category": "docs",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ExtractDocFacts"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "93": {
    "step": 93,
    "id": "RAG.docs.payslipparser.parse",
    "node_id": "PayslipParser",
    "node_label": "PayslipParser.parse",
    "category": "docs",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ExtractDocFacts"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "94": {
    "step": 94,
    "id": "RAG.docs.genericocr.parse.with.layout",
    "node_id": "GenericOCR",
    "node_label": "GenericOCR.parse_with_layout",
    "category": "docs",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ExtractDocFacts"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "95": {
    "step": 95,
    "id": "RAG.facts.extractor.extract.structured.fields",
    "node_id": "ExtractDocFacts",
    "node_label": "Extractor.extract Structured fields",
    "category": "facts",
    "type": "process",
    "neighbors": {
      "incoming": [
        "FatturaParser",
        "F24Parser",
        "ContractParser",
        "PayslipParser",
        "GenericOCR"
      ],
      "outgoing": [
        "StoreBlob"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "96": {
    "step": 96,
    "id": "RAG.docs.blobstore.put.encrypted.ttl.storage",
    "node_id": "StoreBlob",
    "node_label": "BlobStore.put Encrypted TTL storage",
    "category": "docs",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ExtractDocFacts"
      ],
      "outgoing": [
        "Provenance"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "97": {
    "step": 97,
    "id": "RAG.docs.provenance.log.ledger.entry",
    "node_id": "Provenance",
    "node_label": "Provenance.log Ledger entry",
    "category": "docs",
    "type": "process",
    "neighbors": {
      "incoming": [
        "StoreBlob"
      ],
      "outgoing": [
        "ToToolResults"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "98": {
    "step": 98,
    "id": "RAG.facts.convert.to.toolmessage.facts.and.spans",
    "node_id": "ToToolResults",
    "node_label": "Convert to ToolMessage facts and spans",
    "category": "facts",
    "type": "process",
    "neighbors": {
      "incoming": [
        "Provenance"
      ],
      "outgoing": [
        "ToolResults"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "99": {
    "step": 99,
    "id": "RAG.platform.return.to.tool.caller",
    "node_id": "ToolResults",
    "node_label": "Return to tool caller",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ToToolResults",
        "KBQueryTool",
        "FAQQuery",
        "CCNLCalc"
      ],
      "outgoing": [
        "FinalResponse"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "100": {
    "step": 100,
    "id": "RAG.ccnl.ccnlcalculator.calculate.perform.calculations",
    "node_id": "CCNLCalc",
    "node_label": "CCNLCalculator.calculate Perform calculations",
    "category": "ccnl",
    "type": "process",
    "neighbors": {
      "incoming": [
        "PostgresQuery"
      ],
      "outgoing": [
        "ToolResults"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "101": {
    "step": 101,
    "id": "RAG.response.return.to.chat.node.for.final.response",
    "node_id": "FinalResponse",
    "node_label": "Return to chat node for final response",
    "category": "response",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ToolResults",
        "SimpleAIMsg",
        "ToolErr"
      ],
      "outgoing": [
        "ProcessMsg"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "102": {
    "step": 102,
    "id": "RAG.response.langgraphagent.process.messages.convert.to.dict",
    "node_id": "ProcessMsg",
    "node_label": "LangGraphAgent.__process_messages Convert to dict",
    "category": "response",
    "type": "process",
    "neighbors": {
      "incoming": [
        "FinalResponse",
        "ReturnCached"
      ],
      "outgoing": [
        "LogComplete"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "103": {
    "step": 103,
    "id": "RAG.platform.logger.info.log.completion",
    "node_id": "LogComplete",
    "node_label": "Logger.info Log completion",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ProcessMsg"
      ],
      "outgoing": [
        "StreamCheck"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "104": {
    "step": 104,
    "id": "RAG.streaming.streaming.requested",
    "node_id": "StreamCheck",
    "node_label": "Streaming requested?",
    "category": "streaming",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "LogComplete"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "105": {
    "step": 105,
    "id": "RAG.streaming.chatbotcontroller.chat.stream.setup.sse",
    "node_id": "StreamSetup",
    "node_label": "ChatbotController.chat_stream Setup SSE",
    "category": "streaming",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "AsyncGen"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "106": {
    "step": 106,
    "id": "RAG.platform.create.async.generator",
    "node_id": "AsyncGen",
    "node_label": "Create async generator",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [
        "StreamSetup"
      ],
      "outgoing": [
        "SinglePass"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "107": {
    "step": 107,
    "id": "RAG.preflight.singlepassstream.prevent.double.iteration",
    "node_id": "SinglePass",
    "node_label": "SinglePassStream Prevent double iteration",
    "category": "preflight",
    "type": "process",
    "neighbors": {
      "incoming": [
        "AsyncGen"
      ],
      "outgoing": [
        "WriteSSE"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "108": {
    "step": 108,
    "id": "RAG.streaming.write.sse.format.chunks",
    "node_id": "WriteSSE",
    "node_label": "write_sse Format chunks",
    "category": "streaming",
    "type": "process",
    "neighbors": {
      "incoming": [
        "SinglePass"
      ],
      "outgoing": [
        "StreamResponse"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "109": {
    "step": 109,
    "id": "RAG.streaming.streamingresponse.send.chunks",
    "node_id": "StreamResponse",
    "node_label": "StreamingResponse Send chunks",
    "category": "streaming",
    "type": "process",
    "neighbors": {
      "incoming": [
        "WriteSSE"
      ],
      "outgoing": [
        "SendDone"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "110": {
    "step": 110,
    "id": "RAG.platform.send.done.frame",
    "node_id": "SendDone",
    "node_label": "Send DONE frame",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [
        "StreamResponse"
      ],
      "outgoing": [
        "CollectMetrics"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "111": {
    "step": 111,
    "id": "RAG.metrics.collect.usage.metrics",
    "node_id": "CollectMetrics",
    "node_label": "Collect usage metrics",
    "category": "metrics",
    "type": "process",
    "neighbors": {
      "incoming": [
        "SendDone",
        "ReturnComplete"
      ],
      "outgoing": [
        "End",
        "FeedbackUI"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "112": {
    "step": 112,
    "id": "RAG.response.return.response.to.user",
    "node_id": "End",
    "node_label": "Return response to user",
    "category": "response",
    "type": "startEnd",
    "neighbors": {
      "incoming": [
        "CollectMetrics",
        "Error400",
        "Error500"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "113": {
    "step": 113,
    "id": "RAG.feedback.feedbackui.show.options.correct.incomplete.wrong",
    "node_id": "FeedbackUI",
    "node_label": "FeedbackUI.show_options Correct Incomplete Wrong",
    "category": "feedback",
    "type": "process",
    "neighbors": {
      "incoming": [
        "CollectMetrics"
      ],
      "outgoing": [
        "FeedbackProvided"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "114": {
    "step": 114,
    "id": "RAG.feedback.user.provides.feedback",
    "node_id": "FeedbackProvided",
    "node_label": "User provides feedback?",
    "category": "feedback",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "FeedbackUI"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "115": {
    "step": 115,
    "id": "RAG.feedback.no.feedback",
    "node_id": "FeedbackEnd",
    "node_label": "No feedback",
    "category": "feedback",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "116": {
    "step": 116,
    "id": "RAG.feedback.feedback.type.selected",
    "node_id": "FeedbackTypeSel",
    "node_label": "Feedback type selected",
    "category": "feedback",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "FAQFeedback",
        "KnowledgeFeedback",
        "ExpertFeedbackCollector"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "117": {
    "step": 117,
    "id": "RAG.golden.post.api.v1.faq.feedback",
    "node_id": "FAQFeedback",
    "node_label": "POST /api/v1/faq/feedback",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [
        "FeedbackTypeSel"
      ],
      "outgoing": [
        "ExpertFeedbackCollector"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "118": {
    "step": 118,
    "id": "RAG.kb.post.api.v1.knowledge.feedback",
    "node_id": "KnowledgeFeedback",
    "node_label": "POST /api/v1/knowledge/feedback",
    "category": "kb",
    "type": "process",
    "neighbors": {
      "incoming": [
        "FeedbackTypeSel"
      ],
      "outgoing": [
        "ExpertFeedbackCollector"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "119": {
    "step": 119,
    "id": "RAG.metrics.expertfeedbackcollector.collect.feedback",
    "node_id": "ExpertFeedbackCollector",
    "node_label": "ExpertFeedbackCollector.collect_feedback",
    "category": "metrics",
    "type": "process",
    "neighbors": {
      "incoming": [
        "FeedbackTypeSel",
        "FAQFeedback",
        "KnowledgeFeedback"
      ],
      "outgoing": [
        "ValidateExpert"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "120": {
    "step": 120,
    "id": "RAG.platform.validate.expert.credentials",
    "node_id": "ValidateExpert",
    "node_label": "Validate expert credentials",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [
        "ExpertFeedbackCollector"
      ],
      "outgoing": [
        "TrustScoreOK"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "121": {
    "step": 121,
    "id": "RAG.classify.trust.score.at.least.0.7",
    "node_id": "TrustScoreOK",
    "node_label": "Trust score at least 0.7?",
    "category": "classify",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "ValidateExpert"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "122": {
    "step": 122,
    "id": "RAG.feedback.feedback.rejected",
    "node_id": "FeedbackRejected",
    "node_label": "Feedback rejected",
    "category": "feedback",
    "type": "error",
    "neighbors": {
      "incoming": [],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "123": {
    "step": 123,
    "id": "RAG.feedback.create.expertfeedback.record",
    "node_id": "CreateFeedbackRec",
    "node_label": "Create ExpertFeedback record",
    "category": "feedback",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "UpdateExpertMetrics"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "124": {
    "step": 124,
    "id": "RAG.metrics.update.expert.metrics",
    "node_id": "UpdateExpertMetrics",
    "node_label": "Update expert metrics",
    "category": "metrics",
    "type": "process",
    "neighbors": {
      "incoming": [
        "CreateFeedbackRec"
      ],
      "outgoing": [
        "CacheFeedback"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "125": {
    "step": 125,
    "id": "RAG.cache.cache.feedback.1h.ttl",
    "node_id": "CacheFeedback",
    "node_label": "Cache feedback 1h TTL",
    "category": "cache",
    "type": "process",
    "neighbors": {
      "incoming": [
        "UpdateExpertMetrics"
      ],
      "outgoing": [
        "DetermineAction"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "126": {
    "step": 126,
    "id": "RAG.platform.determine.action",
    "node_id": "DetermineAction",
    "node_label": "Determine action",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [
        "CacheFeedback"
      ],
      "outgoing": [
        "GoldenCandidate"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "127": {
    "step": 127,
    "id": "RAG.golden.goldensetupdater.propose.candidate.from.expert.feedback",
    "node_id": "GoldenCandidate",
    "node_label": "GoldenSetUpdater.propose_candidate from expert feedback",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [
        "DetermineAction",
        "GoldenRules"
      ],
      "outgoing": [
        "GoldenApproval"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "128": {
    "step": 128,
    "id": "RAG.golden.auto.threshold.met.or.manual.approval",
    "node_id": "GoldenApproval",
    "node_label": "Auto threshold met or manual approval?",
    "category": "golden",
    "type": "decision",
    "neighbors": {
      "incoming": [
        "GoldenCandidate"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "129": {
    "step": 129,
    "id": "RAG.golden.goldenset.publish.or.update.versioned.entry",
    "node_id": "PublishGolden",
    "node_label": "GoldenSet.publish_or_update versioned entry",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "InvalidateFAQCache",
        "VectorReindex",
        "GoldenSetDB"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "130": {
    "step": 130,
    "id": "RAG.preflight.cacheservice.invalidate.faq.by.id.or.signature",
    "node_id": "InvalidateFAQCache",
    "node_label": "CacheService.invalidate_faq by id or signature",
    "category": "preflight",
    "type": "process",
    "neighbors": {
      "incoming": [
        "PublishGolden"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "131": {
    "step": 131,
    "id": "RAG.golden.vectorindex.upsert.faq.update.embeddings",
    "node_id": "VectorReindex",
    "node_label": "VectorIndex.upsert_faq update embeddings",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [
        "PublishGolden"
      ],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "132": {
    "step": 132,
    "id": "RAG.kb.rss.monitor",
    "node_id": "RSSMonitor",
    "node_label": "RSS Monitor",
    "category": "kb",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": []
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "133": {
    "step": 133,
    "id": "RAG.platform.fetch.and.parse.sources",
    "node_id": "FetchFeeds",
    "node_label": "Fetch and parse sources",
    "category": "platform",
    "type": "process",
    "neighbors": {
      "incoming": [],
      "outgoing": [
        "ParseDocs"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "134": {
    "step": 134,
    "id": "RAG.docs.extract.text.and.metadata",
    "node_id": "ParseDocs",
    "node_label": "Extract text and metadata",
    "category": "docs",
    "type": "process",
    "neighbors": {
      "incoming": [
        "FetchFeeds"
      ],
      "outgoing": [
        "KnowledgeStore"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "135": {
    "step": 135,
    "id": "RAG.golden.goldensetupdater.auto.rule.eval.new.or.obsolete.candidates",
    "node_id": "GoldenRules",
    "node_label": "GoldenSetUpdater.auto_rule_eval new or obsolete candidates",
    "category": "golden",
    "type": "process",
    "neighbors": {
      "incoming": [
        "KnowledgeStore"
      ],
      "outgoing": [
        "GoldenCandidate"
      ]
    },
    "code_refs": {
      "by_stepnum": [],
      "by_stepid": [],
      "by_nodeid": [],
      "by_label": []
    },
    "orchestrator": false,
    "tests_refs": [],
    "runtime_hits": 0,
    "status": "missing"
  },
  "step_59": {
    "step": 59,
    "blueprint_node": "KBDelta",
    "expected_behavior": "Knowledge base delta calculation",
    "status": "missing_implementation",
    "transformation_recipe": {
      "current_implementation": "Logic may be in app/services/context_builder.py:build_context",
      "target_implementation": "app/orchestrators/kb.py:step_59__kbdelta",
      "transformation_notes": "Extract KB delta calculation from ContextBuilder into explicit orchestrator node",
      "claude_code_instructions": [
        "# Step 59: Extract KB Delta Logic",
        "",
        "# 1. Find existing delta logic in ContextBuilder",
        "grep -r 'delta\\|diff\\|change' app/services/context_builder.py",
        "",
        "# 2. Create explicit orchestrator method",
        "cat >> app/orchestrators/kb.py << 'EOF'",
        "",
        "    def step_59__kbdelta(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 59: Calculate knowledge base delta changes\"\"\"",
        "        from app.services.context_builder import ContextBuilder",
        "        from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "        ",
        "        with rag_step_timer(\"step_59_kb_delta\"):",
        "            # Calculate delta changes in knowledge base",
        "            context_builder = ContextBuilder()",
        "            delta_result = context_builder.calculate_kb_delta(",
        "                query=context.get(\"query\", \"\"),",
        "                kb_version=context.get(\"kb_version\"),",
        "                last_update=context.get(\"last_update\")",
        "            )",
        "            ",
        "            payload = {",
        "                \"kb_delta_calculated\": True,",
        "                \"delta_changes\": delta_result.get(\"changes\", []),",
        "                \"delta_size\": delta_result.get(\"size\", 0),",
        "                \"requires_refresh\": delta_result.get(\"refresh_needed\", False)",
        "            }",
        "            ",
        "            rag_step_log(\"step_59_kb_delta\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# 3. Create parity test",
        "cat > tests/test_rag_step_59_kb_delta.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.kb import KBOrchestrator",
        "",
        "def test_step_59_kb_delta():",
        "    \"\"\"Test KB delta calculation orchestrator node\"\"\"",
        "    orchestrator = KBOrchestrator()",
        "    ",
        "    context = {",
        "        \"query\": \"test query\",",
        "        \"kb_version\": \"1.0.0\",",
        "        \"last_update\": \"2025-01-01T00:00:00Z\"",
        "    }",
        "    ",
        "    result = orchestrator.step_59__kbdelta(context)",
        "    ",
        "    assert \"kb_delta_calculated\" in result",
        "    assert \"delta_changes\" in result",
        "    assert \"requires_refresh\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for KB delta calculation with version comparison"
    }
  },
  "step_60": {
    "step": 60,
    "blueprint_node": "EpochStamps",
    "expected_behavior": "Generate epoch timestamps for versioning",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/golden/step_60_rag_golden_epochstamps_resolve_kb_epoch_golden_epoch_ccnl_epoch_parser_version.py (exists)",
      "target_implementation": "app/orchestrators/versioning.py:step_60__epochstamps",
      "transformation_notes": "RAG step exists, create orchestrator wrapper to expose epoch resolution logic",
      "claude_code_instructions": [
        "# Step 60: Create EpochStamps Orchestrator",
        "",
        "# 1. Create versioning orchestrator if not exists",
        "mkdir -p app/orchestrators",
        "touch app/orchestrators/versioning.py",
        "",
        "# 2. Add orchestrator wrapper for epoch stamps",
        "cat >> app/orchestrators/versioning.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class VersioningOrchestrator:",
        "    \"\"\"Orchestrator for version and epoch management\"\"\"",
        "",
        "    def step_60__epochstamps(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 60: Resolve KB, Golden, CCNL epochs and parser version\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_60_epochstamps\"):",
        "            # Import the actual RAG step implementation",
        "            from app.ragsteps.golden.step_60_rag_golden_epochstamps_resolve_kb_epoch_golden_epoch_ccnl_epoch_parser_version import main as step_60_impl",
        "            ",
        "            # Execute the actual step logic",
        "            step_result = step_60_impl(context)",
        "            ",
        "            # Ensure proper payload structure",
        "            payload = {",
        "                \"kb_epoch\": step_result.get(\"kb_epoch\"),",
        "                \"golden_epoch\": step_result.get(\"golden_epoch\"), ",
        "                \"ccnl_epoch\": step_result.get(\"ccnl_epoch\"),",
        "                \"parser_version\": step_result.get(\"parser_version\"),",
        "                \"epochs_resolved\": True",
        "            }",
        "            ",
        "            rag_step_log(\"step_60_epochstamps\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create parity test",
        "cat > tests/test_rag_step_60_epochstamps.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.versioning import VersioningOrchestrator",
        "",
        "def test_step_60_epochstamps():",
        "    \"\"\"Test epoch stamps resolution\"\"\"",
        "    orchestrator = VersioningOrchestrator()",
        "    ",
        "    context = {\"query\": \"test\"}",
        "    result = orchestrator.step_60__epochstamps(context)",
        "    ",
        "    assert \"epochs_resolved\" in result",
        "    assert \"kb_epoch\" in result",
        "    assert \"golden_epoch\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for epoch timestamp resolution with all version types"
    }
  },
  "step_61": {
    "step": 61,
    "blueprint_node": "CacheServiceGenerateResponseKey",
    "expected_behavior": "Generate cache key from query signature and document hashes",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/cache/step_61_rag_cache_cacheservice_generate_response_key_sig_and_doc_hashes_and_epochs_and_v.py (exists)",
      "target_implementation": "app/orchestrators/cache.py:step_61__generate_cache_key",
      "transformation_notes": "RAG step exists, create orchestrator wrapper for cache key generation",
      "claude_code_instructions": [
        "# Step 61: Create Cache Key Generation Orchestrator",
        "",
        "# 1. Create cache orchestrator if not exists",
        "touch app/orchestrators/cache.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/cache.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class CacheOrchestrator:",
        "    \"\"\"Orchestrator for caching operations\"\"\"",
        "",
        "    def step_61__generate_cache_key(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 61: Generate response cache key from signature and doc hashes\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_61_generate_cache_key\"):",
        "            from app.ragsteps.cache.step_61_rag_cache_cacheservice_generate_response_key_sig_and_doc_hashes_and_epochs_and_v import main as step_61_impl",
        "            ",
        "            step_result = step_61_impl(context)",
        "            ",
        "            payload = {",
        "                \"cache_key\": step_result.get(\"cache_key\"),",
        "                \"key_components\": {",
        "                    \"query_signature\": step_result.get(\"query_signature\"),",
        "                    \"doc_hashes\": step_result.get(\"doc_hashes\", []),",
        "                    \"epochs\": step_result.get(\"epochs\", {}),",
        "                    \"version\": step_result.get(\"version\")",
        "                },",
        "                \"cache_key_generated\": True",
        "            }",
        "            ",
        "            rag_step_log(\"step_61_generate_cache_key\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create parity test",
        "cat > tests/test_rag_step_61_cache_key.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.cache import CacheOrchestrator",
        "",
        "def test_step_61_generate_cache_key():",
        "    \"\"\"Test cache key generation from signatures and hashes\"\"\"",
        "    orchestrator = CacheOrchestrator()",
        "    ",
        "    context = {",
        "        \"query\": \"test query\",",
        "        \"document_hashes\": [\"hash1\", \"hash2\"],",
        "        \"epochs\": {\"kb\": 1, \"golden\": 2}",
        "    }",
        "    ",
        "    result = orchestrator.step_61__generate_cache_key(context)",
        "    ",
        "    assert \"cache_key_generated\" in result",
        "    assert \"cache_key\" in result",
        "    assert \"key_components\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for cache key generation with signature and hash inputs"
    }
  },
  "step_62": {
    "step": 62,
    "blueprint_node": "CacheHit",
    "expected_behavior": "Check if response exists in cache",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/cache/step_62_rag_cache_cache_hit.py (exists)",
      "target_implementation": "app/orchestrators/cache.py:step_62__cache_hit",
      "transformation_notes": "RAG step exists, create orchestrator wrapper for cache hit detection",
      "claude_code_instructions": [
        "# Step 62: Add Cache Hit Check to Orchestrator",
        "",
        "# Add method to existing cache orchestrator",
        "cat >> app/orchestrators/cache.py << 'EOF'",
        "",
        "    def step_62__cache_hit(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 62: Check if response exists in cache\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_62_cache_hit\"):",
        "            from app.ragsteps.cache.step_62_rag_cache_cache_hit import main as step_62_impl",
        "            ",
        "            step_result = step_62_impl(context)",
        "            ",
        "            payload = {",
        "                \"cache_hit\": step_result.get(\"hit\", False),",
        "                \"cached_response\": step_result.get(\"response\") if step_result.get(\"hit\") else None,",
        "                \"cache_metadata\": step_result.get(\"metadata\", {}),",
        "                \"hit_timestamp\": step_result.get(\"timestamp\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_62_cache_hit\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test case",
        "cat >> tests/test_rag_step_61_cache_key.py << 'EOF'",
        "",
        "def test_step_62_cache_hit():",
        "    \"\"\"Test cache hit detection\"\"\"",
        "    orchestrator = CacheOrchestrator()",
        "    ",
        "    context = {\"cache_key\": \"test_key_123\"}",
        "    result = orchestrator.step_62__cache_hit(context)",
        "    ",
        "    assert \"cache_hit\" in result",
        "    assert isinstance(result[\"cache_hit\"], bool)",
        "EOF"
      ],
      "test_requirements": "Unit test for cache hit detection with valid and invalid keys"
    }
  },
  "step_63": {
    "step": 63,
    "blueprint_node": "UsageTrackerTrackCacheHit",
    "expected_behavior": "Track cache hit metrics",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/cache/step_63_rag_cache_usagetracker_track_track_cache_hit.py (exists)",
      "target_implementation": "app/orchestrators/metrics.py:step_63__track_cache_hit",
      "transformation_notes": "RAG step exists, create metrics orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 63: Create Cache Hit Metrics Tracking",
        "",
        "# 1. Create metrics orchestrator if not exists",
        "touch app/orchestrators/metrics.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/metrics.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class MetricsOrchestrator:",
        "    \"\"\"Orchestrator for metrics and usage tracking\"\"\"",
        "",
        "    def step_63__track_cache_hit(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 63: Track cache hit metrics for usage analytics\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_63_track_cache_hit\"):",
        "            from app.ragsteps.cache.step_63_rag_cache_usagetracker_track_track_cache_hit import main as step_63_impl",
        "            ",
        "            step_result = step_63_impl(context)",
        "            ",
        "            payload = {",
        "                \"cache_hit_tracked\": True,",
        "                \"metrics_updated\": step_result.get(\"metrics_updated\", False),",
        "                \"hit_rate\": step_result.get(\"hit_rate\"),",
        "                \"tracking_timestamp\": step_result.get(\"timestamp\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_63_track_cache_hit\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_63_cache_metrics.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.metrics import MetricsOrchestrator",
        "",
        "def test_step_63_track_cache_hit():",
        "    \"\"\"Test cache hit metrics tracking\"\"\"",
        "    orchestrator = MetricsOrchestrator()",
        "    ",
        "    context = {",
        "        \"cache_hit\": True,",
        "        \"cache_key\": \"test_key\"",
        "    }",
        "    ",
        "    result = orchestrator.step_63__track_cache_hit(context)",
        "    ",
        "    assert \"cache_hit_tracked\" in result",
        "    assert result[\"cache_hit_tracked\"] is True",
        "EOF"
      ],
      "test_requirements": "Unit test for cache hit metrics with hit rate calculation"
    }
  },
  "step_64": {
    "step": 64,
    "blueprint_node": "CacheMiss",
    "expected_behavior": "Handle cache miss scenario",
    "status": "missing_implementation",
    "transformation_recipe": {
      "current_implementation": "Missing - needs implementation",
      "target_implementation": "app/orchestrators/cache.py:step_64__cache_miss",
      "transformation_notes": "Create cache miss handler that prepares for cache population",
      "claude_code_instructions": [
        "# Step 64: Create Cache Miss Handler",
        "",
        "# Add cache miss method to cache orchestrator",
        "cat >> app/orchestrators/cache.py << 'EOF'",
        "",
        "    def step_64__cache_miss(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 64: Handle cache miss scenario\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_64_cache_miss\"):",
        "            # Cache miss detected, prepare for fresh computation",
        "            cache_key = context.get('cache_key')",
        "            query = context.get('query', '')",
        "            ",
        "            payload = {",
        "                \"cache_miss\": True,",
        "                \"cache_key\": cache_key,",
        "                \"requires_computation\": True,",
        "                \"cache_miss_timestamp\": context.get(\"timestamp\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_64_cache_miss\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_61_cache_key.py << 'EOF'",
        "",
        "def test_step_64_cache_miss():",
        "    \"\"\"Test cache miss handling\"\"\"",
        "    orchestrator = CacheOrchestrator()",
        "    ",
        "    context = {\"cache_key\": \"missing_key\", \"query\": \"test\"}",
        "    result = orchestrator.step_64__cache_miss(context)",
        "    ",
        "    assert result[\"cache_miss\"] is True",
        "    assert result[\"requires_computation\"] is True",
        "EOF"
      ],
      "test_requirements": "Unit test for cache miss scenario with computation flag"
    }
  },
  "step_65": {
    "step": 65,
    "blueprint_node": "LoggerInfoLogCacheHit",
    "expected_behavior": "Log cache hit information",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/cache/step_65_rag_cache_logger_info_log_cache_hit.py (exists)",
      "target_implementation": "app/orchestrators/logging.py:step_65__log_cache_hit",
      "transformation_notes": "RAG step exists, create logging orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 65: Create Cache Hit Logging Orchestrator",
        "",
        "# 1. Create logging orchestrator if not exists",
        "touch app/orchestrators/logging.py",
        "",
        "# 2. Add logging orchestrator wrapper",
        "cat >> app/orchestrators/logging.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class LoggingOrchestrator:",
        "    \"\"\"Orchestrator for logging operations\"\"\"",
        "",
        "    def step_65__log_cache_hit(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 65: Log cache hit information for debugging\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_65_log_cache_hit\"):",
        "            from app.ragsteps.cache.step_65_rag_cache_logger_info_log_cache_hit import main as step_65_impl",
        "            ",
        "            step_result = step_65_impl(context)",
        "            ",
        "            payload = {",
        "                \"cache_hit_logged\": True,",
        "                \"log_level\": \"INFO\",",
        "                \"log_message\": step_result.get(\"message\"),",
        "                \"log_timestamp\": step_result.get(\"timestamp\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_65_log_cache_hit\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_65_cache_logging.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.logging import LoggingOrchestrator",
        "",
        "def test_step_65_log_cache_hit():",
        "    \"\"\"Test cache hit logging\"\"\"",
        "    orchestrator = LoggingOrchestrator()",
        "    ",
        "    context = {",
        "        \"cache_hit\": True,",
        "        \"cache_key\": \"test_key\"",
        "    }",
        "    ",
        "    result = orchestrator.step_65__log_cache_hit(context)",
        "    ",
        "    assert \"cache_hit_logged\" in result",
        "    assert result[\"log_level\"] == \"INFO\"",
        "EOF"
      ],
      "test_requirements": "Unit test for cache hit logging with proper log level"
    }
  },
  "step_66": {
    "step": 66,
    "blueprint_node": "ReturnCachedResponse",
    "expected_behavior": "Return cached response to user",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/cache/step_66_rag_cache_return_cached_response.py (exists)",
      "target_implementation": "app/orchestrators/response.py:step_66__return_cached_response",
      "transformation_notes": "RAG step exists, create response orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 66: Create Cached Response Return Orchestrator",
        "",
        "# 1. Create response orchestrator if not exists",
        "touch app/orchestrators/response.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/response.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class ResponseOrchestrator:",
        "    \"\"\"Orchestrator for response handling\"\"\"",
        "",
        "    def step_66__return_cached_response(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 66: Return cached response to user\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_66_return_cached_response\"):",
        "            from app.ragsteps.cache.step_66_rag_cache_return_cached_response import main as step_66_impl",
        "            ",
        "            step_result = step_66_impl(context)",
        "            ",
        "            payload = {",
        "                \"response_returned\": True,",
        "                \"response_source\": \"cache\",",
        "                \"response_data\": step_result.get(\"response\"),",
        "                \"cache_metadata\": step_result.get(\"metadata\", {})",
        "            }",
        "            ",
        "            rag_step_log(\"step_66_return_cached_response\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_66_cached_response.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.response import ResponseOrchestrator",
        "",
        "def test_step_66_return_cached_response():",
        "    \"\"\"Test returning cached response\"\"\"",
        "    orchestrator = ResponseOrchestrator()",
        "    ",
        "    context = {",
        "        \"cached_response\": \"This is a cached answer\",",
        "        \"cache_metadata\": {\"hit_time\": \"2025-01-01T00:00:00Z\"}",
        "    }",
        "    ",
        "    result = orchestrator.step_66__return_cached_response(context)",
        "    ",
        "    assert result[\"response_returned\"] is True",
        "    assert result[\"response_source\"] == \"cache\"",
        "EOF"
      ],
      "test_requirements": "Unit test for cached response return with metadata"
    }
  },
  "step_67": {
    "step": 67,
    "blueprint_node": "LLMCallSuccessful",
    "expected_behavior": "Check if LLM call was successful",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/llm/step_67_rag_llm_llm_call_successful.py (exists)",
      "target_implementation": "app/orchestrators/llm.py:step_67__llm_call_successful",
      "transformation_notes": "RAG step exists, create LLM orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 67: Create LLM Call Success Check Orchestrator",
        "",
        "# 1. Add method to existing LLM orchestrator",
        "cat >> app/orchestrators/llm.py << 'EOF'",
        "",
        "    def step_67__llm_call_successful(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 67: Check if LLM call was successful\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_67_llm_call_successful\"):",
        "            from app.ragsteps.llm.step_67_rag_llm_llm_call_successful import main as step_67_impl",
        "            ",
        "            step_result = step_67_impl(context)",
        "            ",
        "            payload = {",
        "                \"llm_call_successful\": step_result.get(\"success\", False),",
        "                \"response_received\": step_result.get(\"has_response\", False),",
        "                \"error_details\": step_result.get(\"error\") if not step_result.get(\"success\") else None,",
        "                \"response_quality_score\": step_result.get(\"quality_score\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_67_llm_call_successful\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# 2. Add test",
        "cat >> tests/test_rag_step_36_llm_better.py << 'EOF'",
        "",
        "def test_step_67_llm_call_successful():",
        "    \"\"\"Test LLM call success validation\"\"\"",
        "    orchestrator = LLMOrchestrator()",
        "    ",
        "    context = {",
        "        \"llm_response\": \"Generated response\",",
        "        \"provider\": \"openai\"",
        "    }",
        "    ",
        "    result = orchestrator.step_67__llm_call_successful(context)",
        "    ",
        "    assert \"llm_call_successful\" in result",
        "    assert \"response_received\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for LLM success validation with error handling"
    }
  },
  "step_68": {
    "step": 68,
    "blueprint_node": "CacheServiceCacheResponse",
    "expected_behavior": "Store response in Redis cache",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/cache/step_68_rag_cache_cacheservice_cache_response_store_in_redis.py (exists)",
      "target_implementation": "app/orchestrators/cache.py:step_68__cache_response",
      "transformation_notes": "RAG step exists, add to cache orchestrator",
      "claude_code_instructions": [
        "# Step 68: Add Response Caching to Cache Orchestrator",
        "",
        "# Add method to existing cache orchestrator",
        "cat >> app/orchestrators/cache.py << 'EOF'",
        "",
        "    def step_68__cache_response(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 68: Store response in Redis cache for future use\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_68_cache_response\"):",
        "            from app.ragsteps.cache.step_68_rag_cache_cacheservice_cache_response_store_in_redis import main as step_68_impl",
        "            ",
        "            step_result = step_68_impl(context)",
        "            ",
        "            payload = {",
        "                \"response_cached\": step_result.get(\"cached\", False),",
        "                \"cache_key\": step_result.get(\"cache_key\"),",
        "                \"cache_ttl\": step_result.get(\"ttl_seconds\"),",
        "                \"cache_size_bytes\": step_result.get(\"size_bytes\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_68_cache_response\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_61_cache_key.py << 'EOF'",
        "",
        "def test_step_68_cache_response():",
        "    \"\"\"Test response caching to Redis\"\"\"",
        "    orchestrator = CacheOrchestrator()",
        "    ",
        "    context = {",
        "        \"response\": \"Generated answer\",",
        "        \"cache_key\": \"test_response_key\"",
        "    }",
        "    ",
        "    result = orchestrator.step_68__cache_response(context)",
        "    ",
        "    assert \"response_cached\" in result",
        "    assert \"cache_key\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for response caching with TTL and size validation"
    }
  },
  "step_69": {
    "step": 69,
    "blueprint_node": "AnotherAttemptAllowed",
    "expected_behavior": "Check if retry is allowed based on attempt count",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_69_rag_platform_another_attempt_allowed.py (exists)",
      "target_implementation": "app/orchestrators/retry.py:step_69__another_attempt_allowed",
      "transformation_notes": "RAG step exists, create retry orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 69: Create Retry Attempt Check Orchestrator",
        "",
        "# 1. Create retry orchestrator if not exists",
        "touch app/orchestrators/retry.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/retry.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class RetryOrchestrator:",
        "    \"\"\"Orchestrator for retry logic and attempt management\"\"\"",
        "",
        "    def step_69__another_attempt_allowed(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 69: Check if another retry attempt is allowed\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_69_another_attempt_allowed\"):",
        "            from app.ragsteps.platform.step_69_rag_platform_another_attempt_allowed import main as step_69_impl",
        "            ",
        "            step_result = step_69_impl(context)",
        "            ",
        "            payload = {",
        "                \"retry_allowed\": step_result.get(\"allowed\", False),",
        "                \"attempt_count\": step_result.get(\"attempt_count\", 0),",
        "                \"max_attempts\": step_result.get(\"max_attempts\", 3),",
        "                \"retry_delay_seconds\": step_result.get(\"delay_seconds\", 1)",
        "            }",
        "            ",
        "            rag_step_log(\"step_69_another_attempt_allowed\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_69_retry_check.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.retry import RetryOrchestrator",
        "",
        "def test_step_69_another_attempt_allowed():",
        "    \"\"\"Test retry attempt validation\"\"\"",
        "    orchestrator = RetryOrchestrator()",
        "    ",
        "    context = {",
        "        \"attempt_count\": 1,",
        "        \"max_attempts\": 3",
        "    }",
        "    ",
        "    result = orchestrator.step_69__another_attempt_allowed(context)",
        "    ",
        "    assert \"retry_allowed\" in result",
        "    assert \"attempt_count\" in result",
        "    assert \"max_attempts\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for retry attempt validation with limit checking"
    }
  },
  "step_70": {
    "step": 70,
    "blueprint_node": "ProdEnvironmentAndLastRetry",
    "expected_behavior": "Check production environment and final retry status",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_70_rag_platform_prod_environment_and_last_retry.py (exists)",
      "target_implementation": "app/orchestrators/environment.py:step_70__prod_env_final_retry",
      "transformation_notes": "RAG step exists, create environment orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 70: Create Production Environment Final Retry Check",
        "",
        "# 1. Create environment orchestrator if not exists",
        "touch app/orchestrators/environment.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/environment.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class EnvironmentOrchestrator:",
        "    \"\"\"Orchestrator for environment-specific logic\"\"\"",
        "",
        "    def step_70__prod_env_final_retry(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 70: Check production environment and final retry status\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_70_prod_env_final_retry\"):",
        "            from app.ragsteps.platform.step_70_rag_platform_prod_environment_and_last_retry import main as step_70_impl",
        "            ",
        "            step_result = step_70_impl(context)",
        "            ",
        "            payload = {",
        "                \"is_production\": step_result.get(\"is_production\", False),",
        "                \"is_final_retry\": step_result.get(\"is_final_retry\", False),",
        "                \"environment\": step_result.get(\"environment\", \"unknown\"),",
        "                \"should_fail_fast\": step_result.get(\"fail_fast\", False)",
        "            }",
        "            ",
        "            rag_step_log(\"step_70_prod_env_final_retry\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_70_prod_env.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.environment import EnvironmentOrchestrator",
        "",
        "def test_step_70_prod_env_final_retry():",
        "    \"\"\"Test production environment and final retry check\"\"\"",
        "    orchestrator = EnvironmentOrchestrator()",
        "    ",
        "    context = {",
        "        \"environment\": \"production\",",
        "        \"attempt_count\": 3,",
        "        \"max_attempts\": 3",
        "    }",
        "    ",
        "    result = orchestrator.step_70__prod_env_final_retry(context)",
        "    ",
        "    assert \"is_production\" in result",
        "    assert \"is_final_retry\" in result",
        "    assert \"should_fail_fast\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for production environment and final retry validation"
    }
  },
  "step_71": {
    "step": 71,
    "blueprint_node": "Return500Error",
    "expected_behavior": "Return 500 internal server error response",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_71_rag_platform_return_500_error.py (exists)",
      "target_implementation": "app/orchestrators/error_handling.py:step_71__return_500_error",
      "transformation_notes": "RAG step exists, create error handling orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 71: Create 500 Error Response Orchestrator",
        "",
        "# 1. Create error handling orchestrator if not exists",
        "touch app/orchestrators/error_handling.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/error_handling.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class ErrorHandlingOrchestrator:",
        "    \"\"\"Orchestrator for error handling and HTTP responses\"\"\"",
        "",
        "    def step_71__return_500_error(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 71: Return 500 internal server error response\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_71_return_500_error\"):",
        "            from app.ragsteps.platform.step_71_rag_platform_return_500_error import main as step_71_impl",
        "            ",
        "            step_result = step_71_impl(context)",
        "            ",
        "            payload = {",
        "                \"error_response_sent\": True,",
        "                \"http_status\": 500,",
        "                \"error_message\": step_result.get(\"error_message\", \"Internal Server Error\"),",
        "                \"error_code\": step_result.get(\"error_code\"),",
        "                \"request_id\": context.get(\"request_id\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_71_return_500_error\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_71_error_handling.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.error_handling import ErrorHandlingOrchestrator",
        "",
        "def test_step_71_return_500_error():",
        "    \"\"\"Test 500 error response generation\"\"\"",
        "    orchestrator = ErrorHandlingOrchestrator()",
        "    ",
        "    context = {",
        "        \"error\": \"Database connection failed\",",
        "        \"request_id\": \"req_12345\"",
        "    }",
        "    ",
        "    result = orchestrator.step_71__return_500_error(context)",
        "    ",
        "    assert result[\"error_response_sent\"] is True",
        "    assert result[\"http_status\"] == 500",
        "    assert \"error_message\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for 500 error response with proper HTTP status and error details"
    }
  },
  "step_72": {
    "step": 72,
    "blueprint_node": "GetFailoverProvider",
    "expected_behavior": "Get alternative LLM provider for failover",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/providers/step_72_rag_providers_get_failover_provider.py (exists)",
      "target_implementation": "app/orchestrators/providers.py:step_72__get_failover_provider",
      "transformation_notes": "RAG step exists, add to providers orchestrator",
      "claude_code_instructions": [
        "# Step 72: Add Failover Provider Selection to Providers Orchestrator",
        "",
        "# Add method to existing providers orchestrator",
        "cat >> app/orchestrators/providers.py << 'EOF'",
        "",
        "    def step_72__get_failover_provider(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 72: Get alternative LLM provider for failover\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_72_get_failover_provider\"):",
        "            from app.ragsteps.providers.step_72_rag_providers_get_failover_provider import main as step_72_impl",
        "            ",
        "            step_result = step_72_impl(context)",
        "            ",
        "            payload = {",
        "                \"failover_provider\": step_result.get(\"provider\"),",
        "                \"failover_available\": step_result.get(\"available\", False),",
        "                \"failed_provider\": context.get(\"current_provider\"),",
        "                \"failover_reason\": context.get(\"failure_reason\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_72_get_failover_provider\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_50_routing_strategy.py << 'EOF'",
        "",
        "def test_step_72_get_failover_provider():",
        "    \"\"\"Test failover provider selection\"\"\"",
        "    orchestrator = ProvidersOrchestrator()",
        "    ",
        "    context = {",
        "        \"current_provider\": \"openai\",",
        "        \"failure_reason\": \"rate_limit_exceeded\"",
        "    }",
        "    ",
        "    result = orchestrator.step_72__get_failover_provider(context)",
        "    ",
        "    assert \"failover_provider\" in result",
        "    assert \"failover_available\" in result",
        "    assert \"failed_provider\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for failover provider selection with failure reason tracking"
    }
  },
  "step_73": {
    "step": 73,
    "blueprint_node": "RetrySameProvider",
    "expected_behavior": "Retry with the same LLM provider",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/providers/step_73_rag_providers_retry_same_provider.py (exists)",
      "target_implementation": "app/orchestrators/retry.py:step_73__retry_same_provider",
      "transformation_notes": "RAG step exists, add to retry orchestrator",
      "claude_code_instructions": [
        "# Step 73: Add Same Provider Retry to Retry Orchestrator",
        "",
        "# Add method to existing retry orchestrator",
        "cat >> app/orchestrators/retry.py << 'EOF'",
        "",
        "    def step_73__retry_same_provider(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 73: Retry with the same LLM provider\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_73_retry_same_provider\"):",
        "            from app.ragsteps.providers.step_73_rag_providers_retry_same_provider import main as step_73_impl",
        "            ",
        "            step_result = step_73_impl(context)",
        "            ",
        "            payload = {",
        "                \"retry_initiated\": True,",
        "                \"retry_provider\": step_result.get(\"provider\"),",
        "                \"retry_attempt\": step_result.get(\"attempt_number\", 1),",
        "                \"retry_delay_applied\": step_result.get(\"delay_seconds\", 0)",
        "            }",
        "            ",
        "            rag_step_log(\"step_73_retry_same_provider\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_69_retry_check.py << 'EOF'",
        "",
        "def test_step_73_retry_same_provider():",
        "    \"\"\"Test same provider retry logic\"\"\"",
        "    orchestrator = RetryOrchestrator()",
        "    ",
        "    context = {",
        "        \"current_provider\": \"anthropic\",",
        "        \"attempt_count\": 1",
        "    }",
        "    ",
        "    result = orchestrator.step_73__retry_same_provider(context)",
        "    ",
        "    assert result[\"retry_initiated\"] is True",
        "    assert \"retry_provider\" in result",
        "    assert \"retry_attempt\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for same provider retry with attempt counting and delay"
    }
  },
  "step_74": {
    "step": 74,
    "blueprint_node": "UsageTrackerTrackAPIUsage",
    "expected_behavior": "Track API usage metrics",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/metrics/step_74_rag_metrics_usagetracker_track_track_api_usage.py (exists)",
      "target_implementation": "app/orchestrators/metrics.py:step_74__track_api_usage",
      "transformation_notes": "RAG step exists, add to metrics orchestrator",
      "claude_code_instructions": [
        "# Step 74: Add API Usage Tracking to Metrics Orchestrator",
        "",
        "# Add method to existing metrics orchestrator",
        "cat >> app/orchestrators/metrics.py << 'EOF'",
        "",
        "    def step_74__track_api_usage(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 74: Track API usage metrics for billing and analytics\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_74_track_api_usage\"):",
        "            from app.ragsteps.metrics.step_74_rag_metrics_usagetracker_track_track_api_usage import main as step_74_impl",
        "            ",
        "            step_result = step_74_impl(context)",
        "            ",
        "            payload = {",
        "                \"api_usage_tracked\": True,",
        "                \"tokens_used\": step_result.get(\"tokens\", 0),",
        "                \"cost_cents\": step_result.get(\"cost_cents\", 0),",
        "                \"provider\": step_result.get(\"provider\"),",
        "                \"request_duration_ms\": step_result.get(\"duration_ms\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_74_track_api_usage\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_63_cache_metrics.py << 'EOF'",
        "",
        "def test_step_74_track_api_usage():",
        "    \"\"\"Test API usage metrics tracking\"\"\"",
        "    orchestrator = MetricsOrchestrator()",
        "    ",
        "    context = {",
        "        \"llm_response\": \"Generated text\",",
        "        \"provider\": \"openai\",",
        "        \"tokens_used\": 150",
        "    }",
        "    ",
        "    result = orchestrator.step_74__track_api_usage(context)",
        "    ",
        "    assert result[\"api_usage_tracked\"] is True",
        "    assert \"tokens_used\" in result",
        "    assert \"cost_cents\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for API usage tracking with tokens, cost, and duration metrics"
    }
  },
  "step_75": {
    "step": 75,
    "blueprint_node": "ResponseHasToolCalls",
    "expected_behavior": "Check if LLM response contains tool calls",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/response/step_75_rag_response_response_has_tool_calls.py (exists)",
      "target_implementation": "app/orchestrators/response.py:step_75__response_has_tool_calls",
      "transformation_notes": "RAG step exists, add to response orchestrator",
      "claude_code_instructions": [
        "# Step 75: Add Tool Call Detection to Response Orchestrator",
        "",
        "# Add method to existing response orchestrator",
        "cat >> app/orchestrators/response.py << 'EOF'",
        "",
        "    def step_75__response_has_tool_calls(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 75: Check if LLM response contains tool calls\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_75_response_has_tool_calls\"):",
        "            from app.ragsteps.response.step_75_rag_response_response_has_tool_calls import main as step_75_impl",
        "            ",
        "            step_result = step_75_impl(context)",
        "            ",
        "            payload = {",
        "                \"has_tool_calls\": step_result.get(\"has_tools\", False),",
        "                \"tool_call_count\": step_result.get(\"tool_count\", 0),",
        "                \"tool_names\": step_result.get(\"tool_names\", []),",
        "                \"requires_tool_execution\": step_result.get(\"needs_execution\", False)",
        "            }",
        "            ",
        "            rag_step_log(\"step_75_response_has_tool_calls\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_66_cached_response.py << 'EOF'",
        "",
        "def test_step_75_response_has_tool_calls():",
        "    \"\"\"Test tool call detection in responses\"\"\"",
        "    orchestrator = ResponseOrchestrator()",
        "    ",
        "    context = {",
        "        \"llm_response\": {",
        "            \"tool_calls\": [{\"name\": \"search_kb\", \"args\": {}}]",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_75__response_has_tool_calls(context)",
        "    ",
        "    assert \"has_tool_calls\" in result",
        "    assert \"tool_call_count\" in result",
        "    assert \"tool_names\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for tool call detection with count and name extraction"
    }
  },
  "step_76": {
    "step": 76,
    "blueprint_node": "ConvertToAIMessageWithToolCalls",
    "expected_behavior": "Convert response to AI message with tool calls",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_76_rag_platform_convert_to_aimessage_with_tool_calls.py (exists)",
      "target_implementation": "app/orchestrators/message_conversion.py:step_76__convert_to_ai_message_with_tools",
      "transformation_notes": "RAG step exists, create message conversion orchestrator",
      "claude_code_instructions": [
        "# Step 76: Create AI Message with Tool Calls Conversion",
        "",
        "# 1. Create message conversion orchestrator if not exists",
        "touch app/orchestrators/message_conversion.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/message_conversion.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class MessageConversionOrchestrator:",
        "    \"\"\"Orchestrator for message format conversions\"\"\"",
        "",
        "    def step_76__convert_to_ai_message_with_tools(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 76: Convert response to AI message with tool calls\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_76_convert_to_ai_message_with_tools\"):",
        "            from app.ragsteps.platform.step_76_rag_platform_convert_to_aimessage_with_tool_calls import main as step_76_impl",
        "            ",
        "            step_result = step_76_impl(context)",
        "            ",
        "            payload = {",
        "                \"ai_message_created\": True,",
        "                \"message_type\": \"ai_with_tools\",",
        "                \"tool_calls\": step_result.get(\"tool_calls\", []),",
        "                \"message_content\": step_result.get(\"content\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_76_convert_to_ai_message_with_tools\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_76_message_conversion.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.message_conversion import MessageConversionOrchestrator",
        "",
        "def test_step_76_convert_to_ai_message_with_tools():",
        "    \"\"\"Test AI message conversion with tool calls\"\"\"",
        "    orchestrator = MessageConversionOrchestrator()",
        "    ",
        "    context = {",
        "        \"llm_response\": \"Response with tools\",",
        "        \"tool_calls\": [{\"name\": \"search\", \"args\": {\"query\": \"test\"}}]",
        "    }",
        "    ",
        "    result = orchestrator.step_76__convert_to_ai_message_with_tools(context)",
        "    ",
        "    assert result[\"ai_message_created\"] is True",
        "    assert result[\"message_type\"] == \"ai_with_tools\"",
        "    assert \"tool_calls\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for AI message conversion with tool call structure validation"
    }
  },
  "step_77": {
    "step": 77,
    "blueprint_node": "ConvertToSimpleAIMessage",
    "expected_behavior": "Convert response to simple AI message without tools",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_77_rag_platform_convert_to_simple_aimessage.py (exists)",
      "target_implementation": "app/orchestrators/message_conversion.py:step_77__convert_to_simple_ai_message",
      "transformation_notes": "RAG step exists, add to message conversion orchestrator",
      "claude_code_instructions": [
        "# Step 77: Add Simple AI Message Conversion",
        "",
        "# Add method to existing message conversion orchestrator",
        "cat >> app/orchestrators/message_conversion.py << 'EOF'",
        "",
        "    def step_77__convert_to_simple_ai_message(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 77: Convert response to simple AI message without tools\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_77_convert_to_simple_ai_message\"):",
        "            from app.ragsteps.platform.step_77_rag_platform_convert_to_simple_aimessage import main as step_77_impl",
        "            ",
        "            step_result = step_77_impl(context)",
        "            ",
        "            payload = {",
        "                \"ai_message_created\": True,",
        "                \"message_type\": \"simple_ai\",",
        "                \"message_content\": step_result.get(\"content\"),",
        "                \"has_citations\": step_result.get(\"has_citations\", False)",
        "            }",
        "            ",
        "            rag_step_log(\"step_77_convert_to_simple_ai_message\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_76_message_conversion.py << 'EOF'",
        "",
        "def test_step_77_convert_to_simple_ai_message():",
        "    \"\"\"Test simple AI message conversion\"\"\"",
        "    orchestrator = MessageConversionOrchestrator()",
        "    ",
        "    context = {",
        "        \"llm_response\": \"Simple response without tools\"",
        "    }",
        "    ",
        "    result = orchestrator.step_77__convert_to_simple_ai_message(context)",
        "    ",
        "    assert result[\"ai_message_created\"] is True",
        "    assert result[\"message_type\"] == \"simple_ai\"",
        "    assert \"message_content\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for simple AI message conversion without tool calls"
    }
  },
  "step_78": {
    "step": 78,
    "blueprint_node": "LangGraphAgentToolCallExecuteTools",
    "expected_behavior": "Execute tools called by LangGraph agent",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_78_rag_platform_langgraphagent_tool_call_execute_tools.py (exists)",
      "target_implementation": "app/orchestrators/tool_execution.py:step_78__execute_tools",
      "transformation_notes": "RAG step exists, create tool execution orchestrator",
      "claude_code_instructions": [
        "# Step 78: Create Tool Execution Orchestrator",
        "",
        "# 1. Create tool execution orchestrator if not exists",
        "touch app/orchestrators/tool_execution.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/tool_execution.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class ToolExecutionOrchestrator:",
        "    \"\"\"Orchestrator for tool execution and results handling\"\"\"",
        "",
        "    def step_78__execute_tools(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 78: Execute tools called by LangGraph agent\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_78_execute_tools\"):",
        "            from app.ragsteps.platform.step_78_rag_platform_langgraphagent_tool_call_execute_tools import main as step_78_impl",
        "            ",
        "            step_result = step_78_impl(context)",
        "            ",
        "            payload = {",
        "                \"tools_executed\": True,",
        "                \"execution_results\": step_result.get(\"results\", []),",
        "                \"tools_count\": step_result.get(\"tools_executed\", 0),",
        "                \"execution_errors\": step_result.get(\"errors\", [])",
        "            }",
        "            ",
        "            rag_step_log(\"step_78_execute_tools\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_78_tool_execution.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.tool_execution import ToolExecutionOrchestrator",
        "",
        "def test_step_78_execute_tools():",
        "    \"\"\"Test tool execution by LangGraph agent\"\"\"",
        "    orchestrator = ToolExecutionOrchestrator()",
        "    ",
        "    context = {",
        "        \"tool_calls\": [",
        "            {\"name\": \"search_kb\", \"args\": {\"query\": \"test\"}}",
        "        ]",
        "    }",
        "    ",
        "    result = orchestrator.step_78__execute_tools(context)",
        "    ",
        "    assert result[\"tools_executed\"] is True",
        "    assert \"execution_results\" in result",
        "    assert \"tools_count\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for tool execution with result collection and error handling"
    }
  },
  "step_79": {
    "step": 79,
    "blueprint_node": "ToolValidation",
    "expected_behavior": "Validate tool execution results and parameters",
    "status": "missing_implementation",
    "transformation_recipe": {
      "current_implementation": "Missing - needs implementation",
      "target_implementation": "app/orchestrators/tool_execution.py:step_79__validate_tool_results",
      "transformation_notes": "Create tool validation logic to ensure tool results are valid",
      "claude_code_instructions": [
        "# Step 79: Add Tool Result Validation",
        "",
        "# Add validation method to tool execution orchestrator",
        "cat >> app/orchestrators/tool_execution.py << 'EOF'",
        "",
        "    def step_79__validate_tool_results(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 79: Validate tool execution results and parameters\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_79_validate_tool_results\"):",
        "            execution_results = context.get('execution_results', [])",
        "            validation_errors = []",
        "            valid_results = []",
        "            ",
        "            for result in execution_results:",
        "                # Basic validation logic",
        "                if not result or \"error\" in result:",
        "                    validation_errors.append(result.get(\"error\", \"Unknown tool error\"))",
        "                else:",
        "                    valid_results.append(result)",
        "            ",
        "            payload = {",
        "                \"validation_completed\": True,",
        "                \"valid_results\": valid_results,",
        "                \"validation_errors\": validation_errors,",
        "                \"results_valid\": len(validation_errors) == 0,",
        "                \"validated_count\": len(valid_results)",
        "            }",
        "            ",
        "            rag_step_log(\"step_79_validate_tool_results\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_78_tool_execution.py << 'EOF'",
        "",
        "def test_step_79_validate_tool_results():",
        "    \"\"\"Test tool result validation\"\"\"",
        "    orchestrator = ToolExecutionOrchestrator()",
        "    ",
        "    context = {",
        "        \"execution_results\": [",
        "            {\"tool\": \"search_kb\", \"result\": \"Found 3 documents\"},",
        "            {\"tool\": \"calculator\", \"error\": \"Division by zero\"}",
        "        ]",
        "    }",
        "    ",
        "    result = orchestrator.step_79__validate_tool_results(context)",
        "    ",
        "    assert result[\"validation_completed\"] is True",
        "    assert \"valid_results\" in result",
        "    assert \"validation_errors\" in result",
        "    assert result[\"results_valid\"] is False  # Due to error",
        "EOF"
      ],
      "test_requirements": "Unit test for tool result validation with error detection and filtering"
    }
  },
  "step_80": {
    "step": 80,
    "blueprint_node": "KnowledgeSearchOnDemand",
    "expected_behavior": "Search knowledge base on demand",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/kb/step_80_rag_kb_knowledgesearchtool_search_kb_on_demand.py (exists)",
      "target_implementation": "app/orchestrators/kb.py:step_80__search_kb_on_demand",
      "transformation_notes": "RAG step exists, add to KB orchestrator",
      "claude_code_instructions": [
        "# Step 80: Add On-Demand KB Search to KB Orchestrator",
        "",
        "# Add method to existing KB orchestrator",
        "cat >> app/orchestrators/kb.py << 'EOF'",
        "",
        "    def step_80__search_kb_on_demand(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 80: Search knowledge base on demand for additional context\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_80_search_kb_on_demand\"):",
        "            from app.ragsteps.kb.step_80_rag_kb_knowledgesearchtool_search_kb_on_demand import main as step_80_impl",
        "            ",
        "            step_result = step_80_impl(context)",
        "            ",
        "            payload = {",
        "                \"kb_search_completed\": True,",
        "                \"documents_found\": step_result.get(\"documents\", []),",
        "                \"search_query\": step_result.get(\"query\"),",
        "                \"relevance_scores\": step_result.get(\"scores\", []),",
        "                \"total_results\": step_result.get(\"total\", 0)",
        "            }",
        "            ",
        "            rag_step_log(\"step_80_search_kb_on_demand\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat > tests/test_rag_step_80_kb_search.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.kb import KBOrchestrator",
        "",
        "def test_step_80_search_kb_on_demand():",
        "    \"\"\"Test on-demand knowledge base search\"\"\"",
        "    orchestrator = KBOrchestrator()",
        "    ",
        "    context = {",
        "        \"search_query\": \"employment law\",",
        "        \"max_results\": 10",
        "    }",
        "    ",
        "    result = orchestrator.step_80__search_kb_on_demand(context)",
        "    ",
        "    assert result[\"kb_search_completed\"] is True",
        "    assert \"documents_found\" in result",
        "    assert \"search_query\" in result",
        "    assert \"total_results\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for on-demand KB search with relevance scoring and result limits"
    }
  },
  "step_81": {
    "step": 81,
    "blueprint_node": "CCNLTool",
    "expected_behavior": "Query labor agreements using CCNL tool",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/ccnl/step_81_rag_ccnl_ccnltool_ccnl_query_query_labor_agreements.py (exists)",
      "target_implementation": "app/orchestrators/ccnl.py:step_81__query_labor_agreements",
      "transformation_notes": "RAG step exists, create CCNL orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 81: Create CCNL Labor Agreements Query Orchestrator",
        "",
        "# 1. Create CCNL orchestrator if not exists",
        "mkdir -p app/orchestrators",
        "touch app/orchestrators/ccnl.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/ccnl.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class CCNLOrchestrator:",
        "    \"\"\"Orchestrator for CCNL (Contratto Collettivo Nazionale di Lavoro) operations\"\"\"",
        "",
        "    def step_81__query_labor_agreements(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 81: Query labor agreements using CCNL tool\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_81_query_labor_agreements\"):",
        "            from app.ragsteps.ccnl.step_81_rag_ccnl_ccnltool_ccnl_query_query_labor_agreements import main as step_81_impl",
        "            ",
        "            step_result = step_81_impl(context)",
        "            ",
        "            payload = {",
        "                \"ccnl_query_completed\": True,",
        "                \"labor_agreements\": step_result.get(\"agreements\", []),",
        "                \"query_terms\": step_result.get(\"query\"),",
        "                \"total_agreements_found\": step_result.get(\"total\", 0),",
        "                \"relevance_scores\": step_result.get(\"scores\", [])",
        "            }",
        "            ",
        "            rag_step_log(\"step_81_query_labor_agreements\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_81_ccnl_query.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.ccnl import CCNLOrchestrator",
        "",
        "def test_step_81_query_labor_agreements():",
        "    \"\"\"Test CCNL labor agreements query\"\"\"",
        "    orchestrator = CCNLOrchestrator()",
        "    ",
        "    context = {",
        "        \"query\": \"overtime compensation\",",
        "        \"sector\": \"manufacturing\"",
        "    }",
        "    ",
        "    result = orchestrator.step_81__query_labor_agreements(context)",
        "    ",
        "    assert result[\"ccnl_query_completed\"] is True",
        "    assert \"labor_agreements\" in result",
        "    assert \"total_agreements_found\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for CCNL labor agreements query with sector filtering"
    }
  },
  "step_82": {
    "step": 82,
    "blueprint_node": "ToolError",
    "expected_behavior": "Handle tool execution errors",
    "status": "missing_implementation",
    "transformation_recipe": {
      "current_implementation": "Missing - needs implementation",
      "target_implementation": "app/orchestrators/tool_execution.py:step_82__handle_tool_error",
      "transformation_notes": "Create tool error handling logic for failed tool executions",
      "claude_code_instructions": [
        "# Step 82: Add Tool Error Handling",
        "",
        "# Add error handling method to tool execution orchestrator",
        "cat >> app/orchestrators/tool_execution.py << 'EOF'",
        "",
        "    def step_82__handle_tool_error(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 82: Handle tool execution errors and provide fallback\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_82_handle_tool_error\"):",
        "            tool_errors = context.get('tool_errors', [])",
        "            error_count = len(tool_errors)",
        "            ",
        "            # Categorize errors by type",
        "            error_categories = {",
        "                \"timeout\": [],",
        "                \"invalid_params\": [],",
        "                \"service_unavailable\": [],",
        "                \"unknown\": []",
        "            }",
        "            ",
        "            for error in tool_errors:",
        "                error_type = error.get(\"type\", \"unknown\")",
        "                if error_type in error_categories:",
        "                    error_categories[error_type].append(error)",
        "                else:",
        "                    error_categories[\"unknown\"].append(error)",
        "            ",
        "            payload = {",
        "                \"tool_errors_handled\": True,",
        "                \"error_count\": error_count,",
        "                \"error_categories\": error_categories,",
        "                \"should_retry\": error_count < 3,  # Retry logic",
        "                \"fallback_required\": error_count >= 3",
        "            }",
        "            ",
        "            rag_step_log(\"step_82_handle_tool_error\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_78_tool_execution.py << 'EOF'",
        "",
        "def test_step_82_handle_tool_error():",
        "    \"\"\"Test tool error handling and categorization\"\"\"",
        "    orchestrator = ToolExecutionOrchestrator()",
        "    ",
        "    context = {",
        "        \"tool_errors\": [",
        "            {\"tool\": \"search_kb\", \"type\": \"timeout\", \"message\": \"Request timed out\"},",
        "            {\"tool\": \"calculator\", \"type\": \"invalid_params\", \"message\": \"Invalid input\"}",
        "        ]",
        "    }",
        "    ",
        "    result = orchestrator.step_82__handle_tool_error(context)",
        "    ",
        "    assert result[\"tool_errors_handled\"] is True",
        "    assert result[\"error_count\"] == 2",
        "    assert \"error_categories\" in result",
        "    assert result[\"should_retry\"] is True",
        "EOF"
      ],
      "test_requirements": "Unit test for tool error handling with categorization and retry logic"
    }
  },
  "step_83": {
    "step": 83,
    "blueprint_node": "FAQTool",
    "expected_behavior": "Query golden set FAQ database",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/golden/step_83_rag_golden_faqtool_faq_query_query_golden_set.py (exists)",
      "target_implementation": "app/orchestrators/golden.py:step_83__query_golden_set",
      "transformation_notes": "RAG step exists, add to golden orchestrator",
      "claude_code_instructions": [
        "# Step 83: Add FAQ Golden Set Query to Golden Orchestrator",
        "",
        "# Add method to existing golden orchestrator",
        "cat >> app/orchestrators/golden.py << 'EOF'",
        "",
        "    def step_83__query_golden_set(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 83: Query golden set FAQ database for high-quality answers\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_83_query_golden_set\"):",
        "            from app.ragsteps.golden.step_83_rag_golden_faqtool_faq_query_query_golden_set import main as step_83_impl",
        "            ",
        "            step_result = step_83_impl(context)",
        "            ",
        "            payload = {",
        "                \"golden_set_queried\": True,",
        "                \"faq_matches\": step_result.get(\"matches\", []),",
        "                \"confidence_scores\": step_result.get(\"scores\", []),",
        "                \"best_match\": step_result.get(\"best_match\"),",
        "                \"query_used\": step_result.get(\"query\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_83_query_golden_set\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_25_golden_confidence.py << 'EOF'",
        "",
        "def test_step_83_query_golden_set():",
        "    \"\"\"Test golden set FAQ query\"\"\"",
        "    orchestrator = GoldenOrchestrator()",
        "    ",
        "    context = {",
        "        \"user_query\": \"What are the vacation days?\",",
        "        \"max_results\": 5",
        "    }",
        "    ",
        "    result = orchestrator.step_83__query_golden_set(context)",
        "    ",
        "    assert result[\"golden_set_queried\"] is True",
        "    assert \"faq_matches\" in result",
        "    assert \"confidence_scores\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for golden set FAQ query with confidence scoring"
    }
  },
  "step_84": {
    "step": 84,
    "blueprint_node": "AttachmentValidator",
    "expected_behavior": "Validate uploaded attachments",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/preflight/step_84_rag_preflight_attachmentvalidator_validate_check_files_and_limits.py (exists)",
      "target_implementation": "app/orchestrators/documents.py:step_84__validate_attachments",
      "transformation_notes": "RAG step exists, create documents orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 84: Create Attachment Validation Orchestrator",
        "",
        "# 1. Create documents orchestrator if not exists",
        "touch app/orchestrators/documents.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/documents.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class DocumentsOrchestrator:",
        "    \"\"\"Orchestrator for document processing and validation\"\"\"",
        "",
        "    def step_84__validate_attachments(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 84: Validate uploaded attachments for size, type, and security\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_84_validate_attachments\"):",
        "            from app.ragsteps.preflight.step_84_rag_preflight_attachmentvalidator_validate_check_files_and_limits import main as step_84_impl",
        "            ",
        "            step_result = step_84_impl(context)",
        "            ",
        "            payload = {",
        "                \"attachments_validated\": True,",
        "                \"valid_attachments\": step_result.get(\"valid\", []),",
        "                \"invalid_attachments\": step_result.get(\"invalid\", []),",
        "                \"total_size_mb\": step_result.get(\"total_size_mb\", 0),",
        "                \"validation_errors\": step_result.get(\"errors\", [])",
        "            }",
        "            ",
        "            rag_step_log(\"step_84_validate_attachments\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_84_attachment_validation.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.documents import DocumentsOrchestrator",
        "",
        "def test_step_84_validate_attachments():",
        "    \"\"\"Test attachment validation\"\"\"",
        "    orchestrator = DocumentsOrchestrator()",
        "    ",
        "    context = {",
        "        \"attachments\": [",
        "            {\"name\": \"document.pdf\", \"size\": 1048576, \"type\": \"application/pdf\"},",
        "            {\"name\": \"image.jpg\", \"size\": 5242880, \"type\": \"image/jpeg\"}",
        "        ]",
        "    }",
        "    ",
        "    result = orchestrator.step_84__validate_attachments(context)",
        "    ",
        "    assert result[\"attachments_validated\"] is True",
        "    assert \"valid_attachments\" in result",
        "    assert \"total_size_mb\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for attachment validation with size and type checking"
    }
  },
  "step_85": {
    "step": 85,
    "blueprint_node": "ValidAttachments",
    "expected_behavior": "Process valid attachments for ingestion",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/preflight/step_85_rag_preflight_valid_attachments.py (exists)",
      "target_implementation": "app/orchestrators/documents.py:step_85__process_valid_attachments",
      "transformation_notes": "RAG step exists, add to documents orchestrator",
      "claude_code_instructions": [
        "# Step 85: Add Valid Attachments Processing",
        "",
        "# Add method to existing documents orchestrator",
        "cat >> app/orchestrators/documents.py << 'EOF'",
        "",
        "    def step_85__process_valid_attachments(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 85: Process valid attachments for document ingestion\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_85_process_valid_attachments\"):",
        "            from app.ragsteps.preflight.step_85_rag_preflight_valid_attachments import main as step_85_impl",
        "            ",
        "            step_result = step_85_impl(context)",
        "            ",
        "            payload = {",
        "                \"valid_attachments_processed\": True,",
        "                \"processed_attachments\": step_result.get(\"processed\", []),",
        "                \"processing_errors\": step_result.get(\"errors\", []),",
        "                \"ready_for_ingestion\": step_result.get(\"ready_count\", 0)",
        "            }",
        "            ",
        "            rag_step_log(\"step_85_process_valid_attachments\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_84_attachment_validation.py << 'EOF'",
        "",
        "def test_step_85_process_valid_attachments():",
        "    \"\"\"Test valid attachments processing\"\"\"",
        "    orchestrator = DocumentsOrchestrator()",
        "    ",
        "    context = {",
        "        \"valid_attachments\": [",
        "            {\"name\": \"contract.pdf\", \"path\": \"/tmp/contract.pdf\", \"type\": \"pdf\"}",
        "        ]",
        "    }",
        "    ",
        "    result = orchestrator.step_85__process_valid_attachments(context)",
        "    ",
        "    assert result[\"valid_attachments_processed\"] is True",
        "    assert \"processed_attachments\" in result",
        "    assert \"ready_for_ingestion\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for valid attachment processing with ingestion readiness check"
    }
  },
  "step_86": {
    "step": 86,
    "blueprint_node": "ReturnToolErrorInvalidFile",
    "expected_behavior": "Return error for invalid file tools",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_86_rag_platform_return_tool_error_invalid_file.py (exists)",
      "target_implementation": "app/orchestrators/error_handling.py:step_86__return_tool_error_invalid_file",
      "transformation_notes": "RAG step exists, add to error handling orchestrator",
      "claude_code_instructions": [
        "# Step 86: Add Tool File Error to Error Handling Orchestrator",
        "",
        "# Add method to existing error handling orchestrator",
        "cat >> app/orchestrators/error_handling.py << 'EOF'",
        "",
        "    def step_86__return_tool_error_invalid_file(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 86: Return error response for invalid file in tool execution\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_86_return_tool_error_invalid_file\"):",
        "            from app.ragsteps.platform.step_86_rag_platform_return_tool_error_invalid_file import main as step_86_impl",
        "            ",
        "            step_result = step_86_impl(context)",
        "            ",
        "            payload = {",
        "                \"tool_error_returned\": True,",
        "                \"error_type\": \"invalid_file\",",
        "                \"error_message\": step_result.get(\"message\", \"Invalid file provided\"),",
        "                \"invalid_file\": step_result.get(\"file_path\"),",
        "                \"tool_name\": step_result.get(\"tool\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_86_return_tool_error_invalid_file\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_71_error_handling.py << 'EOF'",
        "",
        "def test_step_86_return_tool_error_invalid_file():",
        "    \"\"\"Test tool invalid file error handling\"\"\"",
        "    orchestrator = ErrorHandlingOrchestrator()",
        "    ",
        "    context = {",
        "        \"tool_name\": \"document_parser\",",
        "        \"invalid_file\": \"/path/to/nonexistent.pdf\"",
        "    }",
        "    ",
        "    result = orchestrator.step_86__return_tool_error_invalid_file(context)",
        "    ",
        "    assert result[\"tool_error_returned\"] is True",
        "    assert result[\"error_type\"] == \"invalid_file\"",
        "    assert \"error_message\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for tool invalid file error with file path and tool name tracking"
    }
  },
  "step_87": {
    "step": 87,
    "blueprint_node": "DocSanitizer",
    "expected_behavior": "Sanitize documents by removing macros and JavaScript",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/docs/step_87_rag_docs_docsanitizer_sanitize_strip_macros_and_js.py (exists)",
      "target_implementation": "app/orchestrators/documents.py:step_87__sanitize_document",
      "transformation_notes": "RAG step exists, add to documents orchestrator",
      "claude_code_instructions": [
        "# Step 87: Add Document Sanitization to Documents Orchestrator",
        "",
        "# Add method to existing documents orchestrator",
        "cat >> app/orchestrators/documents.py << 'EOF'",
        "",
        "    def step_87__sanitize_document(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 87: Sanitize document by removing macros and JavaScript\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_87_sanitize_document\"):",
        "            from app.ragsteps.docs.step_87_rag_docs_docsanitizer_sanitize_strip_macros_and_js import main as step_87_impl",
        "            ",
        "            step_result = step_87_impl(context)",
        "            ",
        "            payload = {",
        "                \"document_sanitized\": True,",
        "                \"macros_removed\": step_result.get(\"macros_removed\", 0),",
        "                \"scripts_removed\": step_result.get(\"scripts_removed\", 0),",
        "                \"sanitized_path\": step_result.get(\"sanitized_file\"),",
        "                \"original_size\": step_result.get(\"original_size_bytes\"),",
        "                \"sanitized_size\": step_result.get(\"sanitized_size_bytes\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_87_sanitize_document\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_84_attachment_validation.py << 'EOF'",
        "",
        "def test_step_87_sanitize_document():",
        "    \"\"\"Test document sanitization\"\"\"",
        "    orchestrator = DocumentsOrchestrator()",
        "    ",
        "    context = {",
        "        \"document_path\": \"/tmp/document_with_macros.docx\",",
        "        \"sanitization_level\": \"strict\"",
        "    }",
        "    ",
        "    result = orchestrator.step_87__sanitize_document(context)",
        "    ",
        "    assert result[\"document_sanitized\"] is True",
        "    assert \"macros_removed\" in result",
        "    assert \"scripts_removed\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for document sanitization with macro and script removal counting"
    }
  },
  "step_88": {
    "step": 88,
    "blueprint_node": "DocClassifier",
    "expected_behavior": "Classify document type and structure",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/classify/step_88_rag_classify_docclassifier_classify_detect_document_type.py (exists)",
      "target_implementation": "app/orchestrators/classification.py:step_88__classify_document_type",
      "transformation_notes": "RAG step exists, create classification orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 88: Create Document Classification Orchestrator",
        "",
        "# 1. Create classification orchestrator if not exists",
        "touch app/orchestrators/classification.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/classification.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class ClassificationOrchestrator:",
        "    \"\"\"Orchestrator for document and query classification\"\"\"",
        "",
        "    def step_88__classify_document_type(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 88: Classify document type and detect structure\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_88_classify_document_type\"):",
        "            from app.ragsteps.classify.step_88_rag_classify_docclassifier_classify_detect_document_type import main as step_88_impl",
        "            ",
        "            step_result = step_88_impl(context)",
        "            ",
        "            payload = {",
        "                \"document_classified\": True,",
        "                \"document_type\": step_result.get(\"type\"),",
        "                \"confidence_score\": step_result.get(\"confidence\", 0.0),",
        "                \"detected_structure\": step_result.get(\"structure\", {}),",
        "                \"classification_metadata\": step_result.get(\"metadata\", {})",
        "            }",
        "            ",
        "            rag_step_log(\"step_88_classify_document_type\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_88_doc_classification.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.classification import ClassificationOrchestrator",
        "",
        "def test_step_88_classify_document_type():",
        "    \"\"\"Test document type classification\"\"\"",
        "    orchestrator = ClassificationOrchestrator()",
        "    ",
        "    context = {",
        "        \"document_path\": \"/tmp/contract.pdf\",",
        "        \"file_extension\": \"pdf\"",
        "    }",
        "    ",
        "    result = orchestrator.step_88__classify_document_type(context)",
        "    ",
        "    assert result[\"document_classified\"] is True",
        "    assert \"document_type\" in result",
        "    assert \"confidence_score\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for document classification with confidence scoring and structure detection"
    }
  },
  "step_89": {
    "step": 89,
    "blueprint_node": "DocumentType",
    "expected_behavior": "Determine specific document type routing",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/docs/step_89_rag_docs_document_type.py (exists)",
      "target_implementation": "app/orchestrators/documents.py:step_89__determine_document_type_routing",
      "transformation_notes": "RAG step exists, add to documents orchestrator",
      "claude_code_instructions": [
        "# Step 89: Add Document Type Routing to Documents Orchestrator",
        "",
        "# Add method to existing documents orchestrator",
        "cat >> app/orchestrators/documents.py << 'EOF'",
        "",
        "    def step_89__determine_document_type_routing(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 89: Determine specific document type for routing to appropriate parser\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_89_determine_document_type_routing\"):",
        "            from app.ragsteps.docs.step_89_rag_docs_document_type import main as step_89_impl",
        "            ",
        "            step_result = step_89_impl(context)",
        "            ",
        "            payload = {",
        "                \"document_type_determined\": True,",
        "                \"document_type\": step_result.get(\"type\"),",
        "                \"parser_route\": step_result.get(\"parser\"),",
        "                \"specialized_processing\": step_result.get(\"needs_special_processing\", False),",
        "                \"routing_confidence\": step_result.get(\"confidence\", 0.0)",
        "            }",
        "            ",
        "            rag_step_log(\"step_89_determine_document_type_routing\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_84_attachment_validation.py << 'EOF'",
        "",
        "def test_step_89_determine_document_type_routing():",
        "    \"\"\"Test document type routing determination\"\"\"",
        "    orchestrator = DocumentsOrchestrator()",
        "    ",
        "    context = {",
        "        \"document_type\": \"invoice\",",
        "        \"file_extension\": \"xml\"",
        "    }",
        "    ",
        "    result = orchestrator.step_89__determine_document_type_routing(context)",
        "    ",
        "    assert result[\"document_type_determined\"] is True",
        "    assert \"parser_route\" in result",
        "    assert \"routing_confidence\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for document type routing with parser selection and confidence"
    }
  },
  "step_90": {
    "step": 90,
    "blueprint_node": "FatturaParser",
    "expected_behavior": "Parse XML invoices with XSD validation",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/docs/step_90_rag_docs_fatturaparser_parse_xsd_xsd_validation.py (exists)",
      "target_implementation": "app/orchestrators/parsers.py:step_90__parse_fattura_xml",
      "transformation_notes": "RAG step exists, create parsers orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 90: Create Fattura XML Parser Orchestrator",
        "",
        "# 1. Create parsers orchestrator if not exists",
        "touch app/orchestrators/parsers.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/parsers.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class ParsersOrchestrator:",
        "    \"\"\"Orchestrator for specialized document parsers\"\"\"",
        "",
        "    def step_90__parse_fattura_xml(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 90: Parse Italian electronic invoices (Fattura XML) with XSD validation\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_90_parse_fattura_xml\"):",
        "            from app.ragsteps.docs.step_90_rag_docs_fatturaparser_parse_xsd_xsd_validation import main as step_90_impl",
        "            ",
        "            step_result = step_90_impl(context)",
        "            ",
        "            payload = {",
        "                \"fattura_parsed\": True,",
        "                \"xsd_validation_passed\": step_result.get(\"valid\", False),",
        "                \"extracted_data\": step_result.get(\"data\", {}),",
        "                \"validation_errors\": step_result.get(\"validation_errors\", []),",
        "                \"invoice_total\": step_result.get(\"total_amount\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_90_parse_fattura_xml\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_90_fattura_parser.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.parsers import ParsersOrchestrator",
        "",
        "def test_step_90_parse_fattura_xml():",
        "    \"\"\"Test Fattura XML parsing with XSD validation\"\"\"",
        "    orchestrator = ParsersOrchestrator()",
        "    ",
        "    context = {",
        "        \"xml_path\": \"/tmp/fattura_123.xml\",",
        "        \"xsd_validation\": True",
        "    }",
        "    ",
        "    result = orchestrator.step_90__parse_fattura_xml(context)",
        "    ",
        "    assert result[\"fattura_parsed\"] is True",
        "    assert \"xsd_validation_passed\" in result",
        "    assert \"extracted_data\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for Fattura XML parsing with XSD validation and data extraction"
    }
  },
  "step_91": {
    "step": 91,
    "blueprint_node": "F24Parser",
    "expected_behavior": "Parse F24 tax forms using OCR",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/docs/step_91_rag_docs_f24parser_parse_ocr_layout_aware_ocr.py (exists)",
      "target_implementation": "app/orchestrators/parsers.py:step_91__parse_f24_ocr",
      "transformation_notes": "RAG step exists, add to parsers orchestrator",
      "claude_code_instructions": [
        "# Step 91: Add F24 OCR Parser to Parsers Orchestrator",
        "",
        "# Add method to existing parsers orchestrator",
        "cat >> app/orchestrators/parsers.py << 'EOF'",
        "",
        "    def step_91__parse_f24_ocr(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 91: Parse F24 tax forms using layout-aware OCR\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_91_parse_f24_ocr\"):",
        "            from app.ragsteps.docs.step_91_rag_docs_f24parser_parse_ocr_layout_aware_ocr import main as step_91_impl",
        "            ",
        "            step_result = step_91_impl(context)",
        "            ",
        "            payload = {",
        "                \"f24_parsed\": True,",
        "                \"ocr_confidence\": step_result.get(\"ocr_confidence\", 0.0),",
        "                \"extracted_fields\": step_result.get(\"fields\", {}),",
        "                \"tax_amounts\": step_result.get(\"tax_amounts\", []),",
        "                \"form_type\": step_result.get(\"form_type\", \"F24\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_91_parse_f24_ocr\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_90_fattura_parser.py << 'EOF'",
        "",
        "def test_step_91_parse_f24_ocr():",
        "    \"\"\"Test F24 tax form OCR parsing\"\"\"",
        "    orchestrator = ParsersOrchestrator()",
        "    ",
        "    context = {",
        "        \"image_path\": \"/tmp/f24_form.pdf\",",
        "        \"ocr_engine\": \"tesseract\"",
        "    }",
        "    ",
        "    result = orchestrator.step_91__parse_f24_ocr(context)",
        "    ",
        "    assert result[\"f24_parsed\"] is True",
        "    assert \"ocr_confidence\" in result",
        "    assert \"extracted_fields\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for F24 OCR parsing with confidence scoring and field extraction"
    }
  },
  "step_92": {
    "step": 92,
    "blueprint_node": "ContractParser",
    "expected_behavior": "Parse employment contracts and legal documents",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/docs/step_92_rag_docs_contractparser_parse.py (exists)",
      "target_implementation": "app/orchestrators/parsers.py:step_92__parse_contract",
      "transformation_notes": "RAG step exists, add to parsers orchestrator",
      "claude_code_instructions": [
        "# Step 92: Add Contract Parser to Parsers Orchestrator",
        "",
        "# Add method to existing parsers orchestrator",
        "cat >> app/orchestrators/parsers.py << 'EOF'",
        "",
        "    def step_92__parse_contract(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 92: Parse employment contracts and legal documents\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_92_parse_contract\"):",
        "            from app.ragsteps.docs.step_92_rag_docs_contractparser_parse import main as step_92_impl",
        "            ",
        "            step_result = step_92_impl(context)",
        "            ",
        "            payload = {",
        "                \"contract_parsed\": True,",
        "                \"contract_type\": step_result.get(\"type\"),",
        "                \"key_clauses\": step_result.get(\"clauses\", []),",
        "                \"parties\": step_result.get(\"parties\", []),",
        "                \"effective_dates\": step_result.get(\"dates\", {}),",
        "                \"parsing_confidence\": step_result.get(\"confidence\", 0.0)",
        "            }",
        "            ",
        "            rag_step_log(\"step_92_parse_contract\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_90_fattura_parser.py << 'EOF'",
        "",
        "def test_step_92_parse_contract():",
        "    \"\"\"Test employment contract parsing\"\"\"",
        "    orchestrator = ParsersOrchestrator()",
        "    ",
        "    context = {",
        "        \"contract_path\": \"/tmp/employment_contract.pdf\",",
        "        \"contract_language\": \"italian\"",
        "    }",
        "    ",
        "    result = orchestrator.step_92__parse_contract(context)",
        "    ",
        "    assert result[\"contract_parsed\"] is True",
        "    assert \"contract_type\" in result",
        "    assert \"key_clauses\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for contract parsing with clause extraction and party identification"
    }
  },
  "step_93": {
    "step": 93,
    "blueprint_node": "PayslipParser",
    "expected_behavior": "Parse payslips and salary statements",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/docs/step_93_rag_docs_payslipparser_parse.py (exists)",
      "target_implementation": "app/orchestrators/parsers.py:step_93__parse_payslip",
      "transformation_notes": "RAG step exists, add to parsers orchestrator",
      "claude_code_instructions": [
        "# Step 93: Add Payslip Parser to Parsers Orchestrator",
        "",
        "# Add method to existing parsers orchestrator",
        "cat >> app/orchestrators/parsers.py << 'EOF'",
        "",
        "    def step_93__parse_payslip(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 93: Parse payslips and salary statements\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_93_parse_payslip\"):",
        "            from app.ragsteps.docs.step_93_rag_docs_payslipparser_parse import main as step_93_impl",
        "            ",
        "            step_result = step_93_impl(context)",
        "            ",
        "            payload = {",
        "                \"payslip_parsed\": True,",
        "                \"employee_info\": step_result.get(\"employee\", {}),",
        "                \"salary_details\": step_result.get(\"salary\", {}),",
        "                \"deductions\": step_result.get(\"deductions\", []),",
        "                \"pay_period\": step_result.get(\"period\"),",
        "                \"gross_amount\": step_result.get(\"gross\"),",
        "                \"net_amount\": step_result.get(\"net\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_93_parse_payslip\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_90_fattura_parser.py << 'EOF'",
        "",
        "def test_step_93_parse_payslip():",
        "    \"\"\"Test payslip parsing\"\"\"",
        "    orchestrator = ParsersOrchestrator()",
        "    ",
        "    context = {",
        "        \"payslip_path\": \"/tmp/payslip_jan2025.pdf\",",
        "        \"employee_id\": \"EMP123\"",
        "    }",
        "    ",
        "    result = orchestrator.step_93__parse_payslip(context)",
        "    ",
        "    assert result[\"payslip_parsed\"] is True",
        "    assert \"employee_info\" in result",
        "    assert \"salary_details\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for payslip parsing with salary calculation and deductions"
    }
  },
  "step_94": {
    "step": 94,
    "blueprint_node": "GenericOCR",
    "expected_behavior": "Perform layout-aware OCR for generic documents",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/docs/step_94_rag_docs_genericocr_parse_with_layout.py (exists)",
      "target_implementation": "app/orchestrators/parsers.py:step_94__generic_ocr_parse",
      "transformation_notes": "RAG step exists, add to parsers orchestrator",
      "claude_code_instructions": [
        "# Step 94: Add Generic OCR Parser to Parsers Orchestrator",
        "",
        "# Add method to existing parsers orchestrator",
        "cat >> app/orchestrators/parsers.py << 'EOF'",
        "",
        "    def step_94__generic_ocr_parse(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 94: Perform layout-aware OCR for generic documents\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_94_generic_ocr_parse\"):",
        "            from app.ragsteps.docs.step_94_rag_docs_genericocr_parse_with_layout import main as step_94_impl",
        "            ",
        "            step_result = step_94_impl(context)",
        "            ",
        "            payload = {",
        "                \"ocr_completed\": True,",
        "                \"extracted_text\": step_result.get(\"text\"),",
        "                \"layout_regions\": step_result.get(\"regions\", []),",
        "                \"confidence_score\": step_result.get(\"confidence\", 0.0),",
        "                \"detected_language\": step_result.get(\"language\"),",
        "                \"page_count\": step_result.get(\"pages\", 1)",
        "            }",
        "            ",
        "            rag_step_log(\"step_94_generic_ocr_parse\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_90_fattura_parser.py << 'EOF'",
        "",
        "def test_step_94_generic_ocr_parse():",
        "    \"\"\"Test generic OCR parsing with layout awareness\"\"\"",
        "    orchestrator = ParsersOrchestrator()",
        "    ",
        "    context = {",
        "        \"document_path\": \"/tmp/scanned_document.pdf\",",
        "        \"preserve_layout\": True",
        "    }",
        "    ",
        "    result = orchestrator.step_94__generic_ocr_parse(context)",
        "    ",
        "    assert result[\"ocr_completed\"] is True",
        "    assert \"extracted_text\" in result",
        "    assert \"layout_regions\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for generic OCR with layout preservation and language detection"
    }
  },
  "step_95": {
    "step": 95,
    "blueprint_node": "ExtractorExtractStructuredFields",
    "expected_behavior": "Extract structured fields from parsed documents",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/facts/step_95_rag_facts_extractor_extract_structured_fields.py (exists)",
      "target_implementation": "app/orchestrators/extraction.py:step_95__extract_structured_fields",
      "transformation_notes": "RAG step exists, create extraction orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 95: Create Structured Fields Extraction Orchestrator",
        "",
        "# 1. Create extraction orchestrator if not exists",
        "touch app/orchestrators/extraction.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/extraction.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class ExtractionOrchestrator:",
        "    \"\"\"Orchestrator for data extraction and field processing\"\"\"",
        "",
        "    def step_95__extract_structured_fields(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 95: Extract structured fields from parsed documents\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_95_extract_structured_fields\"):",
        "            from app.ragsteps.facts.step_95_rag_facts_extractor_extract_structured_fields import main as step_95_impl",
        "            ",
        "            step_result = step_95_impl(context)",
        "            ",
        "            payload = {",
        "                \"fields_extracted\": True,",
        "                \"structured_fields\": step_result.get(\"fields\", {}),",
        "                \"field_count\": step_result.get(\"count\", 0),",
        "                \"extraction_confidence\": step_result.get(\"confidence\", 0.0),",
        "                \"field_types\": step_result.get(\"types\", {})",
        "            }",
        "            ",
        "            rag_step_log(\"step_95_extract_structured_fields\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_95_field_extraction.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.extraction import ExtractionOrchestrator",
        "",
        "def test_step_95_extract_structured_fields():",
        "    \"\"\"Test structured field extraction\"\"\"",
        "    orchestrator = ExtractionOrchestrator()",
        "    ",
        "    context = {",
        "        \"parsed_document\": {\"text\": \"Contract date: 2025-01-01, Salary: \u20ac50,000\"},",
        "        \"extraction_schema\": [\"date\", \"amount\"]",
        "    }",
        "    ",
        "    result = orchestrator.step_95__extract_structured_fields(context)",
        "    ",
        "    assert result[\"fields_extracted\"] is True",
        "    assert \"structured_fields\" in result",
        "    assert \"field_count\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for structured field extraction with schema validation and confidence scoring"
    }
  },
  "step_96": {
    "step": 96,
    "blueprint_node": "BlobStorePutEncrypted",
    "expected_behavior": "Store document in encrypted blob storage with TTL",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/docs/step_96_rag_docs_blobstore_put_encrypted_ttl_storage.py (exists)",
      "target_implementation": "app/orchestrators/storage.py:step_96__store_encrypted_document",
      "transformation_notes": "RAG step exists, create storage orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 96: Create Encrypted Document Storage Orchestrator",
        "",
        "# 1. Create storage orchestrator if not exists",
        "touch app/orchestrators/storage.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/storage.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class StorageOrchestrator:",
        "    \"\"\"Orchestrator for secure document storage operations\"\"\"",
        "",
        "    def step_96__store_encrypted_document(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 96: Store document in encrypted blob storage with TTL\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_96_store_encrypted_document\"):",
        "            from app.ragsteps.docs.step_96_rag_docs_blobstore_put_encrypted_ttl_storage import main as step_96_impl",
        "            ",
        "            step_result = step_96_impl(context)",
        "            ",
        "            payload = {",
        "                \"document_stored\": True,",
        "                \"storage_key\": step_result.get(\"key\"),",
        "                \"encrypted\": step_result.get(\"encrypted\", False),",
        "                \"ttl_seconds\": step_result.get(\"ttl\"),",
        "                \"storage_size_bytes\": step_result.get(\"size\"),",
        "                \"expiry_timestamp\": step_result.get(\"expires_at\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_96_store_encrypted_document\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_96_encrypted_storage.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.storage import StorageOrchestrator",
        "",
        "def test_step_96_store_encrypted_document():",
        "    \"\"\"Test encrypted document storage with TTL\"\"\"",
        "    orchestrator = StorageOrchestrator()",
        "    ",
        "    context = {",
        "        \"document_data\": \"Sensitive contract data\",",
        "        \"ttl_hours\": 24,",
        "        \"encryption_key\": \"test-key-123\"",
        "    }",
        "    ",
        "    result = orchestrator.step_96__store_encrypted_document(context)",
        "    ",
        "    assert result[\"document_stored\"] is True",
        "    assert result[\"encrypted\"] is True",
        "    assert \"storage_key\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for encrypted document storage with TTL and key management"
    }
  },
  "step_97": {
    "step": 97,
    "blueprint_node": "ProvenanceLogLedgerEntry",
    "expected_behavior": "Create provenance log entry in ledger",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/docs/step_97_rag_docs_provenance_log_ledger_entry.py (exists)",
      "target_implementation": "app/orchestrators/ledger.py:step_97__log_provenance_entry",
      "transformation_notes": "RAG step exists, create ledger orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 97: Create Provenance Ledger Orchestrator",
        "",
        "# 1. Create ledger orchestrator if not exists",
        "touch app/orchestrators/ledger.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/ledger.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class LedgerOrchestrator:",
        "    \"\"\"Orchestrator for audit and provenance logging\"\"\"",
        "",
        "    def step_97__log_provenance_entry(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 97: Create provenance log entry in audit ledger\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_97_log_provenance_entry\"):",
        "            from app.ragsteps.docs.step_97_rag_docs_provenance_log_ledger_entry import main as step_97_impl",
        "            ",
        "            step_result = step_97_impl(context)",
        "            ",
        "            payload = {",
        "                \"provenance_logged\": True,",
        "                \"ledger_entry_id\": step_result.get(\"entry_id\"),",
        "                \"document_hash\": step_result.get(\"hash\"),",
        "                \"processing_steps\": step_result.get(\"steps\", []),",
        "                \"timestamp\": step_result.get(\"timestamp\"),",
        "                \"integrity_verified\": step_result.get(\"verified\", False)",
        "            }",
        "            ",
        "            rag_step_log(\"step_97_log_provenance_entry\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_97_provenance_ledger.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.ledger import LedgerOrchestrator",
        "",
        "def test_step_97_log_provenance_entry():",
        "    \"\"\"Test provenance ledger entry creation\"\"\"",
        "    orchestrator = LedgerOrchestrator()",
        "    ",
        "    context = {",
        "        \"document_id\": \"doc_123\",",
        "        \"processing_chain\": [\"validation\", \"parsing\", \"extraction\"],",
        "        \"user_id\": \"user_456\"",
        "    }",
        "    ",
        "    result = orchestrator.step_97__log_provenance_entry(context)",
        "    ",
        "    assert result[\"provenance_logged\"] is True",
        "    assert \"ledger_entry_id\" in result",
        "    assert \"document_hash\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for provenance logging with audit trail and integrity verification"
    }
  },
  "step_98": {
    "step": 98,
    "blueprint_node": "ConvertToToolMessageFactsAndSpans",
    "expected_behavior": "Convert extracted data to tool message format",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/facts/step_98_rag_facts_convert_to_toolmessage_facts_and_spans.py (exists)",
      "target_implementation": "app/orchestrators/message_conversion.py:step_98__convert_to_tool_message",
      "transformation_notes": "RAG step exists, add to message conversion orchestrator",
      "claude_code_instructions": [
        "# Step 98: Add Tool Message Conversion to Message Conversion Orchestrator",
        "",
        "# Add method to existing message conversion orchestrator",
        "cat >> app/orchestrators/message_conversion.py << 'EOF'",
        "",
        "    def step_98__convert_to_tool_message(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 98: Convert extracted facts and spans to tool message format\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_98_convert_to_tool_message\"):",
        "            from app.ragsteps.facts.step_98_rag_facts_convert_to_toolmessage_facts_and_spans import main as step_98_impl",
        "            ",
        "            step_result = step_98_impl(context)",
        "            ",
        "            payload = {",
        "                \"tool_message_created\": True,",
        "                \"facts\": step_result.get(\"facts\", []),",
        "                \"spans\": step_result.get(\"spans\", []),",
        "                \"message_format\": \"tool_result\",",
        "                \"conversion_metadata\": step_result.get(\"metadata\", {})",
        "            }",
        "            ",
        "            rag_step_log(\"step_98_convert_to_tool_message\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_76_message_conversion.py << 'EOF'",
        "",
        "def test_step_98_convert_to_tool_message():",
        "    \"\"\"Test tool message conversion with facts and spans\"\"\"",
        "    orchestrator = MessageConversionOrchestrator()",
        "    ",
        "    context = {",
        "        \"extracted_facts\": [{\"type\": \"salary\", \"value\": \"\u20ac50,000\"}],",
        "        \"text_spans\": [{\"start\": 10, \"end\": 17, \"text\": \"\u20ac50,000\"}]",
        "    }",
        "    ",
        "    result = orchestrator.step_98__convert_to_tool_message(context)",
        "    ",
        "    assert result[\"tool_message_created\"] is True",
        "    assert \"facts\" in result",
        "    assert \"spans\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for tool message conversion with facts and text spans"
    }
  },
  "step_99": {
    "step": 99,
    "blueprint_node": "ReturnToToolCaller",
    "expected_behavior": "Return processed result to tool caller",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_99_rag_platform_return_to_tool_caller.py (exists)",
      "target_implementation": "app/orchestrators/tool_execution.py:step_99__return_to_tool_caller",
      "transformation_notes": "RAG step exists, add to tool execution orchestrator",
      "claude_code_instructions": [
        "# Step 99: Add Return to Tool Caller to Tool Execution Orchestrator",
        "",
        "# Add method to existing tool execution orchestrator",
        "cat >> app/orchestrators/tool_execution.py << 'EOF'",
        "",
        "    def step_99__return_to_tool_caller(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 99: Return processed result to tool caller\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_99_return_to_tool_caller\"):",
        "            from app.ragsteps.platform.step_99_rag_platform_return_to_tool_caller import main as step_99_impl",
        "            ",
        "            step_result = step_99_impl(context)",
        "            ",
        "            payload = {",
        "                \"result_returned\": True,",
        "                \"tool_caller\": step_result.get(\"caller\"),",
        "                \"result_data\": step_result.get(\"result\"),",
        "                \"success\": step_result.get(\"success\", True),",
        "                \"execution_time_ms\": step_result.get(\"duration_ms\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_99_return_to_tool_caller\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_78_tool_execution.py << 'EOF'",
        "",
        "def test_step_99_return_to_tool_caller():",
        "    \"\"\"Test returning result to tool caller\"\"\"",
        "    orchestrator = ToolExecutionOrchestrator()",
        "    ",
        "    context = {",
        "        \"tool_result\": {\"data\": \"processed result\"},",
        "        \"caller_context\": {\"tool\": \"document_parser\"}",
        "    }",
        "    ",
        "    result = orchestrator.step_99__return_to_tool_caller(context)",
        "    ",
        "    assert result[\"result_returned\"] is True",
        "    assert \"tool_caller\" in result",
        "    assert \"result_data\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for tool caller result return with success status and timing"
    }
  },
  "step_100": {
    "step": 100,
    "blueprint_node": "CCNLCalculatorCalculatePerformCalculations",
    "expected_behavior": "Perform CCNL calculations for labor agreements",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/ccnl/step_100_rag_ccnl_ccnlcalculator_calculate_perform_calculations.py (exists)",
      "target_implementation": "app/orchestrators/ccnl.py:step_100__perform_calculations",
      "transformation_notes": "RAG step exists, add to CCNL orchestrator",
      "claude_code_instructions": [
        "# Step 100: Add CCNL Calculations to CCNL Orchestrator",
        "",
        "# Add method to existing CCNL orchestrator",
        "cat >> app/orchestrators/ccnl.py << 'EOF'",
        "",
        "    def step_100__perform_calculations(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 100: Perform CCNL calculations for labor agreements\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_100_perform_calculations\"):",
        "            from app.ragsteps.ccnl.step_100_rag_ccnl_ccnlcalculator_calculate_perform_calculations import main as step_100_impl",
        "            ",
        "            step_result = step_100_impl(context)",
        "            ",
        "            payload = {",
        "                \"calculations_completed\": True,",
        "                \"calculated_values\": step_result.get(\"values\", {}),",
        "                \"calculation_type\": step_result.get(\"type\"),",
        "                \"input_parameters\": step_result.get(\"inputs\", {}),",
        "                \"calculation_accuracy\": step_result.get(\"accuracy\", 0.0)",
        "            }",
        "            ",
        "            rag_step_log(\"step_100_perform_calculations\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_81_ccnl_query.py << 'EOF'",
        "",
        "def test_step_100_perform_calculations():",
        "    \"\"\"Test CCNL calculations performance\"\"\"",
        "    orchestrator = CCNLOrchestrator()",
        "    ",
        "    context = {",
        "        \"calculation_type\": \"overtime_pay\",",
        "        \"base_salary\": 3000,",
        "        \"hours_worked\": 45",
        "    }",
        "    ",
        "    result = orchestrator.step_100__perform_calculations(context)",
        "    ",
        "    assert result[\"calculations_completed\"] is True",
        "    assert \"calculated_values\" in result",
        "    assert \"calculation_accuracy\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for CCNL calculations with input validation and accuracy metrics"
    }
  },
  "step_101": {
    "step": 101,
    "blueprint_node": "ReturnToChatNodeForFinalResponse",
    "expected_behavior": "Return processing result to chat node for final response",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/response/step_101_rag_response_return_to_chat_node_for_final_response.py (exists)",
      "target_implementation": "app/orchestrators/response.py:step_101__return_to_chat_node",
      "transformation_notes": "RAG step exists, add to response orchestrator",
      "claude_code_instructions": [
        "# Step 101: Add Chat Node Return to Response Orchestrator",
        "",
        "# Add method to existing response orchestrator",
        "cat >> app/orchestrators/response.py << 'EOF'",
        "",
        "    def step_101__return_to_chat_node(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 101: Return processing result to chat node for final response\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_101_return_to_chat_node\"):",
        "            from app.ragsteps.response.step_101_rag_response_return_to_chat_node_for_final_response import main as step_101_impl",
        "            ",
        "            step_result = step_101_impl(context)",
        "            ",
        "            payload = {",
        "                \"returned_to_chat_node\": True,",
        "                \"final_response_ready\": step_result.get(\"ready\", False),",
        "                \"response_data\": step_result.get(\"response\"),",
        "                \"processing_metadata\": step_result.get(\"metadata\", {}),",
        "                \"chat_node_context\": step_result.get(\"context\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_101_return_to_chat_node\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_66_cached_response.py << 'EOF'",
        "",
        "def test_step_101_return_to_chat_node():",
        "    \"\"\"Test returning result to chat node\"\"\"",
        "    orchestrator = ResponseOrchestrator()",
        "    ",
        "    context = {",
        "        \"processed_result\": \"Final answer with citations\",",
        "        \"processing_chain\": [\"classification\", \"retrieval\", \"generation\"]",
        "    }",
        "    ",
        "    result = orchestrator.step_101__return_to_chat_node(context)",
        "    ",
        "    assert result[\"returned_to_chat_node\"] is True",
        "    assert \"final_response_ready\" in result",
        "    assert \"response_data\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for chat node return with response readiness and metadata"
    }
  },
  "step_102": {
    "step": 102,
    "blueprint_node": "LangGraphAgentProcessMessagesConvertToDict",
    "expected_behavior": "Convert processed messages to dictionary format",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/response/step_102_rag_response_langgraphagent_process_messages_convert_to_dict.py (exists)",
      "target_implementation": "app/orchestrators/message_conversion.py:step_102__convert_messages_to_dict",
      "transformation_notes": "RAG step exists, add to message conversion orchestrator",
      "claude_code_instructions": [
        "# Step 102: Add Messages to Dict Conversion to Message Conversion Orchestrator",
        "",
        "# Add method to existing message conversion orchestrator",
        "cat >> app/orchestrators/message_conversion.py << 'EOF'",
        "",
        "    def step_102__convert_messages_to_dict(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 102: Convert processed messages to dictionary format for serialization\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_102_convert_messages_to_dict\"):",
        "            from app.ragsteps.response.step_102_rag_response_langgraphagent_process_messages_convert_to_dict import main as step_102_impl",
        "            ",
        "            step_result = step_102_impl(context)",
        "            ",
        "            payload = {",
        "                \"messages_converted\": True,",
        "                \"dict_messages\": step_result.get(\"messages\", []),",
        "                \"message_count\": step_result.get(\"count\", 0),",
        "                \"conversion_format\": \"dict\",",
        "                \"serialization_ready\": step_result.get(\"serializable\", True)",
        "            }",
        "            ",
        "            rag_step_log(\"step_102_convert_messages_to_dict\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_76_message_conversion.py << 'EOF'",
        "",
        "def test_step_102_convert_messages_to_dict():",
        "    \"\"\"Test message conversion to dictionary format\"\"\"",
        "    orchestrator = MessageConversionOrchestrator()",
        "    ",
        "    context = {",
        "        \"processed_messages\": [",
        "            {\"role\": \"user\", \"content\": \"Question\"},",
        "            {\"role\": \"assistant\", \"content\": \"Answer\"}",
        "        ]",
        "    }",
        "    ",
        "    result = orchestrator.step_102__convert_messages_to_dict(context)",
        "    ",
        "    assert result[\"messages_converted\"] is True",
        "    assert \"dict_messages\" in result",
        "    assert result[\"serialization_ready\"] is True",
        "EOF"
      ],
      "test_requirements": "Unit test for message dictionary conversion with serialization validation"
    }
  },
  "step_103": {
    "step": 103,
    "blueprint_node": "LoggerInfoLogCompletion",
    "expected_behavior": "Log processing completion information",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_103_rag_platform_logger_info_log_completion.py (exists)",
      "target_implementation": "app/orchestrators/logging.py:step_103__log_completion",
      "transformation_notes": "RAG step exists, add to logging orchestrator",
      "claude_code_instructions": [
        "# Step 103: Add Completion Logging to Logging Orchestrator",
        "",
        "# Add method to existing logging orchestrator",
        "cat >> app/orchestrators/logging.py << 'EOF'",
        "",
        "    def step_103__log_completion(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 103: Log processing completion information\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_103_log_completion\"):",
        "            from app.ragsteps.platform.step_103_rag_platform_logger_info_log_completion import main as step_103_impl",
        "            ",
        "            step_result = step_103_impl(context)",
        "            ",
        "            payload = {",
        "                \"completion_logged\": True,",
        "                \"log_level\": \"INFO\",",
        "                \"completion_time\": step_result.get(\"timestamp\"),",
        "                \"processing_duration\": step_result.get(\"duration_ms\"),",
        "                \"success\": step_result.get(\"success\", True),",
        "                \"completion_details\": step_result.get(\"details\", {})",
        "            }",
        "            ",
        "            rag_step_log(\"step_103_log_completion\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_65_cache_logging.py << 'EOF'",
        "",
        "def test_step_103_log_completion():",
        "    \"\"\"Test completion logging\"\"\"",
        "    orchestrator = LoggingOrchestrator()",
        "    ",
        "    context = {",
        "        \"processing_start\": \"2025-01-01T00:00:00Z\",",
        "        \"completion_status\": \"success\"",
        "    }",
        "    ",
        "    result = orchestrator.step_103__log_completion(context)",
        "    ",
        "    assert result[\"completion_logged\"] is True",
        "    assert result[\"log_level\"] == \"INFO\"",
        "    assert \"processing_duration\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for completion logging with duration tracking and status"
    }
  },
  "step_104": {
    "step": 104,
    "blueprint_node": "StreamingRequested",
    "expected_behavior": "Check if streaming response is requested",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/streaming/step_104_rag_streaming_streaming_requested.py (exists)",
      "target_implementation": "app/orchestrators/streaming.py:step_104__check_streaming_requested",
      "transformation_notes": "RAG step exists, create streaming orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 104: Create Streaming Request Check Orchestrator",
        "",
        "# 1. Create streaming orchestrator if not exists",
        "touch app/orchestrators/streaming.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/streaming.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class StreamingOrchestrator:",
        "    \"\"\"Orchestrator for streaming response management\"\"\"",
        "",
        "    def step_104__check_streaming_requested(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 104: Check if streaming response is requested by client\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_104_check_streaming_requested\"):",
        "            from app.ragsteps.streaming.step_104_rag_streaming_streaming_requested import main as step_104_impl",
        "            ",
        "            step_result = step_104_impl(context)",
        "            ",
        "            payload = {",
        "                \"streaming_requested\": step_result.get(\"requested\", False),",
        "                \"stream_type\": step_result.get(\"type\", \"sse\"),",
        "                \"client_supports_streaming\": step_result.get(\"client_support\", False),",
        "                \"streaming_config\": step_result.get(\"config\", {})",
        "            }",
        "            ",
        "            rag_step_log(\"step_104_check_streaming_requested\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_104_streaming_check.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.streaming import StreamingOrchestrator",
        "",
        "def test_step_104_check_streaming_requested():",
        "    \"\"\"Test streaming request detection\"\"\"",
        "    orchestrator = StreamingOrchestrator()",
        "    ",
        "    context = {",
        "        \"request_headers\": {\"Accept\": \"text/event-stream\"},",
        "        \"client_capabilities\": [\"streaming\"]",
        "    }",
        "    ",
        "    result = orchestrator.step_104__check_streaming_requested(context)",
        "    ",
        "    assert \"streaming_requested\" in result",
        "    assert \"stream_type\" in result",
        "    assert \"client_supports_streaming\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for streaming request detection with client capability checking"
    }
  },
  "step_105": {
    "step": 105,
    "blueprint_node": "ChatbotControllerChatStreamSetupSSE",
    "expected_behavior": "Setup Server-Sent Events streaming",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/streaming/step_105_rag_streaming_chatbotcontroller_chat_stream_setup_sse.py (exists)",
      "target_implementation": "app/orchestrators/streaming.py:step_105__setup_sse_stream",
      "transformation_notes": "RAG step exists, add to streaming orchestrator",
      "claude_code_instructions": [
        "# Step 105: Add SSE Stream Setup to Streaming Orchestrator",
        "",
        "# Add method to existing streaming orchestrator",
        "cat >> app/orchestrators/streaming.py << 'EOF'",
        "",
        "    def step_105__setup_sse_stream(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 105: Setup Server-Sent Events streaming for response\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_105_setup_sse_stream\"):",
        "            from app.ragsteps.streaming.step_105_rag_streaming_chatbotcontroller_chat_stream_setup_sse import main as step_105_impl",
        "            ",
        "            step_result = step_105_impl(context)",
        "            ",
        "            payload = {",
        "                \"sse_stream_setup\": True,",
        "                \"stream_id\": step_result.get(\"stream_id\"),",
        "                \"connection_established\": step_result.get(\"connected\", False),",
        "                \"stream_headers\": step_result.get(\"headers\", {}),",
        "                \"keep_alive_interval\": step_result.get(\"keep_alive_seconds\", 30)",
        "            }",
        "            ",
        "            rag_step_log(\"step_105_setup_sse_stream\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_104_streaming_check.py << 'EOF'",
        "",
        "def test_step_105_setup_sse_stream():",
        "    \"\"\"Test SSE stream setup\"\"\"",
        "    orchestrator = StreamingOrchestrator()",
        "    ",
        "    context = {",
        "        \"client_id\": \"client_123\",",
        "        \"stream_config\": {\"keep_alive\": True}",
        "    }",
        "    ",
        "    result = orchestrator.step_105__setup_sse_stream(context)",
        "    ",
        "    assert result[\"sse_stream_setup\"] is True",
        "    assert \"stream_id\" in result",
        "    assert \"connection_established\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for SSE stream setup with connection validation and headers"
    }
  },
  "step_106": {
    "step": 106,
    "blueprint_node": "CreateAsyncGenerator",
    "expected_behavior": "Create async generator for streaming response",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_106_rag_platform_create_async_generator.py (exists)",
      "target_implementation": "app/orchestrators/streaming.py:step_106__create_async_generator",
      "transformation_notes": "RAG step exists, add to streaming orchestrator",
      "claude_code_instructions": [
        "# Step 106: Add Async Generator Creation to Streaming Orchestrator",
        "",
        "# Add method to existing streaming orchestrator",
        "cat >> app/orchestrators/streaming.py << 'EOF'",
        "",
        "    def step_106__create_async_generator(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 106: Create async generator for streaming response chunks\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_106_create_async_generator\"):",
        "            from app.ragsteps.platform.step_106_rag_platform_create_async_generator import main as step_106_impl",
        "            ",
        "            step_result = step_106_impl(context)",
        "            ",
        "            payload = {",
        "                \"async_generator_created\": True,",
        "                \"generator_type\": step_result.get(\"type\", \"streaming\"),",
        "                \"chunk_size\": step_result.get(\"chunk_size\", 1024),",
        "                \"streaming_ready\": step_result.get(\"ready\", False),",
        "                \"generator_config\": step_result.get(\"config\", {})",
        "            }",
        "            ",
        "            rag_step_log(\"step_106_create_async_generator\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_104_streaming_check.py << 'EOF'",
        "",
        "def test_step_106_create_async_generator():",
        "    \"\"\"Test async generator creation\"\"\"",
        "    orchestrator = StreamingOrchestrator()",
        "    ",
        "    context = {",
        "        \"response_data\": \"Long response to be streamed...\",",
        "        \"chunk_config\": {\"size\": 512}",
        "    }",
        "    ",
        "    result = orchestrator.step_106__create_async_generator(context)",
        "    ",
        "    assert result[\"async_generator_created\"] is True",
        "    assert \"generator_type\" in result",
        "    assert \"streaming_ready\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for async generator creation with chunk configuration"
    }
  },
  "step_107": {
    "step": 107,
    "blueprint_node": "SinglePassStreamPreventDoubleIteration",
    "expected_behavior": "Prevent double iteration in single-pass streaming",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/preflight/step_107_rag_preflight_singlepassstream_prevent_double_iteration.py (exists)",
      "target_implementation": "app/orchestrators/streaming.py:step_107__prevent_double_iteration",
      "transformation_notes": "RAG step exists, add to streaming orchestrator",
      "claude_code_instructions": [
        "# Step 107: Add Double Iteration Prevention to Streaming Orchestrator",
        "",
        "# Add method to existing streaming orchestrator",
        "cat >> app/orchestrators/streaming.py << 'EOF'",
        "",
        "    def step_107__prevent_double_iteration(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 107: Prevent double iteration in single-pass streaming\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_107_prevent_double_iteration\"):",
        "            from app.ragsteps.preflight.step_107_rag_preflight_singlepassstream_prevent_double_iteration import main as step_107_impl",
        "            ",
        "            step_result = step_107_impl(context)",
        "            ",
        "            payload = {",
        "                \"iteration_protected\": True,",
        "                \"single_pass_enforced\": step_result.get(\"enforced\", False),",
        "                \"iteration_state\": step_result.get(\"state\", \"initial\"),",
        "                \"protection_mechanism\": step_result.get(\"mechanism\", \"guard\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_107_prevent_double_iteration\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_104_streaming_check.py << 'EOF'",
        "",
        "def test_step_107_prevent_double_iteration():",
        "    \"\"\"Test double iteration prevention in streaming\"\"\"",
        "    orchestrator = StreamingOrchestrator()",
        "    ",
        "    context = {",
        "        \"stream_state\": \"active\",",
        "        \"iteration_count\": 1",
        "    }",
        "    ",
        "    result = orchestrator.step_107__prevent_double_iteration(context)",
        "    ",
        "    assert result[\"iteration_protected\"] is True",
        "    assert \"single_pass_enforced\" in result",
        "    assert \"iteration_state\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for double iteration prevention with state management"
    }
  },
  "step_108": {
    "step": 108,
    "blueprint_node": "WriteSSEFormatChunks",
    "expected_behavior": "Write response chunks in SSE format",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/streaming/step_108_rag_streaming_write_sse_format_chunks.py (exists)",
      "target_implementation": "app/orchestrators/streaming.py:step_108__write_sse_chunks",
      "transformation_notes": "RAG step exists, add to streaming orchestrator",
      "claude_code_instructions": [
        "# Step 108: Add SSE Chunk Writing to Streaming Orchestrator",
        "",
        "# Add method to existing streaming orchestrator",
        "cat >> app/orchestrators/streaming.py << 'EOF'",
        "",
        "    def step_108__write_sse_chunks(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 108: Write response chunks in Server-Sent Events format\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_108_write_sse_chunks\"):",
        "            from app.ragsteps.streaming.step_108_rag_streaming_write_sse_format_chunks import main as step_108_impl",
        "            ",
        "            step_result = step_108_impl(context)",
        "            ",
        "            payload = {",
        "                \"sse_chunks_written\": True,",
        "                \"chunks_sent\": step_result.get(\"chunks_count\", 0),",
        "                \"bytes_sent\": step_result.get(\"bytes\", 0),",
        "                \"sse_format_valid\": step_result.get(\"valid_format\", True),",
        "                \"final_chunk_sent\": step_result.get(\"final\", False)",
        "            }",
        "            ",
        "            rag_step_log(\"step_108_write_sse_chunks\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_104_streaming_check.py << 'EOF'",
        "",
        "def test_step_108_write_sse_chunks():",
        "    \"\"\"Test SSE chunk writing\"\"\"",
        "    orchestrator = StreamingOrchestrator()",
        "    ",
        "    context = {",
        "        \"response_chunks\": [\"chunk1\", \"chunk2\", \"chunk3\"],",
        "        \"stream_id\": \"stream_123\"",
        "    }",
        "    ",
        "    result = orchestrator.step_108__write_sse_chunks(context)",
        "    ",
        "    assert result[\"sse_chunks_written\"] is True",
        "    assert \"chunks_sent\" in result",
        "    assert \"bytes_sent\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for SSE chunk writing with format validation and metrics"
    }
  },
  "step_109": {
    "step": 109,
    "blueprint_node": "StreamingResponseSendChunks",
    "expected_behavior": "Send streaming response chunks to client",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/streaming/step_109_rag_streaming_streamingresponse_send_chunks.py (exists)",
      "target_implementation": "app/orchestrators/streaming.py:step_109__send_chunks",
      "transformation_notes": "RAG step exists, add to streaming orchestrator",
      "claude_code_instructions": [
        "# Step 109: Add Chunk Sending to Streaming Orchestrator",
        "",
        "# Add method to existing streaming orchestrator",
        "cat >> app/orchestrators/streaming.py << 'EOF'",
        "",
        "    def step_109__send_chunks(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 109: Send streaming response chunks to client\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_109_send_chunks\"):",
        "            from app.ragsteps.streaming.step_109_rag_streaming_streamingresponse_send_chunks import main as step_109_impl",
        "            ",
        "            step_result = step_109_impl(context)",
        "            ",
        "            payload = {",
        "                \"chunks_sent\": True,",
        "                \"total_chunks\": step_result.get(\"total\", 0),",
        "                \"successful_sends\": step_result.get(\"successful\", 0),",
        "                \"failed_sends\": step_result.get(\"failed\", 0),",
        "                \"streaming_completed\": step_result.get(\"completed\", False),",
        "                \"client_disconnected\": step_result.get(\"disconnected\", False)",
        "            }",
        "            ",
        "            rag_step_log(\"step_109_send_chunks\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_104_streaming_check.py << 'EOF'",
        "",
        "def test_step_109_send_chunks():",
        "    \"\"\"Test streaming chunk transmission\"\"\"",
        "    orchestrator = StreamingOrchestrator()",
        "    ",
        "    context = {",
        "        \"prepared_chunks\": [\"data: chunk1\\n\\n\", \"data: chunk2\\n\\n\"],",
        "        \"client_connection\": {\"active\": True}",
        "    }",
        "    ",
        "    result = orchestrator.step_109__send_chunks(context)",
        "    ",
        "    assert result[\"chunks_sent\"] is True",
        "    assert \"total_chunks\" in result",
        "    assert \"streaming_completed\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for chunk transmission with success/failure tracking and completion status"
    }
  },
  "step_110": {
    "step": 110,
    "blueprint_node": "SendDoneFrame",
    "expected_behavior": "Send completion frame to indicate streaming is finished",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_110_rag_platform_send_done_frame.py (exists)",
      "target_implementation": "app/orchestrators/streaming.py:step_110__send_done_frame",
      "transformation_notes": "RAG step exists, add to streaming orchestrator",
      "claude_code_instructions": [
        "# Step 110: Add Done Frame Sending to Streaming Orchestrator",
        "",
        "# Add method to existing streaming orchestrator",
        "cat >> app/orchestrators/streaming.py << 'EOF'",
        "",
        "    def step_110__send_done_frame(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 110: Send completion frame to indicate streaming is finished\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_110_send_done_frame\"):",
        "            from app.ragsteps.platform.step_110_rag_platform_send_done_frame import main as step_110_impl",
        "            ",
        "            step_result = step_110_impl(context)",
        "            ",
        "            payload = {",
        "                \"done_frame_sent\": True,",
        "                \"stream_closed\": step_result.get(\"closed\", False),",
        "                \"final_metadata\": step_result.get(\"metadata\", {}),",
        "                \"completion_timestamp\": step_result.get(\"timestamp\"),",
        "                \"stream_duration_ms\": step_result.get(\"duration\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_110_send_done_frame\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_104_streaming_check.py << 'EOF'",
        "",
        "def test_step_110_send_done_frame():",
        "    \"\"\"Test done frame transmission\"\"\"",
        "    orchestrator = StreamingOrchestrator()",
        "    ",
        "    context = {",
        "        \"stream_start_time\": \"2025-01-01T00:00:00Z\",",
        "        \"total_chunks_sent\": 15",
        "    }",
        "    ",
        "    result = orchestrator.step_110__send_done_frame(context)",
        "    ",
        "    assert result[\"done_frame_sent\"] is True",
        "    assert \"stream_closed\" in result",
        "    assert \"completion_timestamp\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for done frame with stream closure and completion metadata"
    }
  },
  "step_111": {
    "step": 111,
    "blueprint_node": "CollectUsageMetrics",
    "expected_behavior": "Collect comprehensive usage metrics for analytics",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/metrics/step_111_rag_metrics_collect_usage_metrics.py (exists)",
      "target_implementation": "app/orchestrators/metrics.py:step_111__collect_usage_metrics",
      "transformation_notes": "RAG step exists, add to metrics orchestrator",
      "claude_code_instructions": [
        "# Step 111: Add Usage Metrics Collection to Metrics Orchestrator",
        "",
        "# Add method to existing metrics orchestrator",
        "cat >> app/orchestrators/metrics.py << 'EOF'",
        "",
        "    def step_111__collect_usage_metrics(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 111: Collect comprehensive usage metrics for analytics\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_111_collect_usage_metrics\"):",
        "            from app.ragsteps.metrics.step_111_rag_metrics_collect_usage_metrics import main as step_111_impl",
        "            ",
        "            step_result = step_111_impl(context)",
        "            ",
        "            payload = {",
        "                \"usage_metrics_collected\": True,",
        "                \"query_count\": step_result.get(\"queries\", 0),",
        "                \"response_time_ms\": step_result.get(\"response_time\", 0),",
        "                \"token_usage\": step_result.get(\"tokens\", 0),",
        "                \"cache_hit_rate\": step_result.get(\"cache_rate\", 0.0),",
        "                \"user_satisfaction\": step_result.get(\"satisfaction\"),",
        "                \"error_count\": step_result.get(\"errors\", 0)",
        "            }",
        "            ",
        "            rag_step_log(\"step_111_collect_usage_metrics\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_63_cache_metrics.py << 'EOF'",
        "",
        "def test_step_111_collect_usage_metrics():",
        "    \"\"\"Test comprehensive usage metrics collection\"\"\"",
        "    orchestrator = MetricsOrchestrator()",
        "    ",
        "    context = {",
        "        \"session_data\": {",
        "            \"queries\": 5,",
        "            \"total_time\": 2500,",
        "            \"tokens_used\": 750",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_111__collect_usage_metrics(context)",
        "    ",
        "    assert result[\"usage_metrics_collected\"] is True",
        "    assert \"query_count\" in result",
        "    assert \"response_time_ms\" in result",
        "    assert \"token_usage\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for comprehensive usage metrics with query count, timing, and satisfaction tracking"
    }
  },
  "step_112": {
    "step": 112,
    "blueprint_node": "ReturnResponseToUser",
    "expected_behavior": "Return final response to user with all processing complete",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/response/step_112_rag_response_return_response_to_user.py (exists)",
      "target_implementation": "app/orchestrators/response.py:step_112__return_response_to_user",
      "transformation_notes": "RAG step exists, add to response orchestrator",
      "claude_code_instructions": [
        "# Step 112: Add Final Response Return to Response Orchestrator",
        "",
        "# Add method to existing response orchestrator",
        "cat >> app/orchestrators/response.py << 'EOF'",
        "",
        "    def step_112__return_response_to_user(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 112: Return final response to user with all processing complete\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_112_return_response_to_user\"):",
        "            from app.ragsteps.response.step_112_rag_response_return_response_to_user import main as step_112_impl",
        "            ",
        "            step_result = step_112_impl(context)",
        "            ",
        "            payload = {",
        "                \"response_delivered\": True,",
        "                \"response_content\": step_result.get(\"content\"),",
        "                \"response_metadata\": step_result.get(\"metadata\", {}),",
        "                \"citations\": step_result.get(\"citations\", []),",
        "                \"confidence_score\": step_result.get(\"confidence\"),",
        "                \"processing_time_ms\": step_result.get(\"processing_time\"),",
        "                \"user_id\": context.get(\"user_id\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_112_return_response_to_user\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_66_cached_response.py << 'EOF'",
        "",
        "def test_step_112_return_response_to_user():",
        "    \"\"\"Test final response delivery to user\"\"\"",
        "    orchestrator = ResponseOrchestrator()",
        "    ",
        "    context = {",
        "        \"final_response\": \"Your vacation days are calculated based on...\",",
        "        \"user_id\": \"user_123\",",
        "        \"processing_start\": \"2025-01-01T00:00:00Z\"",
        "    }",
        "    ",
        "    result = orchestrator.step_112__return_response_to_user(context)",
        "    ",
        "    assert result[\"response_delivered\"] is True",
        "    assert \"response_content\" in result",
        "    assert \"processing_time_ms\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for final response delivery with content, citations, and timing metadata"
    }
  },
  "step_113": {
    "step": 113,
    "blueprint_node": "FeedbackUIShowOptions",
    "expected_behavior": "Show feedback options (correct, incomplete, wrong) to user",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/feedback/step_113_rag_feedback_feedbackui_show_options_correct_incomplete_wrong.py (exists)",
      "target_implementation": "app/orchestrators/feedback.py:step_113__show_feedback_options",
      "transformation_notes": "RAG step exists, create feedback orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 113: Create Feedback UI Options Orchestrator",
        "",
        "# 1. Create feedback orchestrator if not exists",
        "touch app/orchestrators/feedback.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/feedback.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class FeedbackOrchestrator:",
        "    \"\"\"Orchestrator for user feedback collection and processing\"\"\"",
        "",
        "    def step_113__show_feedback_options(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 113: Show feedback options (correct, incomplete, wrong) to user\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_113_show_feedback_options\"):",
        "            from app.ragsteps.feedback.step_113_rag_feedback_feedbackui_show_options_correct_incomplete_wrong import main as step_113_impl",
        "            ",
        "            step_result = step_113_impl(context)",
        "            ",
        "            payload = {",
        "                \"feedback_options_shown\": True,",
        "                \"available_options\": [\"correct\", \"incomplete\", \"wrong\"],",
        "                \"feedback_ui_rendered\": step_result.get(\"ui_rendered\", False),",
        "                \"response_id\": step_result.get(\"response_id\"),",
        "                \"feedback_timeout_seconds\": step_result.get(\"timeout\", 300)",
        "            }",
        "            ",
        "            rag_step_log(\"step_113_show_feedback_options\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_113_feedback_ui.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.feedback import FeedbackOrchestrator",
        "",
        "def test_step_113_show_feedback_options():",
        "    \"\"\"Test feedback options UI display\"\"\"",
        "    orchestrator = FeedbackOrchestrator()",
        "    ",
        "    context = {",
        "        \"response_id\": \"resp_123\",",
        "        \"user_id\": \"user_456\"",
        "    }",
        "    ",
        "    result = orchestrator.step_113__show_feedback_options(context)",
        "    ",
        "    assert result[\"feedback_options_shown\"] is True",
        "    assert \"correct\" in result[\"available_options\"]",
        "    assert \"incomplete\" in result[\"available_options\"]",
        "    assert \"wrong\" in result[\"available_options\"]",
        "EOF"
      ],
      "test_requirements": "Unit test for feedback UI with three options display and timeout configuration"
    }
  },
  "step_114": {
    "step": 114,
    "blueprint_node": "UserProvidesFeedback",
    "expected_behavior": "Handle user feedback submission",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/feedback/step_114_rag_feedback_user_provides_feedback.py (exists)",
      "target_implementation": "app/orchestrators/feedback.py:step_114__handle_user_feedback",
      "transformation_notes": "RAG step exists, add to feedback orchestrator",
      "claude_code_instructions": [
        "# Step 114: Add User Feedback Handling to Feedback Orchestrator",
        "",
        "# Add method to existing feedback orchestrator",
        "cat >> app/orchestrators/feedback.py << 'EOF'",
        "",
        "    def step_114__handle_user_feedback(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 114: Handle user feedback submission and validation\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_114_handle_user_feedback\"):",
        "            from app.ragsteps.feedback.step_114_rag_feedback_user_provides_feedback import main as step_114_impl",
        "            ",
        "            step_result = step_114_impl(context)",
        "            ",
        "            payload = {",
        "                \"feedback_received\": True,",
        "                \"feedback_type\": step_result.get(\"type\"),",
        "                \"feedback_details\": step_result.get(\"details\"),",
        "                \"user_id\": step_result.get(\"user_id\"),",
        "                \"response_id\": step_result.get(\"response_id\"),",
        "                \"feedback_timestamp\": step_result.get(\"timestamp\"),",
        "                \"feedback_valid\": step_result.get(\"valid\", True)",
        "            }",
        "            ",
        "            rag_step_log(\"step_114_handle_user_feedback\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_113_feedback_ui.py << 'EOF'",
        "",
        "def test_step_114_handle_user_feedback():",
        "    \"\"\"Test user feedback handling and validation\"\"\"",
        "    orchestrator = FeedbackOrchestrator()",
        "    ",
        "    context = {",
        "        \"user_feedback\": {",
        "            \"type\": \"incomplete\",",
        "            \"details\": \"Missing information about overtime rates\"",
        "        },",
        "        \"response_id\": \"resp_123\",",
        "        \"user_id\": \"user_456\"",
        "    }",
        "    ",
        "    result = orchestrator.step_114__handle_user_feedback(context)",
        "    ",
        "    assert result[\"feedback_received\"] is True",
        "    assert result[\"feedback_type\"] == \"incomplete\"",
        "    assert \"feedback_details\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for user feedback handling with validation and timestamp tracking"
    }
  },
  "step_115": {
    "step": 115,
    "blueprint_node": "NoFeedback",
    "expected_behavior": "Handle scenario when user provides no feedback",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/feedback/step_115_rag_feedback_no_feedback.py (exists)",
      "target_implementation": "app/orchestrators/feedback.py:step_115__handle_no_feedback",
      "transformation_notes": "RAG step exists, add to feedback orchestrator",
      "claude_code_instructions": [
        "# Step 115: Add No Feedback Handling to Feedback Orchestrator",
        "",
        "# Add method to existing feedback orchestrator",
        "cat >> app/orchestrators/feedback.py << 'EOF'",
        "",
        "    def step_115__handle_no_feedback(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 115: Handle scenario when user provides no feedback\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_115_handle_no_feedback\"):",
        "            from app.ragsteps.feedback.step_115_rag_feedback_no_feedback import main as step_115_impl",
        "            ",
        "            step_result = step_115_impl(context)",
        "            ",
        "            payload = {",
        "                \"no_feedback_handled\": True,",
        "                \"timeout_occurred\": step_result.get(\"timeout\", False),",
        "                \"implicit_satisfaction\": step_result.get(\"assumed_satisfied\", True),",
        "                \"response_id\": step_result.get(\"response_id\"),",
        "                \"waiting_time_seconds\": step_result.get(\"wait_time\", 0)",
        "            }",
        "            ",
        "            rag_step_log(\"step_115_handle_no_feedback\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_113_feedback_ui.py << 'EOF'",
        "",
        "def test_step_115_handle_no_feedback():",
        "    \"\"\"Test no feedback scenario handling\"\"\"",
        "    orchestrator = FeedbackOrchestrator()",
        "    ",
        "    context = {",
        "        \"response_id\": \"resp_123\",",
        "        \"feedback_timeout\": True,",
        "        \"wait_duration\": 300",
        "    }",
        "    ",
        "    result = orchestrator.step_115__handle_no_feedback(context)",
        "    ",
        "    assert result[\"no_feedback_handled\"] is True",
        "    assert \"timeout_occurred\" in result",
        "    assert \"implicit_satisfaction\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for no feedback handling with timeout detection and implicit satisfaction scoring"
    }
  },
  "step_116": {
    "step": 116,
    "blueprint_node": "FeedbackTypeSelected",
    "expected_behavior": "Process selected feedback type and route accordingly",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/feedback/step_116_rag_feedback_feedback_type_selected.py (exists)",
      "target_implementation": "app/orchestrators/feedback.py:step_116__process_feedback_type",
      "transformation_notes": "RAG step exists, add to feedback orchestrator",
      "claude_code_instructions": [
        "# Step 116: Add Feedback Type Processing to Feedback Orchestrator",
        "",
        "# Add method to existing feedback orchestrator",
        "cat >> app/orchestrators/feedback.py << 'EOF'",
        "",
        "    def step_116__process_feedback_type(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 116: Process selected feedback type and route accordingly\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_116_process_feedback_type\"):",
        "            from app.ragsteps.feedback.step_116_rag_feedback_feedback_type_selected import main as step_116_impl",
        "            ",
        "            step_result = step_116_impl(context)",
        "            ",
        "            # Route based on feedback type",
        "            feedback_type = step_result.get('type')",
        "            routing_actions = {",
        "                \"correct\": \"positive_feedback_path\",",
        "                \"incomplete\": \"improvement_needed_path\", ",
        "                \"wrong\": \"error_correction_path\"",
        "            }",
        "            ",
        "            payload = {",
        "                \"feedback_type_processed\": True,",
        "                \"selected_type\": feedback_type,",
        "                \"routing_path\": routing_actions.get(feedback_type, \"unknown\"),",
        "                \"requires_expert_review\": feedback_type in [\"incomplete\", \"wrong\"],",
        "                \"quality_score_impact\": step_result.get(\"score_delta\", 0)",
        "            }",
        "            ",
        "            rag_step_log(\"step_116_process_feedback_type\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_113_feedback_ui.py << 'EOF'",
        "",
        "def test_step_116_process_feedback_type():",
        "    \"\"\"Test feedback type processing and routing\"\"\"",
        "    orchestrator = FeedbackOrchestrator()",
        "    ",
        "    context = {",
        "        \"feedback_type\": \"incomplete\",",
        "        \"response_id\": \"resp_123\"",
        "    }",
        "    ",
        "    result = orchestrator.step_116__process_feedback_type(context)",
        "    ",
        "    assert result[\"feedback_type_processed\"] is True",
        "    assert result[\"selected_type\"] == \"incomplete\"",
        "    assert result[\"routing_path\"] == \"improvement_needed_path\"",
        "    assert result[\"requires_expert_review\"] is True",
        "EOF"
      ],
      "test_requirements": "Unit test for feedback type processing with routing logic and expert review flags"
    }
  },
  "step_117": {
    "step": 117,
    "blueprint_node": "PostApiv1FaqFeedback",
    "expected_behavior": "Submit FAQ feedback to API endpoint",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/golden/step_117_rag_golden_post_api_v1_faq_feedback.py (exists)",
      "target_implementation": "app/orchestrators/api.py:step_117__post_faq_feedback",
      "transformation_notes": "RAG step exists, create API orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 117: Create FAQ Feedback API Orchestrator",
        "",
        "# 1. Create API orchestrator if not exists",
        "touch app/orchestrators/api.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/api.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class APIOrchestrator:",
        "    \"\"\"Orchestrator for external API calls and integrations\"\"\"",
        "",
        "    def step_117__post_faq_feedback(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 117: Submit FAQ feedback to API endpoint\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_117_post_faq_feedback\"):",
        "            from app.ragsteps.golden.step_117_rag_golden_post_api_v1_faq_feedback import main as step_117_impl",
        "            ",
        "            step_result = step_117_impl(context)",
        "            ",
        "            payload = {",
        "                \"faq_feedback_posted\": True,",
        "                \"api_response_status\": step_result.get(\"status_code\"),",
        "                \"feedback_id\": step_result.get(\"feedback_id\"),",
        "                \"api_success\": step_result.get(\"success\", False),",
        "                \"api_error\": step_result.get(\"error\") if not step_result.get(\"success\") else None",
        "            }",
        "            ",
        "            rag_step_log(\"step_117_post_faq_feedback\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_117_api_feedback.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.api import APIOrchestrator",
        "",
        "def test_step_117_post_faq_feedback():",
        "    \"\"\"Test FAQ feedback API submission\"\"\"",
        "    orchestrator = APIOrchestrator()",
        "    ",
        "    context = {",
        "        \"feedback_data\": {",
        "            \"faq_id\": \"faq_123\",",
        "            \"user_rating\": \"helpful\",",
        "            \"comments\": \"Very clear explanation\"",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_117__post_faq_feedback(context)",
        "    ",
        "    assert result[\"faq_feedback_posted\"] is True",
        "    assert \"api_response_status\" in result",
        "    assert \"feedback_id\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for FAQ feedback API submission with status code and error handling"
    }
  },
  "step_118": {
    "step": 118,
    "blueprint_node": "PostApiv1KnowledgeFeedback",
    "expected_behavior": "Submit knowledge base feedback to API endpoint",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/kb/step_118_rag_kb_post_api_v1_knowledge_feedback.py (exists)",
      "target_implementation": "app/orchestrators/api.py:step_118__post_kb_feedback",
      "transformation_notes": "RAG step exists, add to API orchestrator",
      "claude_code_instructions": [
        "# Step 118: Add Knowledge Base Feedback to API Orchestrator",
        "",
        "# Add method to existing API orchestrator",
        "cat >> app/orchestrators/api.py << 'EOF'",
        "",
        "    def step_118__post_kb_feedback(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 118: Submit knowledge base feedback to API endpoint\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_118_post_kb_feedback\"):",
        "            from app.ragsteps.kb.step_118_rag_kb_post_api_v1_knowledge_feedback import main as step_118_impl",
        "            ",
        "            step_result = step_118_impl(context)",
        "            ",
        "            payload = {",
        "                \"kb_feedback_posted\": True,",
        "                \"api_response_status\": step_result.get(\"status_code\"),",
        "                \"feedback_id\": step_result.get(\"feedback_id\"),",
        "                \"knowledge_item_id\": step_result.get(\"kb_item_id\"),",
        "                \"api_success\": step_result.get(\"success\", False)",
        "            }",
        "            ",
        "            rag_step_log(\"step_118_post_kb_feedback\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_117_api_feedback.py << 'EOF'",
        "",
        "def test_step_118_post_kb_feedback():",
        "    \"\"\"Test knowledge base feedback API submission\"\"\"",
        "    orchestrator = APIOrchestrator()",
        "    ",
        "    context = {",
        "        \"kb_feedback\": {",
        "            \"kb_item_id\": \"kb_456\",",
        "            \"relevance_score\": 0.8,",
        "            \"user_comments\": \"Information was outdated\"",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_118__post_kb_feedback(context)",
        "    ",
        "    assert result[\"kb_feedback_posted\"] is True",
        "    assert \"knowledge_item_id\" in result",
        "    assert \"api_response_status\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for knowledge base feedback API with item ID tracking and relevance scoring"
    }
  },
  "step_119": {
    "step": 119,
    "blueprint_node": "ExpertFeedbackCollectorCollectFeedback",
    "expected_behavior": "Collect expert feedback for quality assurance",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/metrics/step_119_rag_metrics_expertfeedbackcollector_collect_feedback.py (exists)",
      "target_implementation": "app/orchestrators/feedback.py:step_119__collect_expert_feedback",
      "transformation_notes": "RAG step exists, add to feedback orchestrator",
      "claude_code_instructions": [
        "# Step 119: Add Expert Feedback Collection to Feedback Orchestrator",
        "",
        "# Add method to existing feedback orchestrator",
        "cat >> app/orchestrators/feedback.py << 'EOF'",
        "",
        "    def step_119__collect_expert_feedback(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 119: Collect expert feedback for quality assurance\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_119_collect_expert_feedback\"):",
        "            from app.ragsteps.metrics.step_119_rag_metrics_expertfeedbackcollector_collect_feedback import main as step_119_impl",
        "            ",
        "            step_result = step_119_impl(context)",
        "            ",
        "            payload = {",
        "                \"expert_feedback_collected\": True,",
        "                \"expert_id\": step_result.get(\"expert_id\"),",
        "                \"quality_rating\": step_result.get(\"rating\"),",
        "                \"accuracy_score\": step_result.get(\"accuracy\"),",
        "                \"completeness_score\": step_result.get(\"completeness\"),",
        "                \"expert_comments\": step_result.get(\"comments\"),",
        "                \"review_timestamp\": step_result.get(\"timestamp\"),",
        "                \"needs_improvement\": step_result.get(\"needs_work\", False)",
        "            }",
        "            ",
        "            rag_step_log(\"step_119_collect_expert_feedback\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_113_feedback_ui.py << 'EOF'",
        "",
        "def test_step_119_collect_expert_feedback():",
        "    \"\"\"Test expert feedback collection\"\"\"",
        "    orchestrator = FeedbackOrchestrator()",
        "    ",
        "    context = {",
        "        \"response_for_review\": \"Employment contract details...\",",
        "        \"expert_assignment\": \"legal_expert_001\"",
        "    }",
        "    ",
        "    result = orchestrator.step_119__collect_expert_feedback(context)",
        "    ",
        "    assert result[\"expert_feedback_collected\"] is True",
        "    assert \"expert_id\" in result",
        "    assert \"quality_rating\" in result",
        "    assert \"accuracy_score\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for expert feedback collection with quality ratings and improvement flags"
    }
  },
  "step_120": {
    "step": 120,
    "blueprint_node": "ValidateExpertCredentials",
    "expected_behavior": "Validate expert credentials and authorization",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_120_rag_platform_validate_expert_credentials.py (exists)",
      "target_implementation": "app/orchestrators/authentication.py:step_120__validate_expert_credentials",
      "transformation_notes": "RAG step exists, create authentication orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 120: Create Expert Credentials Validation Orchestrator",
        "",
        "# 1. Create authentication orchestrator if not exists",
        "touch app/orchestrators/authentication.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/authentication.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class AuthenticationOrchestrator:",
        "    \"\"\"Orchestrator for user authentication and authorization\"\"\"",
        "",
        "    def step_120__validate_expert_credentials(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 120: Validate expert credentials and authorization\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_120_validate_expert_credentials\"):",
        "            from app.ragsteps.platform.step_120_rag_platform_validate_expert_credentials import main as step_120_impl",
        "            ",
        "            step_result = step_120_impl(context)",
        "            ",
        "            payload = {",
        "                \"credentials_validated\": True,",
        "                \"expert_authorized\": step_result.get(\"authorized\", False),",
        "                \"expert_id\": step_result.get(\"expert_id\"),",
        "                \"expertise_areas\": step_result.get(\"expertise\", []),",
        "                \"validation_level\": step_result.get(\"level\", \"basic\"),",
        "                \"credential_expiry\": step_result.get(\"expires_at\"),",
        "                \"permissions\": step_result.get(\"permissions\", [])",
        "            }",
        "            ",
        "            rag_step_log(\"step_120_validate_expert_credentials\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_120_expert_auth.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.authentication import AuthenticationOrchestrator",
        "",
        "def test_step_120_validate_expert_credentials():",
        "    \"\"\"Test expert credentials validation\"\"\"",
        "    orchestrator = AuthenticationOrchestrator()",
        "    ",
        "    context = {",
        "        \"expert_token\": \"expert_jwt_token_123\",",
        "        \"required_expertise\": \"labor_law\"",
        "    }",
        "    ",
        "    result = orchestrator.step_120__validate_expert_credentials(context)",
        "    ",
        "    assert result[\"credentials_validated\"] is True",
        "    assert \"expert_authorized\" in result",
        "    assert \"expertise_areas\" in result",
        "    assert \"permissions\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for expert credentials validation with expertise matching and permission checking"
    }
  },
  "step_121": {
    "step": 121,
    "blueprint_node": "TrustScoreAtLeast07",
    "expected_behavior": "Validate trust score is at least 0.7 for high confidence",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/classify/step_121_rag_classify_trust_score_at_least_0_7.py (exists)",
      "target_implementation": "app/orchestrators/trust.py:step_121__validate_trust_score",
      "transformation_notes": "RAG step exists, create trust scoring orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 121: Create Trust Score Validation Orchestrator",
        "",
        "# 1. Create trust orchestrator if not exists",
        "touch app/orchestrators/trust.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/trust.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class TrustOrchestrator:",
        "    \"\"\"Orchestrator for trust scoring and confidence validation\"\"\"",
        "",
        "    def step_121__validate_trust_score(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 121: Validate trust score is at least 0.7 for high confidence\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_121_validate_trust_score\"):",
        "            from app.ragsteps.classify.step_121_rag_classify_trust_score_at_least_0_7 import main as step_121_impl",
        "            ",
        "            step_result = step_121_impl(context)",
        "            ",
        "            payload = {",
        "                \"trust_score_validated\": True,",
        "                \"trust_score\": step_result.get(\"score\", 0.0),",
        "                \"meets_threshold\": step_result.get(\"score\", 0.0) >= 0.7,",
        "                \"threshold\": 0.7,",
        "                \"confidence_level\": \"high\" if step_result.get(\"score\", 0.0) >= 0.7 else \"low\",",
        "                \"score_components\": step_result.get(\"components\", {})",
        "            }",
        "            ",
        "            rag_step_log(\"step_121_validate_trust_score\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_121_trust_score.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.trust import TrustOrchestrator",
        "",
        "def test_step_121_validate_trust_score():",
        "    \"\"\"Test trust score validation with 0.7 threshold\"\"\"",
        "    orchestrator = TrustOrchestrator()",
        "    ",
        "    context = {",
        "        \"response_confidence\": 0.85,",
        "        \"source_reliability\": 0.9,",
        "        \"factual_accuracy\": 0.8",
        "    }",
        "    ",
        "    result = orchestrator.step_121__validate_trust_score(context)",
        "    ",
        "    assert result[\"trust_score_validated\"] is True",
        "    assert \"trust_score\" in result",
        "    assert \"meets_threshold\" in result",
        "    assert result[\"threshold\"] == 0.7",
        "EOF"
      ],
      "test_requirements": "Unit test for trust score validation with threshold checking and confidence level assignment"
    }
  },
  "step_122": {
    "step": 122,
    "blueprint_node": "FeedbackRejected",
    "expected_behavior": "Handle rejected feedback scenarios",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/feedback/step_122_rag_feedback_feedback_rejected.py (exists)",
      "target_implementation": "app/orchestrators/feedback.py:step_122__handle_feedback_rejected",
      "transformation_notes": "RAG step exists, add to feedback orchestrator",
      "claude_code_instructions": [
        "# Step 122: Add Feedback Rejection Handling to Feedback Orchestrator",
        "",
        "# Add method to existing feedback orchestrator",
        "cat >> app/orchestrators/feedback.py << 'EOF'",
        "",
        "    def step_122__handle_feedback_rejected(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 122: Handle rejected feedback scenarios\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_122_handle_feedback_rejected\"):",
        "            from app.ragsteps.feedback.step_122_rag_feedback_feedback_rejected import main as step_122_impl",
        "            ",
        "            step_result = step_122_impl(context)",
        "            ",
        "            payload = {",
        "                \"feedback_rejected\": True,",
        "                \"rejection_reason\": step_result.get(\"reason\"),",
        "                \"rejected_by\": step_result.get(\"reviewer_id\"),",
        "                \"original_feedback\": step_result.get(\"original\"),",
        "                \"rejection_timestamp\": step_result.get(\"timestamp\"),",
        "                \"requires_resubmission\": step_result.get(\"resubmit\", False)",
        "            }",
        "            ",
        "            rag_step_log(\"step_122_handle_feedback_rejected\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_113_feedback_ui.py << 'EOF'",
        "",
        "def test_step_122_handle_feedback_rejected():",
        "    \"\"\"Test feedback rejection handling\"\"\"",
        "    orchestrator = FeedbackOrchestrator()",
        "    ",
        "    context = {",
        "        \"feedback_id\": \"fb_123\",",
        "        \"rejection_details\": {",
        "            \"reason\": \"Insufficient detail\",",
        "            \"reviewer\": \"expert_456\"",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_122__handle_feedback_rejected(context)",
        "    ",
        "    assert result[\"feedback_rejected\"] is True",
        "    assert \"rejection_reason\" in result",
        "    assert \"rejected_by\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for feedback rejection with reason tracking and resubmission flags"
    }
  },
  "step_123": {
    "step": 123,
    "blueprint_node": "CreateExpertFeedbackRecord",
    "expected_behavior": "Create formal expert feedback record",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/feedback/step_123_rag_feedback_create_expertfeedback_record.py (exists)",
      "target_implementation": "app/orchestrators/feedback.py:step_123__create_expert_feedback_record",
      "transformation_notes": "RAG step exists, add to feedback orchestrator",
      "claude_code_instructions": [
        "# Step 123: Add Expert Feedback Record Creation to Feedback Orchestrator",
        "",
        "# Add method to existing feedback orchestrator",
        "cat >> app/orchestrators/feedback.py << 'EOF'",
        "",
        "    def step_123__create_expert_feedback_record(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 123: Create formal expert feedback record for audit trail\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_123_create_expert_feedback_record\"):",
        "            from app.ragsteps.feedback.step_123_rag_feedback_create_expertfeedback_record import main as step_123_impl",
        "            ",
        "            step_result = step_123_impl(context)",
        "            ",
        "            payload = {",
        "                \"expert_feedback_record_created\": True,",
        "                \"record_id\": step_result.get(\"record_id\"),",
        "                \"expert_id\": step_result.get(\"expert_id\"),",
        "                \"feedback_category\": step_result.get(\"category\"),",
        "                \"quality_metrics\": step_result.get(\"metrics\", {}),",
        "                \"audit_trail\": step_result.get(\"audit\", []),",
        "                \"record_status\": step_result.get(\"status\", \"active\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_123_create_expert_feedback_record\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_113_feedback_ui.py << 'EOF'",
        "",
        "def test_step_123_create_expert_feedback_record():",
        "    \"\"\"Test expert feedback record creation\"\"\"",
        "    orchestrator = FeedbackOrchestrator()",
        "    ",
        "    context = {",
        "        \"expert_feedback\": {",
        "            \"expert_id\": \"expert_789\",",
        "            \"category\": \"accuracy_improvement\",",
        "            \"quality_score\": 0.85",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_123__create_expert_feedback_record(context)",
        "    ",
        "    assert result[\"expert_feedback_record_created\"] is True",
        "    assert \"record_id\" in result",
        "    assert \"quality_metrics\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for expert feedback record with audit trail and quality metrics"
    }
  },
  "step_124": {
    "step": 124,
    "blueprint_node": "UpdateExpertMetrics",
    "expected_behavior": "Update expert performance metrics",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/metrics/step_124_rag_metrics_update_expert_metrics.py (exists)",
      "target_implementation": "app/orchestrators/metrics.py:step_124__update_expert_metrics",
      "transformation_notes": "RAG step exists, add to metrics orchestrator",
      "claude_code_instructions": [
        "# Step 124: Add Expert Metrics Update to Metrics Orchestrator",
        "",
        "# Add method to existing metrics orchestrator",
        "cat >> app/orchestrators/metrics.py << 'EOF'",
        "",
        "    def step_124__update_expert_metrics(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 124: Update expert performance metrics and ratings\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_124_update_expert_metrics\"):",
        "            from app.ragsteps.metrics.step_124_rag_metrics_update_expert_metrics import main as step_124_impl",
        "            ",
        "            step_result = step_124_impl(context)",
        "            ",
        "            payload = {",
        "                \"expert_metrics_updated\": True,",
        "                \"expert_id\": step_result.get(\"expert_id\"),",
        "                \"updated_metrics\": step_result.get(\"metrics\", {}),",
        "                \"performance_score\": step_result.get(\"performance\"),",
        "                \"feedback_count\": step_result.get(\"total_feedback\"),",
        "                \"accuracy_rating\": step_result.get(\"accuracy\"),",
        "                \"response_time_avg\": step_result.get(\"avg_response_time\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_124_update_expert_metrics\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_63_cache_metrics.py << 'EOF'",
        "",
        "def test_step_124_update_expert_metrics():",
        "    \"\"\"Test expert metrics updates\"\"\"",
        "    orchestrator = MetricsOrchestrator()",
        "    ",
        "    context = {",
        "        \"expert_id\": \"expert_789\",",
        "        \"recent_feedback\": {",
        "            \"accuracy\": 0.92,",
        "            \"response_time\": 120",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_124__update_expert_metrics(context)",
        "    ",
        "    assert result[\"expert_metrics_updated\"] is True",
        "    assert \"performance_score\" in result",
        "    assert \"accuracy_rating\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for expert metrics updating with performance scoring and time tracking"
    }
  },
  "step_125": {
    "step": 125,
    "blueprint_node": "CacheFeedback1hTtl",
    "expected_behavior": "Cache feedback with 1 hour TTL",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/cache/step_125_rag_cache_cache_feedback_1h_ttl.py (exists)",
      "target_implementation": "app/orchestrators/cache.py:step_125__cache_feedback",
      "transformation_notes": "RAG step exists, add to cache orchestrator",
      "claude_code_instructions": [
        "# Step 125: Add Feedback Caching to Cache Orchestrator",
        "",
        "# Add method to existing cache orchestrator",
        "cat >> app/orchestrators/cache.py << 'EOF'",
        "",
        "    def step_125__cache_feedback(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 125: Cache feedback with 1 hour TTL for temporary storage\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_125_cache_feedback\"):",
        "            from app.ragsteps.cache.step_125_rag_cache_cache_feedback_1h_ttl import main as step_125_impl",
        "            ",
        "            step_result = step_125_impl(context)",
        "            ",
        "            payload = {",
        "                \"feedback_cached\": True,",
        "                \"cache_key\": step_result.get(\"cache_key\"),",
        "                \"ttl_seconds\": 3600,  # 1 hour",
        "                \"cached_data\": step_result.get(\"cached_feedback\"),",
        "                \"expiry_timestamp\": step_result.get(\"expires_at\"),",
        "                \"cache_size_bytes\": step_result.get(\"size_bytes\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_125_cache_feedback\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_61_cache_key.py << 'EOF'",
        "",
        "def test_step_125_cache_feedback():",
        "    \"\"\"Test feedback caching with 1 hour TTL\"\"\"",
        "    orchestrator = CacheOrchestrator()",
        "    ",
        "    context = {",
        "        \"feedback_data\": {",
        "            \"type\": \"incomplete\",",
        "            \"details\": \"Missing overtime calculation\",",
        "            \"user_id\": \"user_123\"",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_125__cache_feedback(context)",
        "    ",
        "    assert result[\"feedback_cached\"] is True",
        "    assert result[\"ttl_seconds\"] == 3600",
        "    assert \"cache_key\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for feedback caching with TTL validation and expiry timestamp"
    }
  },
  "step_126": {
    "step": 126,
    "blueprint_node": "DetermineAction",
    "expected_behavior": "Determine next action based on current context",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_126_rag_platform_determine_action.py (exists)",
      "target_implementation": "app/orchestrators/workflow.py:step_126__determine_action",
      "transformation_notes": "RAG step exists, create workflow orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 126: Create Action Determination Workflow Orchestrator",
        "",
        "# 1. Create workflow orchestrator if not exists",
        "touch app/orchestrators/workflow.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/workflow.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class WorkflowOrchestrator:",
        "    \"\"\"Orchestrator for workflow management and action routing\"\"\"",
        "",
        "    def step_126__determine_action(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 126: Determine next action based on current context and state\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_126_determine_action\"):",
        "            from app.ragsteps.platform.step_126_rag_platform_determine_action import main as step_126_impl",
        "            ",
        "            step_result = step_126_impl(context)",
        "            ",
        "            payload = {",
        "                \"action_determined\": True,",
        "                \"next_action\": step_result.get(\"action\"),",
        "                \"action_parameters\": step_result.get(\"parameters\", {}),",
        "                \"decision_confidence\": step_result.get(\"confidence\", 0.0),",
        "                \"decision_reasoning\": step_result.get(\"reasoning\"),",
        "                \"alternative_actions\": step_result.get(\"alternatives\", [])",
        "            }",
        "            ",
        "            rag_step_log(\"step_126_determine_action\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_126_workflow_action.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.workflow import WorkflowOrchestrator",
        "",
        "def test_step_126_determine_action():",
        "    \"\"\"Test action determination based on workflow context\"\"\"",
        "    orchestrator = WorkflowOrchestrator()",
        "    ",
        "    context = {",
        "        \"current_state\": \"feedback_received\",",
        "        \"feedback_type\": \"incomplete\",",
        "        \"trust_score\": 0.6",
        "    }",
        "    ",
        "    result = orchestrator.step_126__determine_action(context)",
        "    ",
        "    assert result[\"action_determined\"] is True",
        "    assert \"next_action\" in result",
        "    assert \"decision_confidence\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for action determination with confidence scoring and alternative actions"
    }
  },
  "step_127": {
    "step": 127,
    "blueprint_node": "GoldenSetUpdaterProposeCandidateFromExpertFeedback",
    "expected_behavior": "Propose golden set candidate from expert feedback",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/golden/step_127_rag_golden_goldensetupdater_propose_candidate_from_expert_feedback.py (exists)",
      "target_implementation": "app/orchestrators/golden.py:step_127__propose_golden_candidate",
      "transformation_notes": "RAG step exists, add to golden orchestrator",
      "claude_code_instructions": [
        "# Step 127: Add Golden Set Candidate Proposal to Golden Orchestrator",
        "",
        "# Add method to existing golden orchestrator",
        "cat >> app/orchestrators/golden.py << 'EOF'",
        "",
        "    def step_127__propose_golden_candidate(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 127: Propose golden set candidate based on expert feedback\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_127_propose_golden_candidate\"):",
        "            from app.ragsteps.golden.step_127_rag_golden_goldensetupdater_propose_candidate_from_expert_feedback import main as step_127_impl",
        "            ",
        "            step_result = step_127_impl(context)",
        "            ",
        "            payload = {",
        "                \"golden_candidate_proposed\": True,",
        "                \"candidate_id\": step_result.get(\"candidate_id\"),",
        "                \"expert_feedback_id\": step_result.get(\"feedback_id\"),",
        "                \"quality_score\": step_result.get(\"quality_score\"),",
        "                \"proposal_confidence\": step_result.get(\"confidence\"),",
        "                \"candidate_content\": step_result.get(\"content\"),",
        "                \"review_required\": step_result.get(\"needs_review\", True)",
        "            }",
        "            ",
        "            rag_step_log(\"step_127_propose_golden_candidate\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_25_golden_confidence.py << 'EOF'",
        "",
        "def test_step_127_propose_golden_candidate():",
        "    \"\"\"Test golden set candidate proposal from expert feedback\"\"\"",
        "    orchestrator = GoldenOrchestrator()",
        "    ",
        "    context = {",
        "        \"expert_feedback\": {",
        "            \"feedback_id\": \"fb_456\",",
        "            \"improved_answer\": \"Enhanced vacation policy explanation...\",",
        "            \"quality_rating\": 0.95",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_127__propose_golden_candidate(context)",
        "    ",
        "    assert result[\"golden_candidate_proposed\"] is True",
        "    assert \"candidate_id\" in result",
        "    assert \"quality_score\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for golden candidate proposal with quality scoring and review requirements"
    }
  },
  "step_128": {
    "step": 128,
    "blueprint_node": "AutoThresholdMetOrManualApproval",
    "expected_behavior": "Check if auto threshold met or requires manual approval",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/golden/step_128_rag_golden_auto_threshold_met_or_manual_approval.py (exists)",
      "target_implementation": "app/orchestrators/approval.py:step_128__check_approval_threshold",
      "transformation_notes": "RAG step exists, create approval orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 128: Create Approval Threshold Check Orchestrator",
        "",
        "# 1. Create approval orchestrator if not exists",
        "touch app/orchestrators/approval.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/approval.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class ApprovalOrchestrator:",
        "    \"\"\"Orchestrator for approval workflows and threshold management\"\"\"",
        "",
        "    def step_128__check_approval_threshold(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 128: Check if auto threshold met or requires manual approval\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_128_check_approval_threshold\"):",
        "            from app.ragsteps.golden.step_128_rag_golden_auto_threshold_met_or_manual_approval import main as step_128_impl",
        "            ",
        "            step_result = step_128_impl(context)",
        "            ",
        "            payload = {",
        "                \"threshold_check_completed\": True,",
        "                \"auto_threshold_met\": step_result.get(\"auto_approved\", False),",
        "                \"requires_manual_approval\": step_result.get(\"manual_required\", True),",
        "                \"threshold_score\": step_result.get(\"threshold\", 0.9),",
        "                \"current_score\": step_result.get(\"score\"),",
        "                \"approval_path\": \"automatic\" if step_result.get(\"auto_approved\") else \"manual\"",
        "            }",
        "            ",
        "            rag_step_log(\"step_128_check_approval_threshold\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_128_approval_threshold.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.approval import ApprovalOrchestrator",
        "",
        "def test_step_128_check_approval_threshold():",
        "    \"\"\"Test approval threshold checking with auto/manual routing\"\"\"",
        "    orchestrator = ApprovalOrchestrator()",
        "    ",
        "    context = {",
        "        \"candidate_score\": 0.95,",
        "        \"expert_consensus\": True,",
        "        \"validation_passed\": True",
        "    }",
        "    ",
        "    result = orchestrator.step_128__check_approval_threshold(context)",
        "    ",
        "    assert result[\"threshold_check_completed\"] is True",
        "    assert \"auto_threshold_met\" in result",
        "    assert \"approval_path\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for approval threshold with automatic and manual routing paths"
    }
  },
  "step_129": {
    "step": 129,
    "blueprint_node": "GoldenSetPublishOrUpdateVersionedEntry",
    "expected_behavior": "Publish or update versioned entry in golden set",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/golden/step_129_rag_golden_goldenset_publish_or_update_versioned_entry.py (exists)",
      "target_implementation": "app/orchestrators/golden.py:step_129__publish_versioned_entry",
      "transformation_notes": "RAG step exists, add to golden orchestrator",
      "claude_code_instructions": [
        "# Step 129: Add Versioned Entry Publishing to Golden Orchestrator",
        "",
        "# Add method to existing golden orchestrator",
        "cat >> app/orchestrators/golden.py << 'EOF'",
        "",
        "    def step_129__publish_versioned_entry(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 129: Publish or update versioned entry in golden set\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_129_publish_versioned_entry\"):",
        "            from app.ragsteps.golden.step_129_rag_golden_goldenset_publish_or_update_versioned_entry import main as step_129_impl",
        "            ",
        "            step_result = step_129_impl(context)",
        "            ",
        "            payload = {",
        "                \"golden_entry_published\": True,",
        "                \"entry_id\": step_result.get(\"entry_id\"),",
        "                \"version\": step_result.get(\"version\"),",
        "                \"operation\": step_result.get(\"operation\"),  # \"publish\" or \"update\"",
        "                \"content_hash\": step_result.get(\"hash\"),",
        "                \"publication_timestamp\": step_result.get(\"timestamp\"),",
        "                \"previous_version\": step_result.get(\"previous_version\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_129_publish_versioned_entry\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_25_golden_confidence.py << 'EOF'",
        "",
        "def test_step_129_publish_versioned_entry():",
        "    \"\"\"Test versioned golden set entry publishing\"\"\"",
        "    orchestrator = GoldenOrchestrator()",
        "    ",
        "    context = {",
        "        \"approved_candidate\": {",
        "            \"content\": \"Approved vacation policy answer...\",",
        "            \"quality_score\": 0.92",
        "        },",
        "        \"entry_metadata\": {",
        "            \"topic\": \"vacation_policy\",",
        "            \"version\": \"2.1\"",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_129__publish_versioned_entry(context)",
        "    ",
        "    assert result[\"golden_entry_published\"] is True",
        "    assert \"entry_id\" in result",
        "    assert \"version\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for golden set publishing with versioning and content hashing"
    }
  },
  "step_130": {
    "step": 130,
    "blueprint_node": "CacheServiceInvalidateFaqByIdOrSignature",
    "expected_behavior": "Invalidate FAQ cache by ID or signature",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/preflight/step_130_rag_preflight_cacheservice_invalidate_faq_by_id_or_signature.py (exists)",
      "target_implementation": "app/orchestrators/cache.py:step_130__invalidate_faq_cache",
      "transformation_notes": "RAG step exists, add to cache orchestrator",
      "claude_code_instructions": [
        "# Step 130: Add FAQ Cache Invalidation to Cache Orchestrator",
        "",
        "# Add method to existing cache orchestrator",
        "cat >> app/orchestrators/cache.py << 'EOF'",
        "",
        "    def step_130__invalidate_faq_cache(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 130: Invalidate FAQ cache entries by ID or signature\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_130_invalidate_faq_cache\"):",
        "            from app.ragsteps.preflight.step_130_rag_preflight_cacheservice_invalidate_faq_by_id_or_signature import main as step_130_impl",
        "            ",
        "            step_result = step_130_impl(context)",
        "            ",
        "            payload = {",
        "                \"faq_cache_invalidated\": True,",
        "                \"invalidation_method\": step_result.get(\"method\"),  # \"id\" or \"signature\"",
        "                \"invalidated_keys\": step_result.get(\"keys\", []),",
        "                \"cache_entries_removed\": step_result.get(\"removed_count\", 0),",
        "                \"invalidation_reason\": step_result.get(\"reason\"),",
        "                \"affected_faqs\": step_result.get(\"faq_ids\", [])",
        "            }",
        "            ",
        "            rag_step_log(\"step_130_invalidate_faq_cache\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_61_cache_key.py << 'EOF'",
        "",
        "def test_step_130_invalidate_faq_cache():",
        "    \"\"\"Test FAQ cache invalidation by ID and signature\"\"\"",
        "    orchestrator = CacheOrchestrator()",
        "    ",
        "    context = {",
        "        \"invalidation_target\": {",
        "            \"method\": \"signature\",",
        "            \"signature\": \"vacation_policy_v2\",",
        "            \"reason\": \"golden_set_update\"",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_130__invalidate_faq_cache(context)",
        "    ",
        "    assert result[\"faq_cache_invalidated\"] is True",
        "    assert \"invalidation_method\" in result",
        "    assert \"cache_entries_removed\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for FAQ cache invalidation with ID and signature-based targeting"
    }
  },
  "step_131": {
    "step": 131,
    "blueprint_node": "VectorIndexUpsertFaqUpdateEmbeddings",
    "expected_behavior": "Update FAQ embeddings in vector index",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/golden/step_131_rag_golden_vectorindex_upsert_faq_update_embeddings.py (exists)",
      "target_implementation": "app/orchestrators/vector_index.py:step_131__upsert_faq_embeddings",
      "transformation_notes": "RAG step exists, create vector index orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 131: Create FAQ Embeddings Update Vector Index Orchestrator",
        "",
        "# 1. Create vector index orchestrator if not exists",
        "touch app/orchestrators/vector_index.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/vector_index.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class VectorIndexOrchestrator:",
        "    \"\"\"Orchestrator for vector index operations and embeddings management\"\"\"",
        "",
        "    def step_131__upsert_faq_embeddings(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 131: Update FAQ embeddings in vector index\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_131_upsert_faq_embeddings\"):",
        "            from app.ragsteps.golden.step_131_rag_golden_vectorindex_upsert_faq_update_embeddings import main as step_131_impl",
        "            ",
        "            step_result = step_131_impl(context)",
        "            ",
        "            payload = {",
        "                \"faq_embeddings_updated\": True,",
        "                \"upserted_faqs\": step_result.get(\"upserted\", []),",
        "                \"embedding_dimensions\": step_result.get(\"dimensions\"),",
        "                \"index_size\": step_result.get(\"total_entries\"),",
        "                \"update_operation\": step_result.get(\"operation\", \"upsert\"),",
        "                \"similarity_threshold\": step_result.get(\"threshold\", 0.8)",
        "            }",
        "            ",
        "            rag_step_log(\"step_131_upsert_faq_embeddings\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_131_vector_embeddings.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.vector_index import VectorIndexOrchestrator",
        "",
        "def test_step_131_upsert_faq_embeddings():",
        "    \"\"\"Test FAQ embeddings upsert in vector index\"\"\"",
        "    orchestrator = VectorIndexOrchestrator()",
        "    ",
        "    context = {",
        "        \"faq_updates\": [",
        "            {\"id\": \"faq_123\", \"content\": \"Updated vacation policy...\"},",
        "            {\"id\": \"faq_124\", \"content\": \"New overtime rules...\"}",
        "        ],",
        "        \"embedding_model\": \"text-embedding-ada-002\"",
        "    }",
        "    ",
        "    result = orchestrator.step_131__upsert_faq_embeddings(context)",
        "    ",
        "    assert result[\"faq_embeddings_updated\"] is True",
        "    assert \"upserted_faqs\" in result",
        "    assert \"embedding_dimensions\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for FAQ embeddings upsert with dimension validation and similarity thresholds"
    }
  },
  "step_132": {
    "step": 132,
    "blueprint_node": "RSSMonitor",
    "expected_behavior": "Monitor RSS feeds for knowledge base updates",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/kb/step_132_rag_kb_rss_monitor.py (exists)",
      "target_implementation": "app/orchestrators/monitoring.py:step_132__monitor_rss_feeds",
      "transformation_notes": "RAG step exists, create monitoring orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 132: Create RSS Feed Monitoring Orchestrator",
        "",
        "# 1. Create monitoring orchestrator if not exists",
        "touch app/orchestrators/monitoring.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/monitoring.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class MonitoringOrchestrator:",
        "    \"\"\"Orchestrator for monitoring external data sources and feeds\"\"\"",
        "",
        "    def step_132__monitor_rss_feeds(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 132: Monitor RSS feeds for knowledge base updates\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_132_monitor_rss_feeds\"):",
        "            from app.ragsteps.kb.step_132_rag_kb_rss_monitor import main as step_132_impl",
        "            ",
        "            step_result = step_132_impl(context)",
        "            ",
        "            payload = {",
        "                \"rss_monitoring_completed\": True,",
        "                \"feeds_checked\": step_result.get(\"feeds_checked\", 0),",
        "                \"new_items_found\": step_result.get(\"new_items\", 0),",
        "                \"updated_items\": step_result.get(\"updated_items\", []),",
        "                \"monitoring_timestamp\": step_result.get(\"timestamp\"),",
        "                \"next_check_time\": step_result.get(\"next_check\")",
        "            }",
        "            ",
        "            rag_step_log(\"step_132_monitor_rss_feeds\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_132_rss_monitoring.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.monitoring import MonitoringOrchestrator",
        "",
        "def test_step_132_monitor_rss_feeds():",
        "    \"\"\"Test RSS feed monitoring for knowledge updates\"\"\"",
        "    orchestrator = MonitoringOrchestrator()",
        "    ",
        "    context = {",
        "        \"rss_feeds\": [",
        "            \"https://example.com/labor-law-updates.rss\",",
        "            \"https://example.com/tax-updates.rss\"",
        "        ],",
        "        \"last_check\": \"2025-01-01T00:00:00Z\"",
        "    }",
        "    ",
        "    result = orchestrator.step_132__monitor_rss_feeds(context)",
        "    ",
        "    assert result[\"rss_monitoring_completed\"] is True",
        "    assert \"feeds_checked\" in result",
        "    assert \"new_items_found\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for RSS monitoring with feed checking and new item detection"
    }
  },
  "step_133": {
    "step": 133,
    "blueprint_node": "FetchAndParseSources",
    "expected_behavior": "Fetch and parse external data sources",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/platform/step_133_rag_platform_fetch_and_parse_sources.py (exists)",
      "target_implementation": "app/orchestrators/sources.py:step_133__fetch_and_parse_sources",
      "transformation_notes": "RAG step exists, create sources orchestrator wrapper",
      "claude_code_instructions": [
        "# Step 133: Create Source Fetching and Parsing Orchestrator",
        "",
        "# 1. Create sources orchestrator if not exists",
        "touch app/orchestrators/sources.py",
        "",
        "# 2. Add orchestrator wrapper",
        "cat >> app/orchestrators/sources.py << 'EOF'",
        "from typing import Dict, Any",
        "from app.utils.rag_step_utils import rag_step_log, rag_step_timer",
        "",
        "class SourcesOrchestrator:",
        "    \"\"\"Orchestrator for external source fetching and parsing\"\"\"",
        "",
        "    def step_133__fetch_and_parse_sources(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 133: Fetch and parse external data sources\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_133_fetch_and_parse_sources\"):",
        "            from app.ragsteps.platform.step_133_rag_platform_fetch_and_parse_sources import main as step_133_impl",
        "            ",
        "            step_result = step_133_impl(context)",
        "            ",
        "            payload = {",
        "                \"sources_processed\": True,",
        "                \"fetched_sources\": step_result.get(\"fetched\", []),",
        "                \"parsed_content\": step_result.get(\"parsed\", []),",
        "                \"fetch_errors\": step_result.get(\"errors\", []),",
        "                \"total_sources\": step_result.get(\"total\", 0),",
        "                \"successful_fetches\": step_result.get(\"successful\", 0)",
        "            }",
        "            ",
        "            rag_step_log(\"step_133_fetch_and_parse_sources\", payload)",
        "            return payload",
        "EOF",
        "",
        "# 3. Create test",
        "cat > tests/test_rag_step_133_source_fetching.py << 'EOF'",
        "import pytest",
        "from app.orchestrators.sources import SourcesOrchestrator",
        "",
        "def test_step_133_fetch_and_parse_sources():",
        "    \"\"\"Test external source fetching and parsing\"\"\"",
        "    orchestrator = SourcesOrchestrator()",
        "    ",
        "    context = {",
        "        \"source_urls\": [",
        "            \"https://example.com/document1.pdf\",",
        "            \"https://example.com/policy2.html\"",
        "        ],",
        "        \"parsing_rules\": {\"extract_text\": True}",
        "    }",
        "    ",
        "    result = orchestrator.step_133__fetch_and_parse_sources(context)",
        "    ",
        "    assert result[\"sources_processed\"] is True",
        "    assert \"fetched_sources\" in result",
        "    assert \"parsed_content\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for source fetching with error handling and parsing validation"
    }
  },
  "step_134": {
    "step": 134,
    "blueprint_node": "ExtractTextAndMetadata",
    "expected_behavior": "Extract text and metadata from processed sources",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/docs/step_134_rag_docs_extract_text_and_metadata.py (exists)",
      "target_implementation": "app/orchestrators/extraction.py:step_134__extract_text_and_metadata",
      "transformation_notes": "RAG step exists, add to extraction orchestrator",
      "claude_code_instructions": [
        "# Step 134: Add Text and Metadata Extraction to Extraction Orchestrator",
        "",
        "# Add method to existing extraction orchestrator",
        "cat >> app/orchestrators/extraction.py << 'EOF'",
        "",
        "    def step_134__extract_text_and_metadata(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 134: Extract text and metadata from processed sources\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_134_extract_text_and_metadata\"):",
        "            from app.ragsteps.docs.step_134_rag_docs_extract_text_and_metadata import main as step_134_impl",
        "            ",
        "            step_result = step_134_impl(context)",
        "            ",
        "            payload = {",
        "                \"text_and_metadata_extracted\": True,",
        "                \"extracted_text\": step_result.get(\"text\"),",
        "                \"metadata\": step_result.get(\"metadata\", {}),",
        "                \"extraction_confidence\": step_result.get(\"confidence\", 0.0),",
        "                \"text_length\": len(step_result.get(\"text\", \"\")),",
        "                \"metadata_fields\": list(step_result.get(\"metadata\", {}).keys())",
        "            }",
        "            ",
        "            rag_step_log(\"step_134_extract_text_and_metadata\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_95_field_extraction.py << 'EOF'",
        "",
        "def test_step_134_extract_text_and_metadata():",
        "    \"\"\"Test text and metadata extraction\"\"\"",
        "    orchestrator = ExtractionOrchestrator()",
        "    ",
        "    context = {",
        "        \"source_document\": {",
        "            \"content\": \"Document content with metadata...\",",
        "            \"format\": \"pdf\"",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_134__extract_text_and_metadata(context)",
        "    ",
        "    assert result[\"text_and_metadata_extracted\"] is True",
        "    assert \"extracted_text\" in result",
        "    assert \"metadata\" in result",
        "    assert \"text_length\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for text and metadata extraction with confidence and field validation"
    }
  },
  "step_135": {
    "step": 135,
    "blueprint_node": "GoldenSetUpdaterAutoRuleEvalNewOrObsoleteCandidates",
    "expected_behavior": "Automatically evaluate new or obsolete golden set candidates",
    "status": "needs_orchestrator_wrapper",
    "transformation_recipe": {
      "current_implementation": "app/ragsteps/golden/step_135_rag_golden_goldensetupdater_auto_rule_eval_new_or_obsolete_candidates.py (exists)",
      "target_implementation": "app/orchestrators/golden.py:step_135__auto_evaluate_candidates",
      "transformation_notes": "RAG step exists, add to golden orchestrator - FINAL STEP!",
      "claude_code_instructions": [
        "# Step 135: Add Auto Candidate Evaluation to Golden Orchestrator - FINAL STEP!",
        "",
        "# Add method to existing golden orchestrator",
        "cat >> app/orchestrators/golden.py << 'EOF'",
        "",
        "    def step_135__auto_evaluate_candidates(self, context: Dict[str, Any]) -> Dict[str, Any]:",
        "        \"\"\"Step 135: Automatically evaluate new or obsolete golden set candidates - FINAL STEP!\"\"\"",
        "        ",
        "        with rag_step_timer(\"step_135_auto_evaluate_candidates\"):",
        "            from app.ragsteps.golden.step_135_rag_golden_goldensetupdater_auto_rule_eval_new_or_obsolete_candidates import main as step_135_impl",
        "            ",
        "            step_result = step_135_impl(context)",
        "            ",
        "            payload = {",
        "                \"auto_evaluation_completed\": True,",
        "                \"new_candidates_evaluated\": step_result.get(\"new_evaluated\", 0),",
        "                \"obsolete_candidates_identified\": step_result.get(\"obsolete\", 0),",
        "                \"approved_candidates\": step_result.get(\"approved\", []),",
        "                \"rejected_candidates\": step_result.get(\"rejected\", []),",
        "                \"evaluation_rules_applied\": step_result.get(\"rules\", []),",
        "                \"final_rag_step\": True  # This is step 135 - the final step!",
        "            }",
        "            ",
        "            rag_step_log(\"step_135_auto_evaluate_candidates\", payload)",
        "            return payload",
        "",
        "EOF",
        "",
        "# Add test",
        "cat >> tests/test_rag_step_25_golden_confidence.py << 'EOF'",
        "",
        "def test_step_135_auto_evaluate_candidates():",
        "    \"\"\"Test automatic candidate evaluation - FINAL STEP!\"\"\"",
        "    orchestrator = GoldenOrchestrator()",
        "    ",
        "    context = {",
        "        \"candidate_pool\": [",
        "            {\"id\": \"cand_123\", \"score\": 0.92, \"age_days\": 5},",
        "            {\"id\": \"cand_124\", \"score\": 0.65, \"age_days\": 180}",
        "        ],",
        "        \"evaluation_thresholds\": {",
        "            \"min_score\": 0.8,",
        "            \"obsolete_age_days\": 90",
        "        }",
        "    }",
        "    ",
        "    result = orchestrator.step_135__auto_evaluate_candidates(context)",
        "    ",
        "    assert result[\"auto_evaluation_completed\"] is True",
        "    assert result[\"final_rag_step\"] is True",
        "    assert \"approved_candidates\" in result",
        "    assert \"obsolete_candidates_identified\" in result",
        "EOF"
      ],
      "test_requirements": "Unit test for automatic candidate evaluation with approval/rejection logic and obsolescence detection - FINAL RAG STEP!"
    }
  }
}