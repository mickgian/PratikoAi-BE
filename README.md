 # FastAPI LangGraph Backend Guidelines

  ## Architecture

  ‚Ä¢ Framework: FastAPI with Python 3.13+
  ‚Ä¢ AI/LLM: LangGraph with OpenAI GPT models
  ‚Ä¢ Database: PostgreSQL with SQLAlchemy ORM
  ‚Ä¢ Authentication: JWT tokens with bcrypt password hashing
  ‚Ä¢ Observability: Langfuse for LLM tracing
  ‚Ä¢ Rate Limiting: slowapi (Flask-Limiter for FastAPI)
  ‚Ä¢ Monitoring: Prometheus metrics + Grafana dashboards
  ‚Ä¢ Package Management: UV for fast dependency management

  ## Prerequisites

  ‚Ä¢ Python 3.13+
  ‚Ä¢ UV package manager for dependency management
  ‚Ä¢ PostgreSQL database
  ‚Ä¢ Docker & Docker Compose (optional)

  ## Current Implementation Status

  ‚Ä¢ Authentication: ‚úÖ JWT-based auth with registration/login
  ‚Ä¢ Password Validation: ‚úÖ 8+ chars, upper/lower, number, special char
  ‚Ä¢ Error Handling: ‚úÖ Structured validation errors (422) and business logic errors (400)
  ‚Ä¢ User Management: ‚úÖ User registration, login, profile management
  ‚Ä¢ Session Management: ‚úÖ Session creation and management
  ‚Ä¢ Rate Limiting: ‚úÖ Configurable per-endpoint limits
  ‚Ä¢ Current Feature: Chat implementation with LangGraph integration

  ## Tech Stack

  ‚Ä¢ Web Framework: FastAPI
  ‚Ä¢ ORM: SQLAlchemy with async support
  ‚Ä¢ Database: PostgreSQL
  ‚Ä¢ LLM Framework: LangGraph
  ‚Ä¢ Authentication: JWT with python-jose
  ‚Ä¢ Password Hashing: bcrypt
  ‚Ä¢ Validation: Pydantic models
  ‚Ä¢ Testing: pytest with async support
  ‚Ä¢ Rate Limiting: slowapi
  ‚Ä¢ Observability: Langfuse
  ‚Ä¢ Metrics: Prometheus
  ‚Ä¢ Monitoring: Grafana
  ‚Ä¢ Package Manager: UV

  ## Development Setup

  ‚Ä¢ Install dependencies: `uv sync`
  ‚Ä¢ Copy environment: `cp .env.example .env.development`
  ‚Ä¢ Run development server: `make dev`
  ‚Ä¢ Run staging: `make staging`
  ‚Ä¢ Run production: `make production`
  ‚Ä¢ Swagger docs: http://localhost:8000/docs

  ### Docker Development
  ‚Ä¢ Build Docker: `make docker-build-env ENV=development`
  ‚Ä¢ Run Docker: `make docker-run-env ENV=development`

  ## Key Commands

  ‚Ä¢ Start server: `make dev` (or `uvicorn app.main:app --reload --port 8000`)
  ‚Ä¢ Run tests: `pytest`
  ‚Ä¢ Run tests with coverage: `pytest --cov=app`
  ‚Ä¢ Database migrations: `alembic upgrade head`
  ‚Ä¢ Create migration: `alembic revision --autogenerate -m "description"`
  ‚Ä¢ Install dependencies: `uv sync`

  ## Development Notes

  ‚Ä¢ JWT tokens: 30 days access + 365 days refresh (development) - TODO: reduce for production
  ‚Ä¢ Server runs on: http://localhost:8000
  ‚Ä¢ API docs: http://localhost:8000/docs
  ‚Ä¢ Current branch: main (or specify current feature branch)

  ## API Architecture

  ‚Ä¢ Base URL: `/api/v1`
  ‚Ä¢ Authentication endpoints: `/api/v1/auth/`
  ‚Ä¢ Chat endpoints: `/api/v1/chat/`
  ‚Ä¢ User endpoints: `/api/v1/users/`
  ‚Ä¢ Health check: `/health`

  ## Error Response Format

  ‚Ä¢ **422 Validation Errors**:
  ```json
  {
    "detail": "Validation error",
    "errors": [
      {"field": "password", "message": "Password must contain at least one number"}
    ]
  }

  ‚Ä¢ 400 Business Logic Errors:
  {
    "detail": "Email already registered"
  }

  ‚Ä¢ 401 Authentication Errors:
  {
    "detail": "Invalid credentials"
  }
````
  ## Database Management

  ‚Ä¢ ORM handles table creation automatically
  ‚Ä¢ Manual schema: Run schemas.sql if needed
  ‚Ä¢ Connection format: postgresql://user:pass@host:port/dbname
  ‚Ä¢ Tables: Users, Sessions, Messages, checkpoint tables for LangGraph
  ‚Ä¢ Use Alembic for migrations
  ‚Ä¢ Follow SQLAlchemy async patterns

  ## Database Schema

  ‚Ä¢ Users table: id, email, hashed_password, created_at, updated_at
  ‚Ä¢ Sessions table: session_id, user_id, name, created_at
  ‚Ä¢ Messages table: id, session_id, content, role, timestamp
  ‚Ä¢ Checkpoint tables: checkpoint_blobs, checkpoint_writes, checkpoints (for LangGraph)

  ## LangGraph Configuration

  ‚Ä¢ Default model: gpt-4o-mini
  ‚Ä¢ Temperature: 0.2
  ‚Ä¢ Max tokens: 2000
  ‚Ä¢ Retry attempts: 3
  ‚Ä¢ Langfuse integration for tracing
  ‚Ä¢ Graph state management for conversation flow
  ‚Ä¢ Checkpoint persistence in PostgreSQL

  ## Monitoring Stack

  ### üìä **Quick Access URLs**
  ‚Ä¢ **Grafana Dashboards**: http://localhost:3000 (admin/admin)
  ‚Ä¢ **Prometheus Metrics**: http://localhost:9090
  ‚Ä¢ **Langfuse LLM Tracing**: http://localhost:3000 (if configured)
  ‚Ä¢ **AlertManager**: http://localhost:9093

  ### üéõÔ∏è **Available Dashboards**
  ‚Ä¢ **System Overview**: Executive summary of all key metrics
  ‚Ä¢ **Cost Analysis**: User cost tracking (‚Ç¨2.00/month target)
  ‚Ä¢ **Business Metrics**: Revenue tracking (‚Ç¨25k ARR target)
  ‚Ä¢ **Performance**: API response times and system health
  ‚Ä¢ **Alert Management**: Active alerts and incident tracking

  ### üö® **Alert System**
  ‚Ä¢ **14 Critical Alerts** across cost, business, performance, security
  ‚Ä¢ **4 Notification Channels**: Email, Slack, Webhook, PagerDuty
  ‚Ä¢ **Business-Aligned Thresholds**: ‚Ç¨2.50/user cost, 5s API response, 95% payment success

  ### ü§ñ **Automation Scripts**
  ‚Ä¢ **Daily Reports**: `make monitoring-daily` - Email business summaries
  ‚Ä¢ **Cost Optimization**: `make monitoring-costs` - Identify savings opportunities  
  ‚Ä¢ **Health Checks**: `make monitoring-health` - System validation
  ‚Ä¢ **Dashboard Backups**: `make monitoring-backup` - Configuration protection

  ### üöÄ **Getting Started**
  ```bash
  # Start monitoring stack
  make monitoring-start
  
  # Run health check
  make monitoring-health
  
  # Generate daily report
  make monitoring-daily
  
  # Full monitoring suite
  make monitoring-suite
  ```

  ### üìö **Documentation**
  ‚Ä¢ **Complete Guide**: [MONITORING.md](MONITORING.md) - Full system documentation
  ‚Ä¢ **Quick Start**: [monitoring/QUICK_START.md](monitoring/QUICK_START.md) - 15-minute setup
  ‚Ä¢ **Alert Runbooks**: [monitoring/RUNBOOKS.md](monitoring/RUNBOOKS.md) - Response procedures
  ‚Ä¢ **Automation Guide**: [monitoring/AUTOMATION_GUIDE.md](monitoring/AUTOMATION_GUIDE.md) - Scripts usage

  ## Model Evaluation

  ‚Ä¢ Interactive evaluation: make eval
  ‚Ä¢ Quick evaluation: make eval-quick
  ‚Ä¢ No-report evaluation: make eval-no-report
  ‚Ä¢ Custom metrics: Add markdown files to evals/metrics/prompts/
  ‚Ä¢ Reports generated in: evals/reports/evaluation_report_YYYYMMDD_HHMMSS.json
  ‚Ä¢ Metrics include: success rate, timing, trace-level details

  ## Evaluation Features

  ‚Ä¢ Interactive CLI with colored output and progress bars
  ‚Ä¢ Flexible configuration with runtime customization
  ‚Ä¢ Detailed JSON reports with comprehensive metrics
  ‚Ä¢ Automatic Langfuse trace fetching and analysis

  ## Security Guidelines

  ‚Ä¢ Password requirements: 8+ chars, uppercase, lowercase, number, special char
  ‚Ä¢ JWT secret key must be set in environment variables
  ‚Ä¢ Rate limiting applied to all endpoints
  ‚Ä¢ Input validation using Pydantic models
  ‚Ä¢ SQL injection protection via SQLAlchemy ORM
  ‚Ä¢ CORS configured for frontend origins
  ‚Ä¢ Input sanitization for all user inputs

  ## Environment Variables

  # Required
  JWT_SECRET_KEY=your-secret-key
  POSTGRES_URL=postgresql://user:pass@localhost/dbname
  LLM_API_KEY=your-openai-api-key

  # Optional with defaults
  JWT_ACCESS_TOKEN_EXPIRE_HOURS=720  # 30 days for dev
  JWT_REFRESH_TOKEN_EXPIRE_DAYS=365
  LLM_MODEL=gpt-4o-mini
  LANGFUSE_PUBLIC_KEY=your-key
  LANGFUSE_SECRET_KEY=your-secret
  EVALUATION_LLM=gpt-4o-mini
  EVALUATION_API_KEY=your-openai-key

  ## Environment-Specific Configuration

  ‚Ä¢ .env.development - Local development settings
  ‚Ä¢ .env.staging - Staging environment
  ‚Ä¢ .env.production - Production environment
  ‚Ä¢ .env.example - Template with all available options

  ## Coding Standards

  ‚Ä¢ Use async/await for all database operations
  ‚Ä¢ Follow PEP 8 style guidelines
  ‚Ä¢ Use type hints throughout the codebase
  ‚Ä¢ Keep functions focused on single responsibility
  ‚Ä¢ Use dependency injection for database sessions
  ‚Ä¢ Handle exceptions with appropriate HTTP status codes
  ‚Ä¢ Log all authentication attempts and errors
  ‚Ä¢ Structured logging with environment-specific formatting

  ## Testing Requirements

  ‚Ä¢ Unit tests for all business logic
  ‚Ä¢ Integration tests for API endpoints
  ‚Ä¢ Test database fixtures for clean test data
  ‚Ä¢ Mock external API calls (OpenAI, Langfuse)
  ‚Ä¢ Test authentication flows thoroughly
  ‚Ä¢ Aim for 80%+ code coverage
  ‚Ä¢ Use pytest with async support

  ## Directory Structure
```
  app/
  ‚îú‚îÄ‚îÄ api/v1/          # API route handlers
  ‚îú‚îÄ‚îÄ core/            # Core utilities, config, security
  ‚îú‚îÄ‚îÄ models/          # SQLAlchemy models
  ‚îú‚îÄ‚îÄ schemas/         # Pydantic models
  ‚îú‚îÄ‚îÄ services/        # Business logic layer
  ‚îú‚îÄ‚îÄ utils/           # Helper utilities
  ‚îú‚îÄ‚îÄ main.py          # FastAPI application entry point
  evals/
  ‚îú‚îÄ‚îÄ metrics/prompts/ # Evaluation metric definitions
  ‚îú‚îÄ‚îÄ reports/         # Generated evaluation reports
  ‚îî‚îÄ‚îÄ evaluator.py     # Evaluation framework
  docker/              # Docker configuration files
  monitoring/          # Prometheus & Grafana configs
```

  ## Rate Limiting Configuration

  ‚Ä¢ Default limits: 200 per day, 50 per hour
  ‚Ä¢ Endpoint-specific limits:
  - Chat: 30 per minute
  - Chat stream: 20 per minute
  - Register: 10 per hour
  - Login: 20 per minute
  ‚Ä¢ Configurable via environment variables

  ## Common Tasks

  ‚Ä¢ Add new endpoint: Create route in api/v1/, add schema, implement service
  ‚Ä¢ Database changes: Create Alembic migration, update models
  ‚Ä¢ Add authentication: Use get_current_user dependency
  ‚Ä¢ Rate limiting: Apply @limiter.limit() decorator
  ‚Ä¢ LLM integration: Use LangGraph state management patterns
  ‚Ä¢ Add evaluation metric: Create markdown file in evals/metrics/prompts/

  ## Troubleshooting

  ‚Ä¢ Database connection issues: Check POSTGRES_URL and ensure PostgreSQL is running
  ‚Ä¢ JWT token errors: Verify JWT_SECRET_KEY is set and consistent
  ‚Ä¢ Rate limiting issues: Check slowapi configuration and Redis connection
  ‚Ä¢ LLM API errors: Verify LLM_API_KEY and check OpenAI quota
  ‚Ä¢ Langfuse connection: Check LANGFUSE_* environment variables
  ‚Ä¢ Docker issues: Ensure Docker daemon is running and ports are available
  ‚Ä¢ Evaluation failures: Check evaluation model API key and Langfuse configuration

  ## Performance Considerations

  ‚Ä¢ Use async database operations throughout
  ‚Ä¢ Implement connection pooling for PostgreSQL
  ‚Ä¢ Cache frequently accessed data where appropriate
  ‚Ä¢ Monitor LLM API usage and costs via Langfuse
  ‚Ä¢ Use pagination for large data sets
  ‚Ä¢ Optimize database queries with proper indexing
  ‚Ä¢ Monitor system metrics via Prometheus/Grafana

  ## Production Deployment

  ‚Ä¢ Set secure JWT_SECRET_KEY in production
  ‚Ä¢ Use environment-specific configuration files
  ‚Ä¢ Set up proper logging and monitoring
  ‚Ä¢ Configure rate limiting based on expected traffic
  ‚Ä¢ Secure database with proper credentials
  ‚Ä¢ Set up SSL/TLS for production
  ‚Ä¢ Monitor via Grafana dashboards
  ‚Ä¢ Set up automated backups for PostgreSQL
  ‚Ä¢ Configure appropriate resource limits

  ## Monitoring & Debugging

  ‚Ä¢ Use Langfuse for LLM call tracing and debugging
  ‚Ä¢ Monitor API performance via Prometheus metrics
  ‚Ä¢ View system health via Grafana dashboards
  ‚Ä¢ Check application logs for detailed error information
  ‚Ä¢ Use evaluation reports to track model performance over time
