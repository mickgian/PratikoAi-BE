# Expert Feedback Test Suite - Execution Notes

## Current Status

Created 5 comprehensive test files with 65+ test cases covering all aspects of the Expert Feedback System.

## Known Issue: Timezone Mismatch

### Error
```
TypeError: can't subtract offset-naive and offset-aware datetimes
sqlalchemy.dialects.postgresql.asyncpg.AsyncAdapt_asyncpg_dbapi.Error: invalid input for query argument $1
```

### Root Cause
The User model's `created_at` field (from BaseModel) uses timezone-aware datetime, but test fixtures create timezone-naive datetimes.

### Fix Options

#### Option 1: Use Timezone-Aware Datetimes in Tests (Recommended)
Add to test fixtures:
```python
from datetime import datetime, timezone

@pytest.fixture
async def test_user(real_db):
    user = User(
        email=f"test_{uuid4()}@test.com",
        role=UserRole.SUPER_USER.value,
        # Let SQLAlchemy handle timestamps automatically
    )
    real_db.add(user)
    await real_db.commit()
    await real_db.refresh(user)
    return user
```

#### Option 2: Configure Database to Handle Timezones
In `app/models/base.py` or database configuration:
```python
from sqlalchemy import DateTime
from datetime import datetime, timezone

class BaseModel:
    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
```

#### Option 3: Mock Datetime in Tests
```python
from unittest.mock import patch
from datetime import datetime, timezone

@patch('datetime.datetime')
def test_something(mock_datetime):
    mock_datetime.now.return_value = datetime.now(timezone.utc)
```

### Recommended Action
The User model likely inherits from BaseModel which has `created_at` field. The simplest fix is to let SQLAlchemy auto-generate timestamps instead of manually setting them in tests.

**Update all test fixtures to remove explicit timestamp fields:**
```python
# BAD (causes timezone error)
user = User(
    email="test@test.com",
    created_at=datetime.now()  # Timezone-naive
)

# GOOD (let SQLAlchemy handle it)
user = User(
    email="test@test.com"
    # created_at is auto-generated by database
)
```

## Test Execution Commands

### Fix Tests First
```bash
# Check if tests pass after removing explicit timestamps
uv run pytest tests/models/test_enum_serialization.py::TestFeedbackTypeEnumSerialization::test_feedback_type_correct_roundtrip -v
```

### Run All Tests
```bash
uv run pytest tests/models/test_enum_serialization.py \
                 tests/api/test_expert_feedback_submission.py \
                 tests/api/test_expert_profile_retrieval.py \
                 tests/services/test_expert_feedback_background_tasks.py \
                 tests/integration/test_expert_feedback_complete_flow.py \
                 -v --tb=short
```

### Generate Coverage Report
```bash
uv run pytest tests/models/test_enum_serialization.py \
                 tests/api/test_expert_feedback_submission.py \
                 tests/api/test_expert_profile_retrieval.py \
                 tests/services/test_expert_feedback_background_tasks.py \
                 tests/integration/test_expert_feedback_complete_flow.py \
                 --cov=app/models/quality_analysis \
                 --cov=app/api/v1/expert_feedback \
                 --cov=app/schemas/expert_feedback \
                 --cov=app/services/task_generator_service \
                 --cov-report=html \
                 --cov-report=term-missing

open htmlcov/index.html
```

## Expected Test Results

### If All Tests Pass
```
tests/models/test_enum_serialization.py::TestFeedbackTypeEnumSerialization::test_feedback_type_correct_roundtrip PASSED
tests/models/test_enum_serialization.py::TestFeedbackTypeEnumSerialization::test_feedback_type_incomplete_roundtrip PASSED
tests/models/test_enum_serialization.py::TestFeedbackTypeEnumSerialization::test_feedback_type_incorrect_roundtrip PASSED
...
======================== 65 passed in 15.23s =========================

Coverage Report:
app/models/quality_analysis.py         85%
app/api/v1/expert_feedback.py          92%
app/schemas/expert_feedback.py         96%
app/services/task_generator_service.py 88%
```

### If Tests Fail (Bugs Detected)
The tests will show exactly which bug was caught:
```
FAILED tests/models/test_enum_serialization.py::test_feedback_type_correct_roundtrip
E   AssertionError: assert FeedbackType.INCORRECT == FeedbackType.CORRECT
E    +  where FeedbackType.INCORRECT = <ExpertFeedback at 0x123>.feedback_type
```

This would indicate Bug #7 (enum serialization) - storing enum.name instead of enum.value.

## Test Maintenance

### Adding New Tests
When adding new Expert Feedback features:
1. Write test first (TDD RED phase)
2. Test should fail (feature doesn't exist yet)
3. Implement feature (GREEN phase)
4. Test should pass
5. Refactor code (REFACTOR phase)
6. Tests still pass

### Updating Existing Tests
When fixing bugs:
1. Add test that reproduces bug
2. Test fails (bug exists)
3. Fix bug
4. Test passes
5. All other tests still pass

## Integration with CI/CD

### Pre-commit Hook
Tests run automatically before every commit:
```bash
# .pre-commit-config.yaml
- repo: local
  hooks:
    - id: expert-feedback-tests
      name: Run Expert Feedback Tests
      entry: uv run pytest tests/models/test_enum_serialization.py tests/api/test_expert_feedback_submission.py -x
      language: system
      pass_filenames: false
```

### GitHub Actions
Tests run on every push:
```yaml
# .github/workflows/tests.yml
- name: Run Expert Feedback Tests
  run: |
    uv run pytest tests/models/test_enum_serialization.py \
                   tests/api/test_expert_feedback_submission.py \
                   tests/api/test_expert_profile_retrieval.py \
                   tests/services/test_expert_feedback_background_tasks.py \
                   tests/integration/test_expert_feedback_complete_flow.py \
                   --cov=app \
                   --cov-report=term \
                   --cov-fail-under=69.5
```

### Pull Request Requirements
- All tests must pass
- Coverage must be ≥69.5%
- No new code without tests

## Next Actions

1. **Fix timezone issue** in test fixtures
2. **Run tests** and verify they pass
3. **Generate coverage report** and verify ≥80%
4. **Update CI/CD** to include expert feedback tests
5. **Add to pre-commit hook** to prevent regressions
6. **Document any additional edge cases** found during execution

## Contact
For questions about these tests, contact:
- **Test Specialist:** @Clelia (PratikoAI Test Generation Subagent)
- **Feature Owner:** @Backend Expert (DEV-BE-72)
