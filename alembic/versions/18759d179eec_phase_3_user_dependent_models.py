"""phase_3_user_dependent_models

Revision ID: 18759d179eec
Revises: 20251126_add_query_signature
Create Date: 2025-11-28 14:38:29.870764

"""

from typing import Sequence, Union

import pgvector.sqlalchemy
import sqlalchemy as sa
import sqlmodel
from sqlalchemy.dialects import postgresql

from alembic import op

# revision identifiers, used by Alembic.
revision: str = "18759d179eec"
down_revision: str | Sequence[str] | None = "20251126_add_query_signature"
branch_labels: str | Sequence[str] | None = None
depends_on: str | Sequence[str] | None = None


# Helper functions for idempotent operations (CI creates tables via SQLModel.metadata.create_all)
def safe_drop_index(index_name: str, table_name: str) -> None:
    """Drop index only if it exists."""
    op.execute(sa.text(f'DROP INDEX IF EXISTS "{index_name}"'))


def safe_drop_table(table_name: str) -> None:
    """Drop table only if it exists."""
    op.execute(sa.text(f'DROP TABLE IF EXISTS "{table_name}" CASCADE'))


def safe_drop_constraint(constraint_name: str, table_name: str, type_: str) -> None:
    """Drop constraint only if it exists."""
    conn = op.get_bind()
    if type_ == "foreignkey":
        result = conn.execute(
            sa.text(f"""
            SELECT constraint_name FROM information_schema.table_constraints
            WHERE table_name = '{table_name}' AND constraint_name = '{constraint_name}'
        """)
        )
    else:
        result = conn.execute(
            sa.text(f"""
            SELECT constraint_name FROM information_schema.table_constraints
            WHERE table_name = '{table_name}' AND constraint_name = '{constraint_name}'
        """)
        )
    if result.fetchone():
        op.drop_constraint(constraint_name, table_name, type_=type_)


def safe_drop_column(table_name: str, column_name: str) -> None:
    """Drop column only if it exists."""
    conn = op.get_bind()
    result = conn.execute(
        sa.text(f"""
        SELECT column_name FROM information_schema.columns
        WHERE table_name = '{table_name}' AND column_name = '{column_name}'
    """)
    )
    if result.fetchone():
        op.drop_column(table_name, column_name)


def safe_add_column(table_name: str, column: sa.Column) -> None:
    """Add column only if it doesn't exist."""
    conn = op.get_bind()
    result = conn.execute(
        sa.text(f"""
        SELECT column_name FROM information_schema.columns
        WHERE table_name = '{table_name}' AND column_name = '{column.name}'
    """)
    )
    if not result.fetchone():
        op.add_column(table_name, column)


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "expert_validations",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("query_id", sa.Uuid(), nullable=False),
        sa.Column("validation_type", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("complexity_level", sa.Integer(), nullable=False),
        sa.Column("specialization_required", postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column("assigned_experts", postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column("completed_validations", sa.Integer(), nullable=False),
        sa.Column("required_validations", sa.Integer(), nullable=False),
        sa.Column("consensus_reached", sa.Boolean(), nullable=False),
        sa.Column("consensus_confidence", sa.Float(), nullable=False),
        sa.Column("disagreement_areas", postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column("validated_answer", sa.Text(), nullable=True),
        sa.Column("validation_notes", sa.Text(), nullable=True),
        sa.Column("regulatory_confirmations", postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column("final_confidence_score", sa.Float(), nullable=False),
        sa.Column("expert_agreement_score", sa.Float(), nullable=False),
        sa.Column("requested_at", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.Column("target_completion", sa.DateTime(), nullable=False),
        sa.Column("completed_at", sa.DateTime(), nullable=True),
        sa.Column("status", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=False),
        sa.Column("created_at", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.Column("updated_at", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.CheckConstraint("completed_validations <= required_validations", name="logical_validation_counts"),
        sa.CheckConstraint("complexity_level >= 1 AND complexity_level <= 5", name="complexity_level_range"),
        sa.CheckConstraint(
            "consensus_confidence >= 0.0 AND consensus_confidence <= 1.0", name="consensus_confidence_range"
        ),
        sa.CheckConstraint(
            "expert_agreement_score >= 0.0 AND expert_agreement_score <= 1.0", name="agreement_score_range"
        ),
        sa.CheckConstraint(
            "final_confidence_score >= 0.0 AND final_confidence_score <= 1.0", name="final_confidence_range"
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index("idx_expert_validations_complexity", "expert_validations", ["complexity_level"], unique=False)
    op.create_index("idx_expert_validations_query", "expert_validations", ["query_id"], unique=False)
    op.create_index("idx_expert_validations_status", "expert_validations", ["status"], unique=False)
    op.create_index("idx_expert_validations_target", "expert_validations", ["target_completion"], unique=False)
    op.create_table(
        "failure_patterns",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("pattern_name", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),
        sa.Column("pattern_type", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("description", sa.Text(), nullable=False),
        sa.Column("categories", postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column("example_queries", postgresql.ARRAY(sa.Text()), nullable=True),
        sa.Column("frequency_count", sa.Integer(), nullable=False),
        sa.Column("impact_score", sa.Float(), nullable=False),
        sa.Column("confidence_score", sa.Float(), nullable=False),
        sa.Column("detection_algorithm", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("cluster_id", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("first_detected", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.Column("last_occurrence", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.Column("is_resolved", sa.Boolean(), nullable=False),
        sa.Column("resolution_date", sa.DateTime(), nullable=True),
        sa.Column("resolution_method", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=True),
        sa.Column("created_at", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.Column("updated_at", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.CheckConstraint("confidence_score >= 0.0 AND confidence_score <= 1.0", name="confidence_score_range"),
        sa.CheckConstraint("frequency_count >= 0", name="non_negative_frequency"),
        sa.CheckConstraint("impact_score >= 0.0 AND impact_score <= 1.0", name="impact_score_range"),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index("idx_failure_patterns_frequency", "failure_patterns", ["frequency_count"], unique=False)
    op.create_index("idx_failure_patterns_impact", "failure_patterns", ["impact_score"], unique=False)
    op.create_index("idx_failure_patterns_resolved", "failure_patterns", ["is_resolved"], unique=False)
    op.create_index("idx_failure_patterns_type", "failure_patterns", ["pattern_type"], unique=False)
    op.create_table(
        "prompt_templates",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("name", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),
        sa.Column("version", sqlmodel.sql.sqltypes.AutoString(length=20), nullable=False),
        sa.Column("template_text", sa.Text(), nullable=False),
        sa.Column("variables", postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column("description", sa.Text(), nullable=True),
        sa.Column("category", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("specialization_areas", postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column("complexity_level", sqlmodel.sql.sqltypes.AutoString(length=20), nullable=False),
        sa.Column("clarity_score", sa.Float(), nullable=False),
        sa.Column("completeness_score", sa.Float(), nullable=False),
        sa.Column("accuracy_score", sa.Float(), nullable=False),
        sa.Column("overall_quality_score", sa.Float(), nullable=False),
        sa.Column("usage_count", sa.Integer(), nullable=False),
        sa.Column("success_rate", sa.Float(), nullable=False),
        sa.Column("average_user_rating", sa.Float(), nullable=False),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.Column("variant_group", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=True),
        sa.Column("created_by", sa.Uuid(), nullable=True),
        sa.Column("created_at", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.Column("updated_at", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.CheckConstraint("accuracy_score >= 0.0 AND accuracy_score <= 1.0", name="accuracy_score_range"),
        sa.CheckConstraint("clarity_score >= 0.0 AND clarity_score <= 1.0", name="clarity_score_range"),
        sa.CheckConstraint("completeness_score >= 0.0 AND completeness_score <= 1.0", name="completeness_score_range"),
        sa.CheckConstraint(
            "overall_quality_score >= 0.0 AND overall_quality_score <= 1.0", name="overall_quality_score_range"
        ),
        sa.CheckConstraint("success_rate >= 0.0 AND success_rate <= 1.0", name="success_rate_range"),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("name"),
    )
    op.create_index("idx_prompt_templates_active", "prompt_templates", ["is_active"], unique=False)
    op.create_index("idx_prompt_templates_category", "prompt_templates", ["category"], unique=False)
    op.create_index("idx_prompt_templates_quality", "prompt_templates", ["overall_quality_score"], unique=False)
    op.create_index("idx_prompt_templates_usage", "prompt_templates", ["usage_count"], unique=False)
    op.create_table(
        "quality_metrics",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("metric_name", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("metric_category", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("metric_value", sa.Float(), nullable=False),
        sa.Column("metric_unit", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=False),
        sa.Column("measurement_period", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=False),
        sa.Column("query_category", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("expert_specialization", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("user_segment", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("sample_size", sa.Integer(), nullable=False),
        sa.Column("confidence_interval", sa.Float(), nullable=True),
        sa.Column("standard_deviation", sa.Float(), nullable=True),
        sa.Column("baseline_value", sa.Float(), nullable=True),
        sa.Column("target_value", sa.Float(), nullable=True),
        sa.Column("benchmark_percentile", sa.Float(), nullable=True),
        sa.Column("measurement_date", sa.DateTime(), nullable=False),
        sa.Column("measurement_window_start", sa.DateTime(), nullable=False),
        sa.Column("measurement_window_end", sa.DateTime(), nullable=False),
        sa.Column("calculated_by", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("calculation_method", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=True),
        sa.Column("data_sources", postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column("created_at", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.CheckConstraint("sample_size >= 0", name="non_negative_sample_size"),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index("idx_quality_metrics_category", "quality_metrics", ["metric_category"], unique=False)
    op.create_index(
        "idx_quality_metrics_name_date", "quality_metrics", ["metric_name", "measurement_date"], unique=False
    )
    op.create_index(
        "idx_quality_metrics_period", "quality_metrics", ["measurement_period", "measurement_date"], unique=False
    )
    op.create_table(
        "query_clusters",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("canonical_query", sqlmodel.sql.sqltypes.AutoString(length=500), nullable=False),
        sa.Column("normalized_form", sqlmodel.sql.sqltypes.AutoString(length=500), nullable=False),
        sa.Column("query_count", sa.Integer(), nullable=False),
        sa.Column("first_seen", sa.DateTime(timezone=True), nullable=False),
        sa.Column("last_seen", sa.DateTime(timezone=True), nullable=False),
        sa.Column("total_cost_cents", sa.Integer(), nullable=False),
        sa.Column("avg_cost_cents", sa.Integer(), nullable=False),
        sa.Column("potential_savings_cents", sa.Integer(), nullable=False),
        sa.Column("avg_quality_score", sa.Numeric(precision=3, scale=2), nullable=False),
        sa.Column("avg_response_time_ms", sa.Integer(), nullable=False),
        sa.Column("query_variations", postgresql.ARRAY(sa.String(length=500)), nullable=False),
        sa.Column("semantic_tags", postgresql.ARRAY(sa.String(length=50)), nullable=False),
        sa.Column("topic_distribution", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("roi_score", sa.Numeric(precision=10, scale=2), nullable=False),
        sa.Column("priority_score", sa.Numeric(precision=10, scale=2), nullable=False),
        sa.Column("processing_status", sqlmodel.sql.sqltypes.AutoString(length=20), nullable=False),
        sa.Column("last_analyzed", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=True),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=True),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        "idx_query_clusters_cost", "query_clusters", ["potential_savings_cents", "query_count"], unique=False
    )
    op.create_index("idx_query_clusters_priority", "query_clusters", ["priority_score", "roi_score"], unique=False)
    op.create_index(
        "idx_query_clusters_quality", "query_clusters", ["avg_quality_score", "processing_status"], unique=False
    )
    op.create_index(op.f("ix_query_clusters_canonical_query"), "query_clusters", ["canonical_query"], unique=False)
    op.create_index(op.f("ix_query_clusters_first_seen"), "query_clusters", ["first_seen"], unique=False)
    op.create_index(op.f("ix_query_clusters_last_seen"), "query_clusters", ["last_seen"], unique=False)
    op.create_index(op.f("ix_query_clusters_normalized_form"), "query_clusters", ["normalized_form"], unique=False)
    op.create_table(
        "data_export_requests",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("user_id", sa.Integer(), nullable=False),
        sa.Column("format", sqlmodel.sql.sqltypes.AutoString(length=20), nullable=False),
        sa.Column("privacy_level", sqlmodel.sql.sqltypes.AutoString(length=20), nullable=False),
        sa.Column("include_sensitive", sa.Boolean(), nullable=False),
        sa.Column("anonymize_pii", sa.Boolean(), nullable=False),
        sa.Column("date_from", sa.Date(), nullable=True),
        sa.Column("date_to", sa.Date(), nullable=True),
        sa.Column("include_fatture", sa.Boolean(), nullable=False),
        sa.Column("include_f24", sa.Boolean(), nullable=False),
        sa.Column("include_dichiarazioni", sa.Boolean(), nullable=False),
        sa.Column("mask_codice_fiscale", sa.Boolean(), nullable=False),
        sa.Column("include_profile", sa.Boolean(), nullable=False),
        sa.Column("include_queries", sa.Boolean(), nullable=False),
        sa.Column("include_documents", sa.Boolean(), nullable=False),
        sa.Column("include_calculations", sa.Boolean(), nullable=False),
        sa.Column("include_subscriptions", sa.Boolean(), nullable=False),
        sa.Column("include_invoices", sa.Boolean(), nullable=False),
        sa.Column("include_usage_stats", sa.Boolean(), nullable=False),
        sa.Column("include_faq_interactions", sa.Boolean(), nullable=False),
        sa.Column("include_knowledge_searches", sa.Boolean(), nullable=False),
        sa.Column("status", sqlmodel.sql.sqltypes.AutoString(length=20), nullable=False),
        sa.Column("requested_at", sa.DateTime(), nullable=False),
        sa.Column("started_at", sa.DateTime(), nullable=True),
        sa.Column("completed_at", sa.DateTime(), nullable=True),
        sa.Column("expires_at", sa.DateTime(), nullable=False),
        sa.Column("file_size_bytes", sa.BigInteger(), nullable=True),
        sa.Column("download_url", sqlmodel.sql.sqltypes.AutoString(length=500), nullable=True),
        sa.Column("download_count", sa.Integer(), nullable=False),
        sa.Column("max_downloads", sa.Integer(), nullable=False),
        sa.Column("error_message", sa.Text(), nullable=True),
        sa.Column("retry_count", sa.Integer(), nullable=False),
        sa.Column("max_retries", sa.Integer(), nullable=False),
        sa.Column("request_ip", sqlmodel.sql.sqltypes.AutoString(length=45), nullable=True),
        sa.Column("user_agent", sa.Text(), nullable=True),
        sa.Column("download_ips", sa.JSON(), nullable=True),
        sa.Column("gdpr_lawful_basis", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("data_controller", sqlmodel.sql.sqltypes.AutoString(length=255), nullable=False),
        sa.Column("retention_notice", sa.Text(), nullable=True),
        sa.Column("created_at", sa.DateTime(), nullable=True),
        sa.Column("updated_at", sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["user.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "electronic_invoices",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("user_id", sa.Integer(), nullable=False),
        sa.Column("invoice_number", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=False),
        sa.Column("invoice_date", sa.Date(), nullable=False),
        sa.Column("xml_content", sa.Text(), nullable=True),
        sa.Column("xml_hash", sqlmodel.sql.sqltypes.AutoString(length=64), nullable=True),
        sa.Column("sdi_transmission_id", sqlmodel.sql.sqltypes.AutoString(length=255), nullable=True),
        sa.Column("sdi_status", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=True),
        sa.Column("sdi_response", sa.Text(), nullable=True),
        sa.Column("created_at", sa.DateTime(), nullable=True),
        sa.Column("transmitted_at", sa.DateTime(), nullable=True),
        sa.Column("accepted_at", sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["user.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "export_document_analysis",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("user_id", sa.Integer(), nullable=False),
        sa.Column("filename", sqlmodel.sql.sqltypes.AutoString(length=255), nullable=False),
        sa.Column("file_type", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("file_size_bytes", sa.BigInteger(), nullable=True),
        sa.Column("analysis_type", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("processing_time_ms", sa.Integer(), nullable=True),
        sa.Column("analysis_status", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=False),
        sa.Column("entities_found", sa.Integer(), nullable=True),
        sa.Column("confidence_score", sa.Numeric(precision=3, scale=2), nullable=True),
        sa.Column("document_category", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("tax_year", sa.Integer(), nullable=True),
        sa.Column("uploaded_at", sa.DateTime(), nullable=False),
        sa.Column("analyzed_at", sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["user.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "export_tax_calculations",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("user_id", sa.Integer(), nullable=False),
        sa.Column("calculation_type", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=False),
        sa.Column("input_amount", sa.Numeric(precision=12, scale=2), nullable=False),
        sa.Column("result", sa.JSON(), nullable=False),
        sa.Column("parameters", sa.JSON(), nullable=True),
        sa.Column("tax_year", sa.Integer(), nullable=True),
        sa.Column("region", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=True),
        sa.Column("municipality", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("session_id", sqlmodel.sql.sqltypes.AutoString(length=255), nullable=True),
        sa.Column("timestamp", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["user.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "faq_candidates",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("cluster_id", sa.Uuid(), nullable=False),
        sa.Column("suggested_question", sa.Text(), nullable=False),
        sa.Column("best_response_content", sa.Text(), nullable=False),
        sa.Column("best_response_id", sa.Uuid(), nullable=True),
        sa.Column("suggested_category", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("suggested_tags", postgresql.ARRAY(sa.String(length=50)), nullable=True),
        sa.Column("regulatory_references", postgresql.ARRAY(sa.String(length=200)), nullable=True),
        sa.Column("frequency", sa.Integer(), nullable=False),
        sa.Column("estimated_monthly_savings", sa.Numeric(precision=10, scale=2), nullable=False),
        sa.Column("roi_score", sa.Numeric(precision=10, scale=2), nullable=False),
        sa.Column("priority_score", sa.Numeric(precision=10, scale=2), nullable=False),
        sa.Column("generation_prompt", sa.Text(), nullable=True),
        sa.Column("generation_model_suggested", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=False),
        sa.Column("quality_threshold", sa.Numeric(precision=3, scale=2), nullable=True),
        sa.Column("status", sqlmodel.sql.sqltypes.AutoString(length=30), nullable=False),
        sa.Column("generation_attempts", sa.Integer(), nullable=False),
        sa.Column("max_generation_attempts", sa.Integer(), nullable=False),
        sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=True),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("expires_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("analysis_metadata", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("generation_metadata", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.ForeignKeyConstraint(
            ["cluster_id"],
            ["query_clusters.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index("idx_faq_candidates_priority", "faq_candidates", ["priority_score", "status"], unique=False)
    op.create_index(
        "idx_faq_candidates_roi", "faq_candidates", ["roi_score", "estimated_monthly_savings"], unique=False
    )
    op.create_index("idx_faq_candidates_status", "faq_candidates", ["status", "created_at"], unique=False)
    op.create_table(
        "faq_generation_jobs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("job_type", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=False),
        sa.Column("job_name", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),
        sa.Column("parameters", postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column("priority", sa.Integer(), nullable=False),
        sa.Column("status", sqlmodel.sql.sqltypes.AutoString(length=30), nullable=False),
        sa.Column("progress_percentage", sa.Integer(), nullable=False),
        sa.Column("progress_description", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=True),
        sa.Column("started_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("completed_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("execution_time_seconds", sa.Integer(), nullable=True),
        sa.Column("items_processed", sa.Integer(), nullable=False),
        sa.Column("items_successful", sa.Integer(), nullable=False),
        sa.Column("items_failed", sa.Integer(), nullable=False),
        sa.Column("total_cost_cents", sa.Integer(), nullable=False),
        sa.Column("error_message", sa.Text(), nullable=True),
        sa.Column("retry_count", sa.Integer(), nullable=False),
        sa.Column("max_retries", sa.Integer(), nullable=False),
        sa.Column("result_data", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("output_references", postgresql.ARRAY(sa.String(length=100)), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=True),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("created_by", sa.Integer(), nullable=True),
        sa.Column("celery_task_id", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.ForeignKeyConstraint(
            ["created_by"],
            ["user.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index("idx_faq_jobs_celery", "faq_generation_jobs", ["celery_task_id"], unique=False)
    op.create_index("idx_faq_jobs_status", "faq_generation_jobs", ["status", "priority", "created_at"], unique=False)
    op.create_index("idx_faq_jobs_type", "faq_generation_jobs", ["job_type", "status"], unique=False)
    op.create_index(
        op.f("ix_faq_generation_jobs_celery_task_id"), "faq_generation_jobs", ["celery_task_id"], unique=False
    )
    op.create_table(
        "faq_interactions",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("user_id", sa.Integer(), nullable=False),
        sa.Column("faq_id", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("question", sa.Text(), nullable=False),
        sa.Column("answer", sa.Text(), nullable=True),
        sa.Column("category", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("viewed_at", sa.DateTime(), nullable=False),
        sa.Column("time_spent_seconds", sa.Integer(), nullable=True),
        sa.Column("helpful_rating", sa.Integer(), nullable=True),
        sa.Column("feedback", sa.Text(), nullable=True),
        sa.Column("italian_content", sa.Boolean(), nullable=False),
        sa.Column("tax_related", sa.Boolean(), nullable=False),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["user.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "knowledge_base_searches",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("user_id", sa.Integer(), nullable=False),
        sa.Column("search_query", sa.Text(), nullable=False),
        sa.Column("results_count", sa.Integer(), nullable=False),
        sa.Column("clicked_result_id", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("clicked_position", sa.Integer(), nullable=True),
        sa.Column("search_filters", sa.JSON(), nullable=True),
        sa.Column("search_category", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("italian_query", sa.Boolean(), nullable=False),
        sa.Column("regulatory_content", sa.Boolean(), nullable=False),
        sa.Column("searched_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["user.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "query_history",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("user_id", sa.Integer(), nullable=False),
        sa.Column("query", sa.Text(), nullable=False),
        sa.Column("response", sa.Text(), nullable=True),
        sa.Column("response_cached", sa.Boolean(), nullable=False),
        sa.Column("response_time_ms", sa.Integer(), nullable=True),
        sa.Column("tokens_used", sa.Integer(), nullable=True),
        sa.Column("cost_cents", sa.Integer(), nullable=True),
        sa.Column("model_used", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("session_id", sqlmodel.sql.sqltypes.AutoString(length=255), nullable=True),
        sa.Column("conversation_id", sa.Uuid(), nullable=True),
        sa.Column("query_type", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=True),
        sa.Column("italian_content", sa.Boolean(), nullable=False),
        sa.Column("timestamp", sa.DateTime(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=True),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["user.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "system_improvements",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("improvement_type", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("title", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),
        sa.Column("description", sa.Text(), nullable=False),
        sa.Column("category", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=False),
        sa.Column("trigger_pattern_id", sa.Uuid(), nullable=True),
        sa.Column("expert_feedback_ids", postgresql.ARRAY(sa.String()), nullable=True),
        sa.Column("justification", sa.Text(), nullable=False),
        sa.Column("implementation_details", sa.JSON(), nullable=True),
        sa.Column("status", sqlmodel.sql.sqltypes.AutoString(length=20), nullable=False),
        sa.Column("target_metrics", sa.JSON(), nullable=True),
        sa.Column("baseline_metrics", sa.JSON(), nullable=True),
        sa.Column("actual_metrics", sa.JSON(), nullable=True),
        sa.Column("confidence_score", sa.Float(), nullable=False),
        sa.Column("priority_score", sa.Float(), nullable=False),
        sa.Column("estimated_impact", sa.Float(), nullable=False),
        sa.Column("planned_start_date", sa.DateTime(), nullable=True),
        sa.Column("actual_start_date", sa.DateTime(), nullable=True),
        sa.Column("planned_completion_date", sa.DateTime(), nullable=True),
        sa.Column("actual_completion_date", sa.DateTime(), nullable=True),
        sa.Column("requires_expert_validation", sa.Boolean(), nullable=False),
        sa.Column("expert_approved", sa.Boolean(), nullable=True),
        sa.Column("approving_expert_id", sa.Uuid(), nullable=True),
        sa.Column("approval_date", sa.DateTime(), nullable=True),
        sa.Column("created_by", sa.Uuid(), nullable=True),
        sa.Column("created_at", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.Column("updated_at", sa.DateTime(), server_default=sa.text("now()"), nullable=True),
        sa.CheckConstraint("confidence_score >= 0.0 AND confidence_score <= 1.0", name="improvement_confidence_range"),
        sa.CheckConstraint("estimated_impact >= 0.0 AND estimated_impact <= 1.0", name="estimated_impact_range"),
        sa.CheckConstraint("priority_score >= 0.0 AND priority_score <= 1.0", name="priority_score_range"),
        sa.ForeignKeyConstraint(
            ["trigger_pattern_id"],
            ["failure_patterns.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index("idx_system_improvements_pattern", "system_improvements", ["trigger_pattern_id"], unique=False)
    op.create_index("idx_system_improvements_priority", "system_improvements", ["priority_score"], unique=False)
    op.create_index("idx_system_improvements_status", "system_improvements", ["status"], unique=False)
    op.create_index("idx_system_improvements_type", "system_improvements", ["improvement_type"], unique=False)
    op.create_table(
        "export_audit_logs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("export_request_id", sa.Uuid(), nullable=False),
        sa.Column("user_id", sa.Integer(), nullable=False),
        sa.Column("activity_type", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=False),
        sa.Column("activity_timestamp", sa.DateTime(), nullable=False),
        sa.Column("ip_address", sqlmodel.sql.sqltypes.AutoString(length=45), nullable=True),
        sa.Column("user_agent", sa.Text(), nullable=True),
        sa.Column("session_id", sqlmodel.sql.sqltypes.AutoString(length=255), nullable=True),
        sa.Column("activity_data", sa.JSON(), nullable=True),
        sa.Column("suspicious_activity", sa.Boolean(), nullable=False),
        sa.Column("security_notes", sa.Text(), nullable=True),
        sa.ForeignKeyConstraint(
            ["export_request_id"],
            ["data_export_requests.id"],
        ),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["user.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "generated_faqs",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("candidate_id", sa.Uuid(), nullable=False),
        sa.Column("question", sa.Text(), nullable=False),
        sa.Column("answer", sa.Text(), nullable=False),
        sa.Column("category", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("tags", postgresql.ARRAY(sa.String(length=50)), nullable=True),
        sa.Column("quality_score", sa.Numeric(precision=3, scale=2), nullable=False),
        sa.Column("quality_details", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("validation_notes", sa.Text(), nullable=True),
        sa.Column("regulatory_refs", postgresql.ARRAY(sa.String(length=200)), nullable=True),
        sa.Column("legal_review_required", sa.Boolean(), nullable=False),
        sa.Column("compliance_notes", sa.Text(), nullable=True),
        sa.Column("generation_model", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=False),
        sa.Column("generation_cost_cents", sa.Integer(), nullable=False),
        sa.Column("generation_tokens", sa.Integer(), nullable=True),
        sa.Column("generation_time_ms", sa.Integer(), nullable=True),
        sa.Column("estimated_monthly_savings", sa.Numeric(precision=10, scale=2), nullable=False),
        sa.Column("source_query_count", sa.Integer(), nullable=False),
        sa.Column("approval_status", sqlmodel.sql.sqltypes.AutoString(length=30), nullable=False),
        sa.Column("approved_by", sa.Integer(), nullable=True),
        sa.Column("approved_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("rejection_reason", sa.Text(), nullable=True),
        sa.Column("published", sa.Boolean(), nullable=False),
        sa.Column("published_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("faq_id", sqlmodel.sql.sqltypes.AutoString(length=100), nullable=True),
        sa.Column("view_count", sa.Integer(), nullable=False),
        sa.Column("usage_count", sa.Integer(), nullable=False),
        sa.Column("satisfaction_score", sa.Numeric(precision=3, scale=2), nullable=True),
        sa.Column("feedback_count", sa.Integer(), nullable=False),
        sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=True),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("generation_metadata", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("auto_generated", sa.Boolean(), nullable=False),
        sa.ForeignKeyConstraint(
            ["approved_by"],
            ["user.id"],
        ),
        sa.ForeignKeyConstraint(
            ["candidate_id"],
            ["faq_candidates.id"],
        ),
        sa.ForeignKeyConstraint(
            ["faq_id"],
            ["faq_entries.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("candidate_id", "question", name="uq_candidate_question"),
    )
    op.create_index("idx_generated_faqs_approval", "generated_faqs", ["approval_status", "created_at"], unique=False)
    op.create_index(
        "idx_generated_faqs_performance", "generated_faqs", ["usage_count", "satisfaction_score"], unique=False
    )
    op.create_index("idx_generated_faqs_quality", "generated_faqs", ["quality_score", "approval_status"], unique=False)
    op.create_table(
        "rss_faq_impacts",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("faq_id", sa.Uuid(), nullable=False),
        sa.Column("rss_update_id", sa.Uuid(), nullable=False),
        sa.Column("impact_level", sqlmodel.sql.sqltypes.AutoString(length=20), nullable=False),
        sa.Column("impact_score", sa.Numeric(precision=3, scale=2), nullable=False),
        sa.Column("confidence_score", sa.Numeric(precision=3, scale=2), nullable=False),
        sa.Column("rss_source", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),
        sa.Column("rss_title", sa.Text(), nullable=False),
        sa.Column("rss_summary", sa.Text(), nullable=True),
        sa.Column("rss_published_date", sa.DateTime(timezone=True), nullable=False),
        sa.Column("rss_url", sqlmodel.sql.sqltypes.AutoString(length=500), nullable=True),
        sa.Column("matching_tags", postgresql.ARRAY(sa.String(length=50)), nullable=True),
        sa.Column("matching_keywords", postgresql.ARRAY(sa.String(length=100)), nullable=True),
        sa.Column("regulatory_changes", postgresql.ARRAY(sa.String(length=200)), nullable=True),
        sa.Column("action_required", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=False),
        sa.Column("action_taken", sqlmodel.sql.sqltypes.AutoString(length=50), nullable=True),
        sa.Column("action_date", sa.DateTime(timezone=True), nullable=True),
        sa.Column("action_by", sa.Integer(), nullable=True),
        sa.Column("processed", sa.Boolean(), nullable=False),
        sa.Column("processing_notes", sa.Text(), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.text("now()"), nullable=True),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("analysis_metadata", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.ForeignKeyConstraint(
            ["action_by"],
            ["user.id"],
        ),
        sa.ForeignKeyConstraint(
            ["faq_id"],
            ["generated_faqs.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index("idx_rss_impacts_date", "rss_faq_impacts", ["rss_published_date", "created_at"], unique=False)
    op.create_index("idx_rss_impacts_faq", "rss_faq_impacts", ["faq_id", "impact_level"], unique=False)
    op.create_index(
        "idx_rss_impacts_priority", "rss_faq_impacts", ["impact_level", "action_required", "processed"], unique=False
    )
    safe_drop_index("checkpoint_writes_thread_id_idx", "checkpoint_writes")
    safe_drop_table("checkpoint_writes")
    safe_drop_index("checkpoint_blobs_thread_id_idx", "checkpoint_blobs")
    safe_drop_table("checkpoint_blobs")
    safe_drop_index("idx_expert_faq_candidates_category", "expert_faq_candidates")
    safe_drop_index("idx_expert_faq_candidates_expert", "expert_faq_candidates")
    safe_drop_index("idx_expert_faq_candidates_priority", "expert_faq_candidates")
    safe_drop_index("idx_expert_faq_candidates_status", "expert_faq_candidates")
    safe_drop_index("idx_expert_faq_question_embedding_ivfflat", "expert_faq_candidates")
    safe_drop_table("expert_faq_candidates")
    safe_drop_index("checkpoints_thread_id_idx", "checkpoints")
    safe_drop_table("checkpoints")
    safe_drop_table("checkpoint_migrations")
    safe_drop_index("idx_egt_created_at", "expert_generated_tasks")
    safe_drop_index("idx_egt_expert_id", "expert_generated_tasks")
    safe_drop_index("idx_egt_feedback_id", "expert_generated_tasks")
    safe_drop_index("idx_egt_task_id", "expert_generated_tasks")
    safe_drop_table("expert_generated_tasks")
    op.alter_column(
        "cost_alerts",
        "user_id",
        existing_type=sa.VARCHAR(),
        type_=sa.Integer(),
        existing_nullable=True,
        postgresql_using="user_id::integer",
    )
    op.create_foreign_key(None, "cost_alerts", "user", ["user_id"], ["id"])
    op.alter_column(
        "cost_optimization_suggestions",
        "user_id",
        existing_type=sa.VARCHAR(),
        type_=sa.Integer(),
        existing_nullable=True,
        postgresql_using="user_id::integer",
    )
    op.create_foreign_key(None, "cost_optimization_suggestions", "user", ["user_id"], ["id"])
    op.alter_column(
        "document_collections",
        "name",
        existing_type=sa.VARCHAR(length=200),
        comment=None,
        existing_comment="Collection name",
        existing_nullable=False,
    )
    op.alter_column(
        "document_collections",
        "description",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        comment=None,
        existing_comment="Collection description",
        existing_nullable=True,
    )
    op.alter_column(
        "document_collections",
        "source",
        existing_type=sa.VARCHAR(length=100),
        comment=None,
        existing_comment="Primary source authority",
        existing_nullable=False,
    )
    op.alter_column(
        "document_collections",
        "document_type",
        existing_type=sa.VARCHAR(length=100),
        comment=None,
        existing_comment="Type of documents in collection",
        existing_nullable=False,
    )
    op.alter_column(
        "document_collections",
        "document_count",
        existing_type=sa.INTEGER(),
        comment=None,
        existing_comment="Number of documents",
        existing_nullable=False,
    )
    op.alter_column(
        "document_collections",
        "total_content_length",
        existing_type=sa.INTEGER(),
        comment=None,
        existing_comment="Total content length",
        existing_nullable=False,
    )
    op.alter_column(
        "document_collections",
        "earliest_document",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        comment=None,
        existing_comment="Publication date of earliest document",
        existing_nullable=True,
    )
    op.alter_column(
        "document_collections",
        "latest_document",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        comment=None,
        existing_comment="Publication date of latest document",
        existing_nullable=True,
    )
    op.alter_column(
        "document_collections",
        "status",
        existing_type=sa.VARCHAR(length=20),
        comment=None,
        existing_comment="Collection status",
        existing_nullable=False,
    )
    op.alter_column(
        "document_collections",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        comment=None,
        existing_comment="Collection creation timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "document_collections",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        comment=None,
        existing_comment="Last update timestamp",
        existing_server_default=sa.text("now()"),
    )
    safe_drop_index("idx_document_collections_created_at", "document_collections")
    safe_drop_index("idx_document_collections_document_type", "document_collections")
    safe_drop_index("idx_document_collections_source", "document_collections")
    safe_drop_index("idx_document_collections_status", "document_collections")
    op.alter_column(
        "document_processing_log",
        "document_id",
        existing_type=sa.VARCHAR(length=100),
        comment=None,
        existing_comment="Associated regulatory document ID",
        existing_nullable=True,
    )
    op.alter_column(
        "document_processing_log",
        "document_url",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        comment=None,
        existing_comment="Document URL",
        existing_nullable=False,
    )
    op.alter_column(
        "document_processing_log",
        "operation",
        existing_type=sa.VARCHAR(length=50),
        comment=None,
        existing_comment="Operation type (create, update, archive, etc.)",
        existing_nullable=False,
    )
    op.alter_column(
        "document_processing_log",
        "status",
        existing_type=sa.VARCHAR(length=20),
        comment=None,
        existing_comment="Operation status (success, failed, partial)",
        existing_nullable=False,
    )
    op.alter_column(
        "document_processing_log",
        "processing_time_ms",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        comment=None,
        existing_comment="Processing time in milliseconds",
        existing_nullable=True,
    )
    op.alter_column(
        "document_processing_log",
        "content_length",
        existing_type=sa.INTEGER(),
        comment=None,
        existing_comment="Extracted content length",
        existing_nullable=True,
    )
    op.alter_column(
        "document_processing_log",
        "error_message",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        comment=None,
        existing_comment="Error message if operation failed",
        existing_nullable=True,
    )
    op.alter_column(
        "document_processing_log",
        "error_details",
        existing_type=postgresql.JSON(astext_type=sa.Text()),
        comment=None,
        existing_comment="Detailed error information",
        existing_nullable=True,
    )
    op.alter_column(
        "document_processing_log",
        "triggered_by",
        existing_type=sa.VARCHAR(length=50),
        comment=None,
        existing_comment="What triggered this operation (scheduler, manual, api)",
        existing_nullable=False,
    )
    op.alter_column(
        "document_processing_log",
        "feed_url",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        comment=None,
        existing_comment="Source RSS feed URL",
        existing_nullable=True,
    )
    op.alter_column(
        "document_processing_log",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        comment=None,
        existing_comment="Log entry timestamp",
        existing_server_default=sa.text("now()"),
    )
    safe_drop_index("idx_document_processing_log_created_at", "document_processing_log")
    safe_drop_index("idx_document_processing_log_document_id", "document_processing_log")
    safe_drop_index("idx_document_processing_log_operation", "document_processing_log")
    safe_drop_index("idx_document_processing_log_status", "document_processing_log")
    safe_drop_index("idx_document_processing_log_status_date", "document_processing_log")
    safe_drop_index("idx_document_processing_log_triggered_by", "document_processing_log")
    op.create_foreign_key(None, "document_processing_log", "regulatory_documents", ["document_id"], ["id"])
    op.alter_column(
        "expert_feedback",
        "feedback_type",
        existing_type=postgresql.ENUM("correct", "incomplete", "incorrect", name="feedback_type"),
        type_=sqlmodel.sql.sqltypes.AutoString(length=20),
        existing_nullable=False,
    )
    op.alter_column(
        "expert_feedback",
        "category",
        existing_type=postgresql.ENUM(
            "normativa_obsoleta",
            "interpretazione_errata",
            "caso_mancante",
            "calcolo_sbagliato",
            "troppo_generico",
            name="italian_feedback_category",
        ),
        type_=sqlmodel.sql.sqltypes.AutoString(length=50),
        existing_nullable=True,
    )
    op.alter_column(
        "expert_feedback",
        "regulatory_references",
        existing_type=postgresql.ARRAY(sa.TEXT()),
        type_=postgresql.ARRAY(sa.String()),
        existing_nullable=True,
        existing_server_default=sa.text("'{}'::text[]"),
    )
    op.alter_column(
        "expert_feedback",
        "confidence_score",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        nullable=False,
        existing_server_default=sa.text("0.0"),
    )
    op.alter_column(
        "expert_feedback",
        "feedback_timestamp",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        existing_nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "expert_feedback",
        "improvement_applied",
        existing_type=sa.BOOLEAN(),
        nullable=False,
        existing_server_default=sa.text("false"),
    )
    op.alter_column(
        "expert_feedback",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        existing_nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "expert_feedback",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        existing_nullable=True,
        existing_server_default=sa.text("now()"),
    )
    safe_drop_index("idx_expert_feedback_generated_faq_id", "expert_feedback")
    safe_drop_index("idx_expert_feedback_task_id", "expert_feedback")
    safe_drop_index("idx_expert_feedback_timestamp", "expert_feedback")
    op.execute(
        sa.text("CREATE INDEX IF NOT EXISTS idx_expert_feedback_timestamp ON expert_feedback (feedback_timestamp)")
    )
    safe_drop_constraint("expert_feedback_expert_id_fkey", "expert_feedback", "foreignkey")
    # FK recreation handled by SQLModel.metadata.create_all in CI
    safe_drop_column("expert_feedback", "task_creation_error")
    safe_drop_column("expert_feedback", "generated_task_id")
    safe_drop_column("expert_feedback", "task_creation_success")
    safe_drop_column("expert_feedback", "generated_faq_id")
    safe_drop_column("expert_feedback", "additional_details")
    safe_drop_column("expert_feedback", "task_creation_attempted")
    op.alter_column(
        "expert_profiles",
        "credentials",
        existing_type=postgresql.ARRAY(sa.TEXT()),
        type_=postgresql.ARRAY(sa.String()),
        existing_nullable=True,
        existing_server_default=sa.text("'{}'::text[]"),
    )
    op.alter_column(
        "expert_profiles",
        "credential_types",
        existing_type=postgresql.ARRAY(
            postgresql.ENUM(
                "dottore_commercialista",
                "revisore_legale",
                "consulente_fiscale",
                "consulente_lavoro",
                "caf_operator",
                "admin",
                name="expert_credential_type",
            )
        ),
        type_=postgresql.ARRAY(sa.String()),
        existing_nullable=True,
        existing_server_default=sa.text("'{}'::expert_credential_type[]"),
    )
    op.alter_column(
        "expert_profiles",
        "experience_years",
        existing_type=sa.INTEGER(),
        nullable=False,
        existing_server_default=sa.text("0"),
    )
    op.alter_column(
        "expert_profiles",
        "specializations",
        existing_type=postgresql.ARRAY(sa.TEXT()),
        type_=postgresql.ARRAY(sa.String()),
        existing_nullable=True,
        existing_server_default=sa.text("'{}'::text[]"),
    )
    op.alter_column(
        "expert_profiles",
        "feedback_count",
        existing_type=sa.INTEGER(),
        nullable=False,
        existing_server_default=sa.text("0"),
    )
    op.alter_column(
        "expert_profiles",
        "feedback_accuracy_rate",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        nullable=False,
        existing_server_default=sa.text("0.0"),
    )
    op.alter_column(
        "expert_profiles",
        "average_response_time_seconds",
        existing_type=sa.INTEGER(),
        nullable=False,
        existing_server_default=sa.text("0"),
    )
    op.alter_column(
        "expert_profiles",
        "trust_score",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        nullable=False,
        existing_server_default=sa.text("0.5"),
    )
    op.alter_column(
        "expert_profiles",
        "is_verified",
        existing_type=sa.BOOLEAN(),
        nullable=False,
        existing_server_default=sa.text("false"),
    )
    op.alter_column(
        "expert_profiles",
        "verification_date",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        existing_nullable=True,
    )
    op.alter_column(
        "expert_profiles",
        "is_active",
        existing_type=sa.BOOLEAN(),
        nullable=False,
        existing_server_default=sa.text("true"),
    )
    op.alter_column(
        "expert_profiles",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        existing_nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "expert_profiles",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        existing_nullable=True,
        existing_server_default=sa.text("now()"),
    )
    safe_drop_constraint("expert_profiles_user_id_key", "expert_profiles", "unique")
    safe_drop_constraint("expert_profiles_user_id_fkey", "expert_profiles", "foreignkey")
    # FK recreation handled by SQLModel.metadata.create_all in CI
    safe_add_column("faq_entries", sa.Column("similarity_score", sa.Float(), nullable=True))
    op.alter_column("faq_entries", "question", existing_type=sa.TEXT(), nullable=True)
    op.alter_column("faq_entries", "answer", existing_type=sa.TEXT(), nullable=True)
    op.alter_column(
        "faq_entries",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "faq_entries",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "faq_entries",
        "search_vector",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        existing_nullable=True,
    )
    safe_drop_index("idx_faq_entries_category", "faq_entries")
    safe_drop_index("idx_faq_entries_category_needs_review", "faq_entries")
    safe_drop_index("idx_faq_entries_created_at", "faq_entries")
    safe_drop_index("idx_faq_entries_fts", "faq_entries")
    safe_drop_index("idx_faq_entries_hit_count", "faq_entries")
    safe_drop_index("idx_faq_entries_language", "faq_entries")
    safe_drop_index("idx_faq_entries_language_category", "faq_entries")
    safe_drop_index("idx_faq_entries_last_used", "faq_entries")
    safe_drop_index("idx_faq_entries_needs_review", "faq_entries")
    safe_drop_index("idx_faq_entries_question_embedding_ivfflat", "faq_entries")
    safe_drop_index("idx_faq_entries_update_sensitivity", "faq_entries")
    safe_drop_index("idx_faq_entries_updated_at", "faq_entries")
    safe_drop_column("faq_entries", "question_embedding")
    op.alter_column(
        "feed_status",
        "feed_url",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        comment=None,
        existing_comment="RSS feed URL",
        existing_nullable=False,
    )
    op.alter_column(
        "feed_status",
        "source",
        existing_type=sa.VARCHAR(length=100),
        comment=None,
        existing_comment="Source authority",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "feed_type",
        existing_type=sa.VARCHAR(length=100),
        comment=None,
        existing_comment="Type of feed",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "parser",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "status",
        existing_type=sa.VARCHAR(length=20),
        comment=None,
        existing_comment="Current status (healthy, unhealthy, error)",
        existing_nullable=False,
    )
    op.alter_column(
        "feed_status",
        "last_checked",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        comment=None,
        existing_comment="Last health check timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "feed_status",
        "last_success",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        comment=None,
        existing_comment="Last successful fetch timestamp",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "response_time_ms",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        comment=None,
        existing_comment="Last response time in milliseconds",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "items_found",
        existing_type=sa.INTEGER(),
        comment=None,
        existing_comment="Number of items in last successful fetch",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "consecutive_errors",
        existing_type=sa.INTEGER(),
        comment=None,
        existing_comment="Count of consecutive errors",
        existing_nullable=False,
    )
    op.alter_column(
        "feed_status",
        "errors",
        existing_type=sa.INTEGER(),
        comment=None,
        existing_comment="Total error count",
        existing_nullable=False,
    )
    op.alter_column(
        "feed_status",
        "last_error",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        comment=None,
        existing_comment="Last error message",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "last_error_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        comment=None,
        existing_comment="Last error timestamp",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "check_interval_minutes",
        existing_type=sa.INTEGER(),
        comment=None,
        existing_comment="Check interval in minutes",
        existing_nullable=False,
    )
    op.alter_column(
        "feed_status",
        "enabled",
        existing_type=sa.BOOLEAN(),
        comment=None,
        existing_comment="Whether feed monitoring is enabled",
        existing_nullable=False,
    )
    op.alter_column(
        "feed_status",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        comment=None,
        existing_comment="Record creation timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "feed_status",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        comment=None,
        existing_comment="Last update timestamp",
        existing_server_default=sa.text("now()"),
    )
    safe_drop_index("idx_feed_status_enabled", "feed_status")
    safe_drop_index("idx_feed_status_errors", "feed_status")
    safe_drop_index("idx_feed_status_last_checked", "feed_status")
    safe_drop_index("idx_feed_status_parser", "feed_status")
    safe_drop_index("idx_feed_status_source", "feed_status")
    safe_drop_index("idx_feed_status_status", "feed_status")
    safe_drop_index("idx_feed_status_url_unique", "feed_status")
    op.create_unique_constraint(None, "feed_status", ["feed_url"])
    op.alter_column("knowledge_chunks", "chunk_text", existing_type=sa.TEXT(), nullable=True)
    op.alter_column(
        "knowledge_chunks", "quality_score", existing_type=sa.NUMERIC(), type_=sa.Float(), existing_nullable=True
    )
    safe_drop_index("idx_chunk_search_vector", "knowledge_chunks")
    safe_drop_index("idx_kc_fts", "knowledge_chunks")
    safe_drop_index("idx_kc_not_junk", "knowledge_chunks")
    safe_drop_index("idx_kc_vec", "knowledge_chunks")
    safe_drop_index("idx_chunk_kb_epoch", "knowledge_chunks")
    op.execute(sa.text("CREATE INDEX IF NOT EXISTS idx_chunk_kb_epoch ON knowledge_chunks (kb_epoch)"))
    safe_drop_constraint("knowledge_chunks_knowledge_item_id_fkey", "knowledge_chunks", "foreignkey")
    # FK recreation handled by SQLModel.metadata.create_all in CI
    op.alter_column(
        "knowledge_feedback",
        "user_id",
        existing_type=sa.VARCHAR(),
        type_=sa.Integer(),
        existing_nullable=False,
        postgresql_using="user_id::integer",
    )
    op.create_foreign_key(None, "knowledge_feedback", "user", ["user_id"], ["id"])
    op.alter_column(
        "knowledge_items",
        "last_accessed",
        existing_type=postgresql.TIMESTAMP(),
        type_=sa.DateTime(timezone=True),
        existing_nullable=True,
    )
    op.alter_column(
        "knowledge_items",
        "extraction_method",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        existing_nullable=True,
    )
    op.alter_column(
        "knowledge_items", "text_quality", existing_type=sa.NUMERIC(), type_=sa.Float(), existing_nullable=True
    )
    op.alter_column(
        "knowledge_items",
        "ocr_pages",
        existing_type=postgresql.JSONB(astext_type=sa.Text()),
        type_=sa.JSON(),
        existing_nullable=True,
    )
    op.alter_column(
        "knowledge_items",
        "created_at",
        existing_type=postgresql.TIMESTAMP(),
        type_=sa.DateTime(timezone=True),
        nullable=True,
    )
    op.alter_column(
        "knowledge_items",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(),
        type_=sa.DateTime(timezone=True),
        nullable=True,
    )
    op.alter_column(
        "knowledge_items",
        "reviewed_at",
        existing_type=postgresql.TIMESTAMP(),
        type_=sa.DateTime(timezone=True),
        existing_nullable=True,
    )
    safe_drop_index("idx_ki_extraction_method", "knowledge_items")
    safe_drop_index("idx_ki_fts", "knowledge_items")
    safe_drop_index("idx_ki_publication_date", "knowledge_items")
    safe_drop_index("idx_ki_text_quality", "knowledge_items")
    safe_drop_index("idx_ki_vec", "knowledge_items")
    safe_drop_index("idx_knowledge_items_kb_epoch", "knowledge_items")
    op.alter_column(
        "regulatory_documents",
        "source",
        existing_type=sa.VARCHAR(length=100),
        comment=None,
        existing_comment="Source authority (agenzia_entrate, inps, etc.)",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "source_type",
        existing_type=sa.VARCHAR(length=100),
        comment=None,
        existing_comment="Document type (circolari, risoluzioni, etc.)",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "title",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        comment=None,
        existing_comment="Document title",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "url",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        comment=None,
        existing_comment="Original document URL",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "published_date",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        comment=None,
        existing_comment="Official publication date",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "content",
        existing_type=sa.TEXT(),
        nullable=True,
        comment=None,
        existing_comment="Extracted text content",
    )
    op.alter_column(
        "regulatory_documents",
        "content_hash",
        existing_type=sa.VARCHAR(length=64),
        comment=None,
        existing_comment="SHA256 hash for duplicate detection",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "document_number",
        existing_type=sa.VARCHAR(length=50),
        comment=None,
        existing_comment="Official document number",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "authority",
        existing_type=sa.VARCHAR(length=200),
        comment=None,
        existing_comment="Publishing authority name",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "document_metadata",
        existing_type=postgresql.JSON(astext_type=sa.Text()),
        comment=None,
        existing_comment="Additional document metadata",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "version",
        existing_type=sa.INTEGER(),
        comment=None,
        existing_comment="Document version number",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "previous_version_id",
        existing_type=sa.VARCHAR(length=100),
        comment=None,
        existing_comment="ID of previous version if this is an update",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "status",
        existing_type=sa.VARCHAR(length=20),
        type_=sa.Enum(
            "PENDING", "PROCESSING", "PROCESSED", "FAILED", "ACTIVE", "SUPERSEDED", "ARCHIVED", name="processingstatus"
        ),
        comment=None,
        existing_comment="Current processing status",
        existing_nullable=False,
        postgresql_using="status::text::processingstatus",
    )
    op.alter_column(
        "regulatory_documents",
        "processed_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        comment=None,
        existing_comment="When document was successfully processed",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "processing_errors",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        comment=None,
        existing_comment="Any errors encountered during processing",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "knowledge_item_id",
        existing_type=sa.INTEGER(),
        comment=None,
        existing_comment="Associated knowledge_items record ID",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "topics",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        comment=None,
        existing_comment="Comma-separated list of topics/keywords",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "importance_score",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        comment=None,
        existing_comment="Calculated importance score (0.0-1.0)",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        comment=None,
        existing_comment="Record creation timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "regulatory_documents",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=True,
        comment=None,
        existing_comment="Last update timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "regulatory_documents",
        "archived_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        type_=sa.DateTime(),
        comment=None,
        existing_comment="When document was archived",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "archive_reason",
        existing_type=sa.TEXT(),
        type_=sqlmodel.sql.sqltypes.AutoString(),
        comment=None,
        existing_comment="Reason for archiving",
        existing_nullable=True,
    )
    safe_drop_index("idx_regulatory_documents_content_hash", "regulatory_documents")
    safe_drop_index("idx_regulatory_documents_created_at", "regulatory_documents")
    safe_drop_index("idx_regulatory_documents_published_date", "regulatory_documents")
    safe_drop_index("idx_regulatory_documents_source", "regulatory_documents")
    safe_drop_index("idx_regulatory_documents_source_status", "regulatory_documents")
    safe_drop_index("idx_regulatory_documents_source_type", "regulatory_documents")
    safe_drop_index("idx_regulatory_documents_status", "regulatory_documents")
    safe_drop_index("idx_regulatory_documents_status_published", "regulatory_documents")
    safe_drop_index("idx_regulatory_documents_updated_at", "regulatory_documents")
    safe_drop_index("idx_regulatory_documents_url", "regulatory_documents")
    safe_drop_index("idx_regulatory_documents_url_unique", "regulatory_documents")
    op.create_unique_constraint(None, "regulatory_documents", ["url"])
    op.alter_column(
        "usage_events",
        "user_id",
        existing_type=sa.VARCHAR(),
        type_=sa.Integer(),
        existing_nullable=False,
        postgresql_using="user_id::integer",
    )
    op.create_foreign_key(None, "usage_events", "user", ["user_id"], ["id"])
    op.alter_column(
        "usage_quotas",
        "user_id",
        existing_type=sa.VARCHAR(),
        type_=sa.Integer(),
        existing_nullable=False,
        postgresql_using="user_id::integer",
    )
    op.create_foreign_key(None, "usage_quotas", "user", ["user_id"], ["id"])
    op.alter_column(
        "user",
        "name",
        existing_type=sa.VARCHAR(length=255),
        comment=None,
        existing_comment="User full name from OAuth provider or manual registration",
        existing_nullable=True,
    )
    op.alter_column(
        "user",
        "avatar_url",
        existing_type=sa.VARCHAR(length=512),
        comment=None,
        existing_comment="URL to user profile picture from OAuth provider",
        existing_nullable=True,
    )
    op.alter_column(
        "user",
        "provider",
        existing_type=sa.VARCHAR(length=50),
        comment=None,
        existing_comment="Authentication provider: email, google, linkedin",
        existing_nullable=False,
        existing_server_default=sa.text("'email'::character varying"),
    )
    op.alter_column(
        "user",
        "provider_id",
        existing_type=sa.VARCHAR(length=255),
        comment=None,
        existing_comment="Unique user ID from OAuth provider",
        existing_nullable=True,
    )
    # NOTE: Role column and index are intentionally KEPT - role is part of User model
    # safe_drop_index("ix_user_role", "user")  # REMOVED: keep role index
    safe_drop_constraint("uq_user_provider_provider_id", "user", "unique")
    op.create_index(op.f("ix_user_refresh_token_hash"), "user", ["refresh_token_hash"], unique=False)
    # safe_drop_column("user", "role")  # REMOVED: keep role column
    op.alter_column(
        "user_usage_summaries",
        "user_id",
        existing_type=sa.VARCHAR(),
        type_=sa.Integer(),
        existing_nullable=False,
        postgresql_using="user_id::integer",
    )
    op.create_foreign_key(None, "user_usage_summaries", "user", ["user_id"], ["id"])
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(None, "user_usage_summaries", type_="foreignkey")
    op.alter_column(
        "user_usage_summaries", "user_id", existing_type=sa.Integer(), type_=sa.VARCHAR(), existing_nullable=False
    )
    # NOTE: Role column is kept in upgrade, so no need to re-add in downgrade
    # op.add_column("user", sa.Column("role", ...))  # REMOVED: role column is kept
    safe_drop_index("ix_user_refresh_token_hash", "user")
    op.create_unique_constraint(op.f("uq_user_provider_provider_id"), "user", ["provider", "provider_id"])
    # op.create_index(op.f("ix_user_role"), "user", ["role"], unique=False)  # REMOVED: index is kept
    op.alter_column(
        "user",
        "provider_id",
        existing_type=sa.VARCHAR(length=255),
        comment="Unique user ID from OAuth provider",
        existing_nullable=True,
    )
    op.alter_column(
        "user",
        "provider",
        existing_type=sa.VARCHAR(length=50),
        comment="Authentication provider: email, google, linkedin",
        existing_nullable=False,
        existing_server_default=sa.text("'email'::character varying"),
    )
    op.alter_column(
        "user",
        "avatar_url",
        existing_type=sa.VARCHAR(length=512),
        comment="URL to user profile picture from OAuth provider",
        existing_nullable=True,
    )
    op.alter_column(
        "user",
        "name",
        existing_type=sa.VARCHAR(length=255),
        comment="User full name from OAuth provider or manual registration",
        existing_nullable=True,
    )
    op.drop_constraint(None, "usage_quotas", type_="foreignkey")
    op.alter_column("usage_quotas", "user_id", existing_type=sa.Integer(), type_=sa.VARCHAR(), existing_nullable=False)
    op.drop_constraint(None, "usage_events", type_="foreignkey")
    op.alter_column("usage_events", "user_id", existing_type=sa.Integer(), type_=sa.VARCHAR(), existing_nullable=False)
    op.drop_constraint(None, "regulatory_documents", type_="unique")
    op.create_index(op.f("idx_regulatory_documents_url_unique"), "regulatory_documents", ["url"], unique=True)
    op.create_index(op.f("idx_regulatory_documents_url"), "regulatory_documents", ["url"], unique=False)
    op.create_index(op.f("idx_regulatory_documents_updated_at"), "regulatory_documents", ["updated_at"], unique=False)
    op.create_index(
        op.f("idx_regulatory_documents_status_published"),
        "regulatory_documents",
        ["status", "published_date"],
        unique=False,
    )
    op.create_index(op.f("idx_regulatory_documents_status"), "regulatory_documents", ["status"], unique=False)
    op.create_index(
        op.f("idx_regulatory_documents_source_type"), "regulatory_documents", ["source_type"], unique=False
    )
    op.create_index(
        op.f("idx_regulatory_documents_source_status"), "regulatory_documents", ["source", "status"], unique=False
    )
    op.create_index(op.f("idx_regulatory_documents_source"), "regulatory_documents", ["source"], unique=False)
    op.create_index(
        op.f("idx_regulatory_documents_published_date"), "regulatory_documents", ["published_date"], unique=False
    )
    op.create_index(op.f("idx_regulatory_documents_created_at"), "regulatory_documents", ["created_at"], unique=False)
    op.create_index(
        op.f("idx_regulatory_documents_content_hash"), "regulatory_documents", ["content_hash"], unique=False
    )
    op.alter_column(
        "regulatory_documents",
        "archive_reason",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        comment="Reason for archiving",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "archived_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        comment="When document was archived",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        comment="Last update timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "regulatory_documents",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        comment="Record creation timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "regulatory_documents",
        "importance_score",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        comment="Calculated importance score (0.0-1.0)",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "topics",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        comment="Comma-separated list of topics/keywords",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "knowledge_item_id",
        existing_type=sa.INTEGER(),
        comment="Associated knowledge_items record ID",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "processing_errors",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        comment="Any errors encountered during processing",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "processed_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        comment="When document was successfully processed",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "status",
        existing_type=sa.Enum(
            "PENDING", "PROCESSING", "PROCESSED", "FAILED", "ACTIVE", "SUPERSEDED", "ARCHIVED", name="processingstatus"
        ),
        type_=sa.VARCHAR(length=20),
        comment="Current processing status",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "previous_version_id",
        existing_type=sa.VARCHAR(length=100),
        comment="ID of previous version if this is an update",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "version",
        existing_type=sa.INTEGER(),
        comment="Document version number",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "document_metadata",
        existing_type=postgresql.JSON(astext_type=sa.Text()),
        comment="Additional document metadata",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "authority",
        existing_type=sa.VARCHAR(length=200),
        comment="Publishing authority name",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "document_number",
        existing_type=sa.VARCHAR(length=50),
        comment="Official document number",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "content_hash",
        existing_type=sa.VARCHAR(length=64),
        comment="SHA256 hash for duplicate detection",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents", "content", existing_type=sa.TEXT(), nullable=False, comment="Extracted text content"
    )
    op.alter_column(
        "regulatory_documents",
        "published_date",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        comment="Official publication date",
        existing_nullable=True,
    )
    op.alter_column(
        "regulatory_documents",
        "url",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        comment="Original document URL",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "title",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        comment="Document title",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "source_type",
        existing_type=sa.VARCHAR(length=100),
        comment="Document type (circolari, risoluzioni, etc.)",
        existing_nullable=False,
    )
    op.alter_column(
        "regulatory_documents",
        "source",
        existing_type=sa.VARCHAR(length=100),
        comment="Source authority (agenzia_entrate, inps, etc.)",
        existing_nullable=False,
    )
    op.create_index(
        op.f("idx_knowledge_items_kb_epoch"), "knowledge_items", [sa.literal_column("kb_epoch DESC")], unique=False
    )
    op.create_index(
        op.f("idx_ki_vec"),
        "knowledge_items",
        ["embedding"],
        unique=False,
        postgresql_with={"lists": "50"},
        postgresql_using="ivfflat",
    )
    op.create_index(
        op.f("idx_ki_text_quality"),
        "knowledge_items",
        ["text_quality"],
        unique=False,
        postgresql_where="(text_quality IS NOT NULL)",
    )
    op.create_index(
        op.f("idx_ki_publication_date"),
        "knowledge_items",
        ["publication_date"],
        unique=False,
        postgresql_where="(publication_date IS NOT NULL)",
    )
    op.create_index(op.f("idx_ki_fts"), "knowledge_items", ["search_vector"], unique=False, postgresql_using="gin")
    op.create_index(
        op.f("idx_ki_extraction_method"),
        "knowledge_items",
        ["extraction_method"],
        unique=False,
        postgresql_where="(extraction_method IS NOT NULL)",
    )
    op.alter_column(
        "knowledge_items",
        "reviewed_at",
        existing_type=sa.DateTime(timezone=True),
        type_=postgresql.TIMESTAMP(),
        existing_nullable=True,
    )
    op.alter_column(
        "knowledge_items",
        "updated_at",
        existing_type=sa.DateTime(timezone=True),
        type_=postgresql.TIMESTAMP(),
        nullable=False,
    )
    op.alter_column(
        "knowledge_items",
        "created_at",
        existing_type=sa.DateTime(timezone=True),
        type_=postgresql.TIMESTAMP(),
        nullable=False,
    )
    op.alter_column(
        "knowledge_items",
        "ocr_pages",
        existing_type=sa.JSON(),
        type_=postgresql.JSONB(astext_type=sa.Text()),
        existing_nullable=True,
    )
    op.alter_column(
        "knowledge_items", "text_quality", existing_type=sa.Float(), type_=sa.NUMERIC(), existing_nullable=True
    )
    op.alter_column(
        "knowledge_items",
        "extraction_method",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        existing_nullable=True,
    )
    op.alter_column(
        "knowledge_items",
        "last_accessed",
        existing_type=sa.DateTime(timezone=True),
        type_=postgresql.TIMESTAMP(),
        existing_nullable=True,
    )
    op.drop_constraint(None, "knowledge_feedback", type_="foreignkey")
    op.alter_column(
        "knowledge_feedback", "user_id", existing_type=sa.Integer(), type_=sa.VARCHAR(), existing_nullable=False
    )
    op.drop_constraint(None, "knowledge_chunks", type_="foreignkey")
    op.create_foreign_key(
        op.f("knowledge_chunks_knowledge_item_id_fkey"),
        "knowledge_chunks",
        "knowledge_items",
        ["knowledge_item_id"],
        ["id"],
        ondelete="CASCADE",
    )
    safe_drop_index("idx_chunk_kb_epoch", "knowledge_chunks")
    op.create_index(op.f("idx_chunk_kb_epoch"), "knowledge_chunks", [sa.literal_column("kb_epoch DESC")], unique=False)
    op.create_index(
        op.f("idx_kc_vec"),
        "knowledge_chunks",
        ["embedding"],
        unique=False,
        postgresql_with={"lists": "100"},
        postgresql_using="ivfflat",
    )
    op.create_index(
        op.f("idx_kc_not_junk"),
        "knowledge_chunks",
        ["knowledge_item_id"],
        unique=False,
        postgresql_where="(junk = false)",
    )
    op.create_index(op.f("idx_kc_fts"), "knowledge_chunks", ["search_vector"], unique=False, postgresql_using="gin")
    op.create_index(
        op.f("idx_chunk_search_vector"), "knowledge_chunks", ["search_vector"], unique=False, postgresql_using="gin"
    )
    op.alter_column(
        "knowledge_chunks", "quality_score", existing_type=sa.Float(), type_=sa.NUMERIC(), existing_nullable=True
    )
    op.alter_column("knowledge_chunks", "chunk_text", existing_type=sa.TEXT(), nullable=False)
    op.drop_constraint(None, "feed_status", type_="unique")
    op.create_index(op.f("idx_feed_status_url_unique"), "feed_status", ["feed_url"], unique=True)
    op.create_index(op.f("idx_feed_status_status"), "feed_status", ["status"], unique=False)
    op.create_index(op.f("idx_feed_status_source"), "feed_status", ["source"], unique=False)
    op.create_index(op.f("idx_feed_status_parser"), "feed_status", ["parser"], unique=False)
    op.create_index(op.f("idx_feed_status_last_checked"), "feed_status", ["last_checked"], unique=False)
    op.create_index(op.f("idx_feed_status_errors"), "feed_status", ["consecutive_errors"], unique=False)
    op.create_index(op.f("idx_feed_status_enabled"), "feed_status", ["enabled"], unique=False)
    op.alter_column(
        "feed_status",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        comment="Last update timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "feed_status",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        comment="Record creation timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "feed_status",
        "enabled",
        existing_type=sa.BOOLEAN(),
        comment="Whether feed monitoring is enabled",
        existing_nullable=False,
    )
    op.alter_column(
        "feed_status",
        "check_interval_minutes",
        existing_type=sa.INTEGER(),
        comment="Check interval in minutes",
        existing_nullable=False,
    )
    op.alter_column(
        "feed_status",
        "last_error_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        comment="Last error timestamp",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "last_error",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        comment="Last error message",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status", "errors", existing_type=sa.INTEGER(), comment="Total error count", existing_nullable=False
    )
    op.alter_column(
        "feed_status",
        "consecutive_errors",
        existing_type=sa.INTEGER(),
        comment="Count of consecutive errors",
        existing_nullable=False,
    )
    op.alter_column(
        "feed_status",
        "items_found",
        existing_type=sa.INTEGER(),
        comment="Number of items in last successful fetch",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "response_time_ms",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        comment="Last response time in milliseconds",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "last_success",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        comment="Last successful fetch timestamp",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "last_checked",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        comment="Last health check timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "feed_status",
        "status",
        existing_type=sa.VARCHAR(length=20),
        comment="Current status (healthy, unhealthy, error)",
        existing_nullable=False,
    )
    op.alter_column(
        "feed_status",
        "parser",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "feed_type",
        existing_type=sa.VARCHAR(length=100),
        comment="Type of feed",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "source",
        existing_type=sa.VARCHAR(length=100),
        comment="Source authority",
        existing_nullable=True,
    )
    op.alter_column(
        "feed_status",
        "feed_url",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        comment="RSS feed URL",
        existing_nullable=False,
    )
    safe_add_column(
        "faq_entries",
        sa.Column(
            "question_embedding", pgvector.sqlalchemy.vector.VECTOR(dim=1536), autoincrement=False, nullable=True
        ),
    )
    op.create_index(op.f("idx_faq_entries_updated_at"), "faq_entries", ["updated_at"], unique=False)
    op.create_index(op.f("idx_faq_entries_update_sensitivity"), "faq_entries", ["update_sensitivity"], unique=False)
    op.create_index(
        op.f("idx_faq_entries_question_embedding_ivfflat"),
        "faq_entries",
        ["question_embedding"],
        unique=False,
        postgresql_with={"lists": "100"},
        postgresql_using="ivfflat",
    )
    op.create_index(op.f("idx_faq_entries_needs_review"), "faq_entries", ["needs_review"], unique=False)
    op.create_index(op.f("idx_faq_entries_last_used"), "faq_entries", ["last_used"], unique=False)
    op.create_index(op.f("idx_faq_entries_language_category"), "faq_entries", ["language", "category"], unique=False)
    op.create_index(op.f("idx_faq_entries_language"), "faq_entries", ["language"], unique=False)
    op.create_index(op.f("idx_faq_entries_hit_count"), "faq_entries", ["hit_count"], unique=False)
    op.create_index(
        op.f("idx_faq_entries_fts"),
        "faq_entries",
        [sa.literal_column("to_tsvector('italian'::regconfig, (question || ' '::text) || answer)")],
        unique=False,
        postgresql_using="gin",
    )
    op.create_index(op.f("idx_faq_entries_created_at"), "faq_entries", ["created_at"], unique=False)
    op.create_index(
        op.f("idx_faq_entries_category_needs_review"), "faq_entries", ["category", "needs_review"], unique=False
    )
    op.create_index(op.f("idx_faq_entries_category"), "faq_entries", ["category"], unique=False)
    op.alter_column(
        "faq_entries",
        "search_vector",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        existing_nullable=True,
    )
    op.alter_column(
        "faq_entries",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "faq_entries",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column("faq_entries", "answer", existing_type=sa.TEXT(), nullable=False)
    op.alter_column("faq_entries", "question", existing_type=sa.TEXT(), nullable=False)
    safe_drop_column("faq_entries", "similarity_score")
    op.drop_constraint(None, "expert_profiles", type_="foreignkey")
    op.create_foreign_key(
        op.f("expert_profiles_user_id_fkey"), "expert_profiles", "user", ["user_id"], ["id"], ondelete="CASCADE"
    )
    op.create_unique_constraint(op.f("expert_profiles_user_id_key"), "expert_profiles", ["user_id"])
    op.alter_column(
        "expert_profiles",
        "updated_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        existing_nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "expert_profiles",
        "created_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        existing_nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "expert_profiles",
        "is_active",
        existing_type=sa.BOOLEAN(),
        nullable=True,
        existing_server_default=sa.text("true"),
    )
    op.alter_column(
        "expert_profiles",
        "verification_date",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        existing_nullable=True,
    )
    op.alter_column(
        "expert_profiles",
        "is_verified",
        existing_type=sa.BOOLEAN(),
        nullable=True,
        existing_server_default=sa.text("false"),
    )
    op.alter_column(
        "expert_profiles",
        "trust_score",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        nullable=True,
        existing_server_default=sa.text("0.5"),
    )
    op.alter_column(
        "expert_profiles",
        "average_response_time_seconds",
        existing_type=sa.INTEGER(),
        nullable=True,
        existing_server_default=sa.text("0"),
    )
    op.alter_column(
        "expert_profiles",
        "feedback_accuracy_rate",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        nullable=True,
        existing_server_default=sa.text("0.0"),
    )
    op.alter_column(
        "expert_profiles",
        "feedback_count",
        existing_type=sa.INTEGER(),
        nullable=True,
        existing_server_default=sa.text("0"),
    )
    op.alter_column(
        "expert_profiles",
        "specializations",
        existing_type=postgresql.ARRAY(sa.String()),
        type_=postgresql.ARRAY(sa.TEXT()),
        existing_nullable=True,
        existing_server_default=sa.text("'{}'::text[]"),
    )
    op.alter_column(
        "expert_profiles",
        "experience_years",
        existing_type=sa.INTEGER(),
        nullable=True,
        existing_server_default=sa.text("0"),
    )
    op.alter_column(
        "expert_profiles",
        "credential_types",
        existing_type=postgresql.ARRAY(sa.String()),
        type_=postgresql.ARRAY(
            postgresql.ENUM(
                "dottore_commercialista",
                "revisore_legale",
                "consulente_fiscale",
                "consulente_lavoro",
                "caf_operator",
                "admin",
                name="expert_credential_type",
            )
        ),
        existing_nullable=True,
        existing_server_default=sa.text("'{}'::expert_credential_type[]"),
    )
    op.alter_column(
        "expert_profiles",
        "credentials",
        existing_type=postgresql.ARRAY(sa.String()),
        type_=postgresql.ARRAY(sa.TEXT()),
        existing_nullable=True,
        existing_server_default=sa.text("'{}'::text[]"),
    )
    safe_add_column(
        "expert_feedback",
        sa.Column(
            "task_creation_attempted",
            sa.BOOLEAN(),
            server_default=sa.text("false"),
            autoincrement=False,
            nullable=True,
        ),
    )
    safe_add_column("expert_feedback", sa.Column("additional_details", sa.TEXT(), autoincrement=False, nullable=True))
    safe_add_column(
        "expert_feedback", sa.Column("generated_faq_id", sa.VARCHAR(length=100), autoincrement=False, nullable=True)
    )
    safe_add_column(
        "expert_feedback", sa.Column("task_creation_success", sa.BOOLEAN(), autoincrement=False, nullable=True)
    )
    safe_add_column(
        "expert_feedback", sa.Column("generated_task_id", sa.VARCHAR(length=50), autoincrement=False, nullable=True)
    )
    safe_add_column("expert_feedback", sa.Column("task_creation_error", sa.TEXT(), autoincrement=False, nullable=True))
    op.drop_constraint(None, "expert_feedback", type_="foreignkey")
    op.create_foreign_key(
        op.f("expert_feedback_expert_id_fkey"),
        "expert_feedback",
        "expert_profiles",
        ["expert_id"],
        ["id"],
        ondelete="CASCADE",
    )
    safe_drop_index("idx_expert_feedback_timestamp", "expert_feedback")
    op.create_index(
        op.f("idx_expert_feedback_timestamp"),
        "expert_feedback",
        [sa.literal_column("feedback_timestamp DESC")],
        unique=False,
    )
    op.create_index(
        op.f("idx_expert_feedback_task_id"),
        "expert_feedback",
        ["generated_task_id"],
        unique=False,
        postgresql_where="(generated_task_id IS NOT NULL)",
    )
    op.create_index(
        op.f("idx_expert_feedback_generated_faq_id"), "expert_feedback", ["generated_faq_id"], unique=False
    )
    op.alter_column(
        "expert_feedback",
        "updated_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        existing_nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "expert_feedback",
        "created_at",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        existing_nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "expert_feedback",
        "improvement_applied",
        existing_type=sa.BOOLEAN(),
        nullable=True,
        existing_server_default=sa.text("false"),
    )
    op.alter_column(
        "expert_feedback",
        "feedback_timestamp",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        existing_nullable=True,
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "expert_feedback",
        "confidence_score",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        nullable=True,
        existing_server_default=sa.text("0.0"),
    )
    op.alter_column(
        "expert_feedback",
        "regulatory_references",
        existing_type=postgresql.ARRAY(sa.String()),
        type_=postgresql.ARRAY(sa.TEXT()),
        existing_nullable=True,
        existing_server_default=sa.text("'{}'::text[]"),
    )
    op.alter_column(
        "expert_feedback",
        "category",
        existing_type=sqlmodel.sql.sqltypes.AutoString(length=50),
        type_=postgresql.ENUM(
            "normativa_obsoleta",
            "interpretazione_errata",
            "caso_mancante",
            "calcolo_sbagliato",
            "troppo_generico",
            name="italian_feedback_category",
        ),
        existing_nullable=True,
    )
    op.alter_column(
        "expert_feedback",
        "feedback_type",
        existing_type=sqlmodel.sql.sqltypes.AutoString(length=20),
        type_=postgresql.ENUM("correct", "incomplete", "incorrect", name="feedback_type"),
        existing_nullable=False,
    )
    op.drop_constraint(None, "document_processing_log", type_="foreignkey")
    op.create_index(
        op.f("idx_document_processing_log_triggered_by"), "document_processing_log", ["triggered_by"], unique=False
    )
    op.create_index(
        op.f("idx_document_processing_log_status_date"),
        "document_processing_log",
        ["status", "created_at"],
        unique=False,
    )
    op.create_index(op.f("idx_document_processing_log_status"), "document_processing_log", ["status"], unique=False)
    op.create_index(
        op.f("idx_document_processing_log_operation"), "document_processing_log", ["operation"], unique=False
    )
    op.create_index(
        op.f("idx_document_processing_log_document_id"), "document_processing_log", ["document_id"], unique=False
    )
    op.create_index(
        op.f("idx_document_processing_log_created_at"), "document_processing_log", ["created_at"], unique=False
    )
    op.alter_column(
        "document_processing_log",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        comment="Log entry timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "document_processing_log",
        "feed_url",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        comment="Source RSS feed URL",
        existing_nullable=True,
    )
    op.alter_column(
        "document_processing_log",
        "triggered_by",
        existing_type=sa.VARCHAR(length=50),
        comment="What triggered this operation (scheduler, manual, api)",
        existing_nullable=False,
    )
    op.alter_column(
        "document_processing_log",
        "error_details",
        existing_type=postgresql.JSON(astext_type=sa.Text()),
        comment="Detailed error information",
        existing_nullable=True,
    )
    op.alter_column(
        "document_processing_log",
        "error_message",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        comment="Error message if operation failed",
        existing_nullable=True,
    )
    op.alter_column(
        "document_processing_log",
        "content_length",
        existing_type=sa.INTEGER(),
        comment="Extracted content length",
        existing_nullable=True,
    )
    op.alter_column(
        "document_processing_log",
        "processing_time_ms",
        existing_type=sa.DOUBLE_PRECISION(precision=53),
        comment="Processing time in milliseconds",
        existing_nullable=True,
    )
    op.alter_column(
        "document_processing_log",
        "status",
        existing_type=sa.VARCHAR(length=20),
        comment="Operation status (success, failed, partial)",
        existing_nullable=False,
    )
    op.alter_column(
        "document_processing_log",
        "operation",
        existing_type=sa.VARCHAR(length=50),
        comment="Operation type (create, update, archive, etc.)",
        existing_nullable=False,
    )
    op.alter_column(
        "document_processing_log",
        "document_url",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        comment="Document URL",
        existing_nullable=False,
    )
    op.alter_column(
        "document_processing_log",
        "document_id",
        existing_type=sa.VARCHAR(length=100),
        comment="Associated regulatory document ID",
        existing_nullable=True,
    )
    op.create_index(op.f("idx_document_collections_status"), "document_collections", ["status"], unique=False)
    op.create_index(op.f("idx_document_collections_source"), "document_collections", ["source"], unique=False)
    op.create_index(
        op.f("idx_document_collections_document_type"), "document_collections", ["document_type"], unique=False
    )
    op.create_index(op.f("idx_document_collections_created_at"), "document_collections", ["created_at"], unique=False)
    op.alter_column(
        "document_collections",
        "updated_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        comment="Last update timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "document_collections",
        "created_at",
        existing_type=postgresql.TIMESTAMP(timezone=True),
        nullable=False,
        comment="Collection creation timestamp",
        existing_server_default=sa.text("now()"),
    )
    op.alter_column(
        "document_collections",
        "status",
        existing_type=sa.VARCHAR(length=20),
        comment="Collection status",
        existing_nullable=False,
    )
    op.alter_column(
        "document_collections",
        "latest_document",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        comment="Publication date of latest document",
        existing_nullable=True,
    )
    op.alter_column(
        "document_collections",
        "earliest_document",
        existing_type=sa.DateTime(),
        type_=postgresql.TIMESTAMP(timezone=True),
        comment="Publication date of earliest document",
        existing_nullable=True,
    )
    op.alter_column(
        "document_collections",
        "total_content_length",
        existing_type=sa.INTEGER(),
        comment="Total content length",
        existing_nullable=False,
    )
    op.alter_column(
        "document_collections",
        "document_count",
        existing_type=sa.INTEGER(),
        comment="Number of documents",
        existing_nullable=False,
    )
    op.alter_column(
        "document_collections",
        "document_type",
        existing_type=sa.VARCHAR(length=100),
        comment="Type of documents in collection",
        existing_nullable=False,
    )
    op.alter_column(
        "document_collections",
        "source",
        existing_type=sa.VARCHAR(length=100),
        comment="Primary source authority",
        existing_nullable=False,
    )
    op.alter_column(
        "document_collections",
        "description",
        existing_type=sqlmodel.sql.sqltypes.AutoString(),
        type_=sa.TEXT(),
        comment="Collection description",
        existing_nullable=True,
    )
    op.alter_column(
        "document_collections",
        "name",
        existing_type=sa.VARCHAR(length=200),
        comment="Collection name",
        existing_nullable=False,
    )
    op.drop_constraint(None, "cost_optimization_suggestions", type_="foreignkey")
    op.alter_column(
        "cost_optimization_suggestions",
        "user_id",
        existing_type=sa.Integer(),
        type_=sa.VARCHAR(),
        existing_nullable=True,
    )
    op.drop_constraint(None, "cost_alerts", type_="foreignkey")
    op.alter_column("cost_alerts", "user_id", existing_type=sa.Integer(), type_=sa.VARCHAR(), existing_nullable=True)
    op.create_table(
        "expert_generated_tasks",
        sa.Column("id", sa.UUID(), server_default=sa.text("gen_random_uuid()"), autoincrement=False, nullable=False),
        sa.Column("task_id", sa.VARCHAR(length=50), autoincrement=False, nullable=False),
        sa.Column("task_name", sa.VARCHAR(length=50), autoincrement=False, nullable=False),
        sa.Column("feedback_id", sa.UUID(), autoincrement=False, nullable=False),
        sa.Column("expert_id", sa.UUID(), autoincrement=False, nullable=False),
        sa.Column("question", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("answer", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("additional_details", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column(
            "file_path",
            sa.VARCHAR(length=200),
            server_default=sa.text("'SUPER_USER_TASKS.md'::character varying"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "created_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.ForeignKeyConstraint(
            ["expert_id"], ["expert_profiles.id"], name=op.f("expert_generated_tasks_expert_id_fkey")
        ),
        sa.ForeignKeyConstraint(
            ["feedback_id"],
            ["expert_feedback.id"],
            name=op.f("expert_generated_tasks_feedback_id_fkey"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("expert_generated_tasks_pkey")),
        sa.UniqueConstraint("task_id", name=op.f("expert_generated_tasks_task_id_key")),
    )
    op.create_index(op.f("idx_egt_task_id"), "expert_generated_tasks", ["task_id"], unique=False)
    op.create_index(op.f("idx_egt_feedback_id"), "expert_generated_tasks", ["feedback_id"], unique=False)
    op.create_index(op.f("idx_egt_expert_id"), "expert_generated_tasks", ["expert_id"], unique=False)
    op.create_index(
        op.f("idx_egt_created_at"), "expert_generated_tasks", [sa.literal_column("created_at DESC")], unique=False
    )
    op.create_table(
        "checkpoint_migrations",
        sa.Column("v", sa.INTEGER(), autoincrement=False, nullable=False),
        sa.PrimaryKeyConstraint("v", name=op.f("checkpoint_migrations_pkey")),
    )
    op.create_table(
        "checkpoints",
        sa.Column("thread_id", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("checkpoint_ns", sa.TEXT(), server_default=sa.text("''::text"), autoincrement=False, nullable=False),
        sa.Column("checkpoint_id", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("parent_checkpoint_id", sa.TEXT(), autoincrement=False, nullable=True),
        sa.Column("type", sa.TEXT(), autoincrement=False, nullable=True),
        sa.Column("checkpoint", postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=False),
        sa.Column(
            "metadata",
            postgresql.JSONB(astext_type=sa.Text()),
            server_default=sa.text("'{}'::jsonb"),
            autoincrement=False,
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("thread_id", "checkpoint_ns", "checkpoint_id", name=op.f("checkpoints_pkey")),
    )
    op.create_index(op.f("checkpoints_thread_id_idx"), "checkpoints", ["thread_id"], unique=False)
    op.create_table(
        "expert_faq_candidates",
        sa.Column("id", sa.UUID(), server_default=sa.text("gen_random_uuid()"), autoincrement=False, nullable=False),
        sa.Column("question", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("answer", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column(
            "source",
            sa.VARCHAR(length=20),
            server_default=sa.text("'expert_feedback'::character varying"),
            autoincrement=False,
            nullable=False,
        ),
        sa.Column("expert_id", sa.UUID(), autoincrement=False, nullable=True),
        sa.Column("expert_trust_score", sa.DOUBLE_PRECISION(precision=53), autoincrement=False, nullable=True),
        sa.Column(
            "approval_status",
            sa.VARCHAR(length=20),
            server_default=sa.text("'pending'::character varying"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column("approved_by", sa.INTEGER(), autoincrement=False, nullable=True),
        sa.Column("approved_at", postgresql.TIMESTAMP(timezone=True), autoincrement=False, nullable=True),
        sa.Column("suggested_category", sa.VARCHAR(length=100), autoincrement=False, nullable=True),
        sa.Column(
            "suggested_tags",
            postgresql.ARRAY(sa.TEXT()),
            server_default=sa.text("'{}'::text[]"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "regulatory_references",
            postgresql.ARRAY(sa.TEXT()),
            server_default=sa.text("'{}'::text[]"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column("frequency", sa.INTEGER(), server_default=sa.text("0"), autoincrement=False, nullable=True),
        sa.Column(
            "estimated_monthly_savings",
            sa.NUMERIC(precision=10, scale=2),
            server_default=sa.text("0"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "roi_score",
            sa.NUMERIC(precision=10, scale=2),
            server_default=sa.text("0"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "priority_score",
            sa.NUMERIC(precision=10, scale=2),
            server_default=sa.text("0"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "created_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "updated_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "question_embedding",
            pgvector.sqlalchemy.vector.VECTOR(dim=1536),
            autoincrement=False,
            nullable=True,
            comment="Vector embedding of the FAQ question for semantic similarity search (OpenAI ada-002, 1536 dimensions)",
        ),
        sa.CheckConstraint(
            "approval_status::text = ANY (ARRAY['pending'::character varying, 'approved'::character varying, 'rejected'::character varying]::text[])",
            name=op.f("expert_faq_candidates_approval_status_check"),
        ),
        sa.CheckConstraint(
            "source::text = ANY (ARRAY['expert_feedback'::character varying, 'auto_generated'::character varying]::text[])",
            name=op.f("expert_faq_candidates_source_check"),
        ),
        sa.CheckConstraint("estimated_monthly_savings >= 0::numeric", name=op.f("non_negative_savings")),
        sa.CheckConstraint("frequency >= 0", name=op.f("non_negative_frequency")),
        sa.ForeignKeyConstraint(
            ["approved_by"], ["user.id"], name=op.f("expert_faq_candidates_approved_by_fkey"), ondelete="SET NULL"
        ),
        sa.ForeignKeyConstraint(
            ["expert_id"],
            ["expert_profiles.id"],
            name=op.f("expert_faq_candidates_expert_id_fkey"),
            ondelete="SET NULL",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("expert_faq_candidates_pkey")),
    )
    op.create_index(
        op.f("idx_expert_faq_question_embedding_ivfflat"),
        "expert_faq_candidates",
        ["question_embedding"],
        unique=False,
        postgresql_with={"lists": "100"},
        postgresql_using="ivfflat",
    )
    op.create_index(
        op.f("idx_expert_faq_candidates_status"),
        "expert_faq_candidates",
        ["approval_status", sa.literal_column("created_at DESC")],
        unique=False,
    )
    op.create_index(
        op.f("idx_expert_faq_candidates_priority"),
        "expert_faq_candidates",
        [sa.literal_column("priority_score DESC"), "approval_status"],
        unique=False,
    )
    op.create_index(
        op.f("idx_expert_faq_candidates_expert"),
        "expert_faq_candidates",
        ["expert_id", sa.literal_column("expert_trust_score DESC")],
        unique=False,
    )
    op.create_index(
        op.f("idx_expert_faq_candidates_category"), "expert_faq_candidates", ["suggested_category"], unique=False
    )
    op.create_table(
        "checkpoint_blobs",
        sa.Column("thread_id", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("checkpoint_ns", sa.TEXT(), server_default=sa.text("''::text"), autoincrement=False, nullable=False),
        sa.Column("channel", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("version", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("type", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("blob", postgresql.BYTEA(), autoincrement=False, nullable=True),
        sa.PrimaryKeyConstraint(
            "thread_id", "checkpoint_ns", "channel", "version", name=op.f("checkpoint_blobs_pkey")
        ),
    )
    op.create_index(op.f("checkpoint_blobs_thread_id_idx"), "checkpoint_blobs", ["thread_id"], unique=False)
    op.create_table(
        "checkpoint_writes",
        sa.Column("thread_id", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("checkpoint_ns", sa.TEXT(), server_default=sa.text("''::text"), autoincrement=False, nullable=False),
        sa.Column("checkpoint_id", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("task_id", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("idx", sa.INTEGER(), autoincrement=False, nullable=False),
        sa.Column("channel", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("type", sa.TEXT(), autoincrement=False, nullable=True),
        sa.Column("blob", postgresql.BYTEA(), autoincrement=False, nullable=False),
        sa.Column("task_path", sa.TEXT(), server_default=sa.text("''::text"), autoincrement=False, nullable=False),
        sa.PrimaryKeyConstraint(
            "thread_id", "checkpoint_ns", "checkpoint_id", "task_id", "idx", name=op.f("checkpoint_writes_pkey")
        ),
    )
    op.create_index(op.f("checkpoint_writes_thread_id_idx"), "checkpoint_writes", ["thread_id"], unique=False)
    safe_drop_index("idx_rss_impacts_priority", "rss_faq_impacts")
    safe_drop_index("idx_rss_impacts_faq", "rss_faq_impacts")
    safe_drop_index("idx_rss_impacts_date", "rss_faq_impacts")
    safe_drop_table("rss_faq_impacts")
    safe_drop_index("idx_generated_faqs_quality", "generated_faqs")
    safe_drop_index("idx_generated_faqs_performance", "generated_faqs")
    safe_drop_index("idx_generated_faqs_approval", "generated_faqs")
    safe_drop_table("generated_faqs")
    safe_drop_table("export_audit_logs")
    safe_drop_index("idx_system_improvements_type", "system_improvements")
    safe_drop_index("idx_system_improvements_status", "system_improvements")
    safe_drop_index("idx_system_improvements_priority", "system_improvements")
    safe_drop_index("idx_system_improvements_pattern", "system_improvements")
    safe_drop_table("system_improvements")
    safe_drop_table("query_history")
    safe_drop_table("knowledge_base_searches")
    safe_drop_table("faq_interactions")
    safe_drop_index("ix_faq_generation_jobs_celery_task_id", "faq_generation_jobs")
    safe_drop_index("idx_faq_jobs_type", "faq_generation_jobs")
    safe_drop_index("idx_faq_jobs_status", "faq_generation_jobs")
    safe_drop_index("idx_faq_jobs_celery", "faq_generation_jobs")
    safe_drop_table("faq_generation_jobs")
    safe_drop_index("idx_faq_candidates_status", "faq_candidates")
    safe_drop_index("idx_faq_candidates_roi", "faq_candidates")
    safe_drop_index("idx_faq_candidates_priority", "faq_candidates")
    safe_drop_table("faq_candidates")
    safe_drop_table("export_tax_calculations")
    safe_drop_table("export_document_analysis")
    safe_drop_table("electronic_invoices")
    safe_drop_table("data_export_requests")
    safe_drop_index("ix_query_clusters_normalized_form", "query_clusters")
    safe_drop_index("ix_query_clusters_last_seen", "query_clusters")
    safe_drop_index("ix_query_clusters_first_seen", "query_clusters")
    safe_drop_index("ix_query_clusters_canonical_query", "query_clusters")
    safe_drop_index("idx_query_clusters_quality", "query_clusters")
    safe_drop_index("idx_query_clusters_priority", "query_clusters")
    safe_drop_index("idx_query_clusters_cost", "query_clusters")
    safe_drop_table("query_clusters")
    safe_drop_index("idx_quality_metrics_period", "quality_metrics")
    safe_drop_index("idx_quality_metrics_name_date", "quality_metrics")
    safe_drop_index("idx_quality_metrics_category", "quality_metrics")
    safe_drop_table("quality_metrics")
    safe_drop_index("idx_prompt_templates_usage", "prompt_templates")
    safe_drop_index("idx_prompt_templates_quality", "prompt_templates")
    safe_drop_index("idx_prompt_templates_category", "prompt_templates")
    safe_drop_index("idx_prompt_templates_active", "prompt_templates")
    safe_drop_table("prompt_templates")
    safe_drop_index("idx_failure_patterns_type", "failure_patterns")
    safe_drop_index("idx_failure_patterns_resolved", "failure_patterns")
    safe_drop_index("idx_failure_patterns_impact", "failure_patterns")
    safe_drop_index("idx_failure_patterns_frequency", "failure_patterns")
    safe_drop_table("failure_patterns")
    safe_drop_index("idx_expert_validations_target", "expert_validations")
    safe_drop_index("idx_expert_validations_status", "expert_validations")
    safe_drop_index("idx_expert_validations_query", "expert_validations")
    safe_drop_index("idx_expert_validations_complexity", "expert_validations")
    safe_drop_table("expert_validations")
    # ### end Alembic commands ###
