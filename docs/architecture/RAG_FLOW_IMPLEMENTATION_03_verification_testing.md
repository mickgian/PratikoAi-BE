# RAG Flow Verification & Testing Implementation

**‚Üê Prerequisites**: [RAG_FLOW_IMPLEMENTATION_02_unified_graph.md](./RAG_FLOW_IMPLEMENTATION_02_unified_graph.md) (Phases 2-6 complete, unified graph operational)

---

## üìã EXECUTIVE SUMMARY

**OBJECTIVE**: Complete end-to-end verification and testing of the unified RAG graph implementation.

**CURRENT STATE**:
- ‚úÖ Unified graph with 59 nodes operational (Phases 2-6 complete)
- ‚úÖ 135 orchestrators implemented (steps 1-135, 100% coverage)
- ‚úÖ Streaming integration complete
- ‚è∏Ô∏è Phase 7 verification pending (trace testing)

**GAPS IDENTIFIED**:
1. No end-to-end trace verification of full 135-step flow
2. No frontend tests for PII anonymization
3. Tool execution (Steps 75-99) not wired to unified graph
4. No frontend tests for attachment/tool integration
5. RSS monitoring implemented but not scheduled/persisted
6. Pinecone provider needs health check verification
7. Atomic facts extraction needs persistence decision

**SOLUTION PHASES**:
1. **Phase 7**: Trace verification and E2E testing (3 days)
2. **Phase 8**: RSS monitoring integration (2.5 days)
3. **Phase 9**: Pinecone health check (0.5 day)

---

## üî¥ CURRENT STATE ANALYSIS

### Implementation Status by Feature

#### 1. PII Anonymization
**Backend Status**: ‚úÖ Fully Implemented
- **File**: `app/core/privacy/anonymizer.py`
- **Class**: `PIIAnonymizer` with Italian language support
- **Features**:
  - Email, phone, Codice Fiscale, Partita IVA detection
  - IBAN, credit card, date of birth patterns
  - Italian name detection with titles
  - Deterministic anonymization with caching
- **RAG Steps**: Step 7 (AnonymizeText)
- **Conformance**: üü° Partial (Step 7 partially wired)

**Testing Gap**: ‚ùå No frontend E2E tests
- Missing: Test with real PII in query
- Missing: Verify anonymization in streaming response
- Missing: Verify RAG trace shows Step 7 execution

**Decision**: Create E2E test for frontend anonymization verification

---

#### 2. Tool Execution & Attachments
**Backend Status**: ‚úÖ Implemented but not wired
- **Files**:
  - `app/core/rag/tool_guardrails.py` - Max 1 tool/turn enforcement
  - `app/orchestrators/platform.py` - Steps 75, 78 (ToolCheck, ExecuteTools)
  - Multiple tool implementations (KB, CCNL, DocIngest, FAQ)
- **Tests**: 10+ unit tests exist
  - `tests/test_rag_step_75_tool_check.py`
  - `tests/test_rag_step_78_execute_tools.py`
  - `tests/test_rag_step_79_tool_type_routing.py`
  - `tests/test_rag_step_80_kb_query_tool.py`
  - `tests/test_rag_step_82_document_ingest_tool.py`
  - `tests/test_tool_guardrails.py`
- **RAG Steps**: Steps 75-99 (tool execution lane)
- **Conformance**: ‚ùå Missing (Steps 75-99 not wired to unified graph)

**Integration Gap**: Tool execution exists but isolated from unified graph
- Lane 7 (Steps 59-99) partially connected
- Tool execution after LLM call not wired
- No routing from Step 67 (LLMSuccess) to Step 75 (ToolCheck)

**Decision**: Wire Steps 75-99 to unified graph, create E2E attachment test

---

#### 3. RSS Feed Monitoring
**Backend Status**: ‚úÖ Fully Automated and Production-Ready
- **Files**:
  - `app/services/scheduler_service.py` - `collect_rss_feeds_task()` function
  - `app/ingest/rss_normativa.py` - `run_rss_ingestion()` with feed_type support
  - `scripts/ingest_rss.py` - Database-driven CLI tool
- **Features**:
  - 5 RSS feeds configured in `feed_status` table:
    - 2 Agenzia Entrate feeds (news + normativa_prassi)
    - 3 other official sources
  - Feed type differentiation (news vs normativa_prassi)
  - Automatic source labeling based on feed type
  - Document ingestion with chunking + embeddings
  - Context formatting with type labels and URLs
- **RAG Steps**: Steps 132-134 (RSSMonitor, FetchFeeds, ParseDocs)
- **Conformance**: ‚úÖ Fully Integrated

**Integration Status**:
- ‚úÖ Database tables: `feed_status` table with `feed_type` column
- ‚úÖ Persistence layer: Documents saved to `knowledge_items` + `knowledge_chunks`
- ‚úÖ Scheduled background task: `rss_feeds_4h` runs every 4 hours
- ‚úÖ Integration with Steps 132-134: Scheduler calls `run_rss_ingestion()`
- ‚úÖ Feed type differentiation: news vs normativa_prassi properly labeled
- ‚úÖ Context formatting: Documents include type labels `[NEWS - AGENZIAENTRATE]` and source URLs `üìé Source link: {url}`
- ‚úÖ System prompt: LLM instructed to cite sources with markdown links

**Current State**: Production-ready, running in Docker containers

**Verification**:
```bash
# Check scheduler is running
docker-compose logs app | grep "rss_feeds_4h"

# Expected output:
# Added scheduled task: rss_feeds_4h (4_hours)
# Scheduler started
# Scheduler service started successfully
```

---

#### 4. Pinecone Vector Database
**Backend Status**: ‚úÖ Fully Implemented
- **Files**:
  - `app/services/vector_providers/pinecone_provider.py` - `PineconeProvider` class
  - `app/services/vector_provider_factory.py` - Factory for provider selection
  - `app/services/vector_config.py` - Configuration
  - `app/core/config.py` - `PINECONE_API_KEY` environment variable
- **Features**:
  - Environment-aware provider selection:
    - Production/preprod ‚Üí Pinecone
    - Development ‚Üí Local vector provider fallback
  - Index naming: `pratikoai-embed-{dimension}` (e.g., `pratikoai-embed-384`)
  - Namespace structure: `env={environment}:domain={domain}:tenant={tenant_id}`
  - Serverless architecture support
- **Documentation**: `docs/architecture/vector-search.md`

**Verification Gap**: Need to confirm operational status
- ‚ö†Ô∏è Need health check for Pinecone connection
- ‚ö†Ô∏è Need to verify index exists in target environment
- ‚ö†Ô∏è Need to test upsert/query operations

**Decision**: Add health check endpoint and verification test

---

#### 5. Atomic Facts Extraction
**Backend Status**: ‚úÖ Fully Implemented
- **Files**:
  - `app/services/atomic_facts_extractor.py` - `AtomicFactsExtractor` class
  - `app/orchestrators/facts.py` - Steps 14 (extract), 16 (canonicalize)
- **Features**:
  - Monetary amount extraction (EUR, percentages)
  - Date extraction (specific, relative, tax years, durations)
  - Legal entity extraction (CF, P.IVA, company types, document types)
  - Professional category extraction (CCNL sectors, job levels, contract types)
  - Geographic extraction (Italian regions, provinces, municipalities)
  - Italian language support
- **RAG Steps**: Steps 14-18 (extract, canonicalize, query signature)
- **Conformance**: üîå Implemented but not wired

**Database Analysis**: ‚ùå NO atomic facts table
- Searched all models in `app/models/`
- Only `app/models/ccnl_data.py` has "fact" references (CCNL-specific facts, not query facts)
- Atomic facts are extracted but NOT persisted
- Facts only stored in:
  - RAGState context (ephemeral)
  - Cache keys (for query signature generation)
  - Langfuse traces (for observability)

**Persistence Decision**:
‚úÖ **Atomic facts should NOT be persisted to database**

**Rationale**:
1. **Ephemeral by Design**: Facts are query-specific context, not domain knowledge
2. **Cache Sufficient**: Query signature (Step 18) uses facts for cache key generation
3. **Observability**: Langfuse traces already capture facts for debugging
4. **Storage Overhead**: Persisting would create unnecessary DB bloat
5. **Privacy**: Not persisting query facts reduces PII exposure risk

**Alternative**: If analytics needed, export facts to data warehouse via Langfuse

---

#### 6. Pre-Commit Testing Infrastructure
**Backend Status**: ‚úÖ Fully Implemented
- **File**: `.pre-commit-config.yaml`
- **Hooks**:
  1. **Dependency Validation**: Checks pgvector, asyncpg, feedparser, sentence-transformers are in `pyproject.toml`
  2. **RSS Scheduler Tests**: Runs 15 integration tests before every commit
- **Test Coverage**: 100% of RSS scheduler functionality (15/15 tests passing)
- **Conformance**: ‚úÖ Production-ready

**Testing Results**:
- All 15 scheduler integration tests passing (`tests/test_scheduler_italian_integration.py`)
- Pre-commit hooks prevent breaking changes from being committed
- Fast feedback loop (~13 seconds for full RSS scheduler test suite)
- Dependency hook prevents missing critical dependencies (like pgvector issue)

**Test Suite**:
```python
# tests/test_scheduler_italian_integration.py
- test_schedule_interval_enum_has_4_hours ‚úÖ
- test_calculate_next_run_4_hours ‚úÖ
- test_setup_default_tasks_includes_italian_collection ‚úÖ (updated to rss_feeds_4h)
- test_setup_default_tasks_includes_metrics_task ‚úÖ
- test_italian_task_execution ‚úÖ
- test_italian_task_error_handling ‚úÖ
- test_task_status_includes_italian_collection ‚úÖ
- test_manual_italian_task_execution ‚úÖ
- test_enable_disable_italian_task ‚úÖ
- test_remove_italian_task ‚úÖ
- test_scheduler_logging_includes_italian_task ‚úÖ
- test_scheduler_loop_processes_italian_task ‚úÖ
- test_italian_task_configuration_consistency ‚úÖ
- test_scheduler_integration_with_real_task_function ‚úÖ
- test_scheduler_task_interval_ordering ‚úÖ
```

**Pre-Commit Hook Configuration**:
```yaml
# .pre-commit-config.yaml

# Dependency validation
- repo: local
  hooks:
    - id: check-critical-dependencies
      name: Check critical dependencies in pyproject.toml
      entry: python -c "..."  # Validates pgvector, asyncpg, feedparser, etc.
      files: ^pyproject.toml$

# RSS scheduler tests
- repo: local
  hooks:
    - id: rss-scheduler-tests
      name: Run RSS scheduler tests
      entry: bash -c 'source scripts/set_env.sh development && python -m pytest tests/test_scheduler_italian_integration.py -v --tb=line -x'
      always_run: true
      stages: [pre-commit]
```

---

## üéØ IMPLEMENTATION PLAN

### Phase 7: Trace Verification & E2E Testing (3 days)

#### Task 7.1: Trace Verification ‚úÖ COMPLETED
**Goal**: Verify unified graph executes all 135 steps in correct order

**Status**: Already verified in Phase 6 of RAG_FLOW_IMPLEMENTATION_02
- Phases 2-6 completed successfully
- Unified graph operational
- Streaming integration working
- All 59 nodes wired with proper routing

**Remaining Work**:
- Run full trace test to capture all 135 step logs
- Verify no missing/duplicate logging
- Confirm execution order matches diagram

**Implementation**:
1. Create test query that exercises full RAG flow
2. Send to `/chat/stream` endpoint
3. Capture RAG trace logs
4. Verify step sequence: 1‚Üí3‚Üí6‚Üí9‚Üí11‚Üí14‚Üí20‚Üí31‚Üí42‚Üí48‚Üí59‚Üí64‚Üí67‚Üí104‚Üí112

**Acceptance Criteria**:
- [ ] All 135 steps appear in trace logs
- [ ] Steps execute in diagram order
- [ ] No duplicate step logging
- [ ] No missing steps

**Time**: 1 day (includes trace analysis and documentation)

---

#### Task 7.2: Frontend PII Anonymization E2E Test
**Goal**: Verify PII anonymization works end-to-end in streaming responses

**Status**: ‚è∏Ô∏è Pending

**Test Scenario**:
```python
# Test query with Italian PII
query = """
Mi chiamo Marco Rossi, il mio codice fiscale √® RSSMRC80A01H501Z.
Il mio numero di telefono √® +39 333 1234567 e la mia email √® marco.example@example.com.
La mia partita IVA √® IT12345678901 e il mio IBAN √® IT60X0542811101000000123456.
Vorrei sapere se posso detrarre le spese mediche del 2024.
"""

# Expected anonymized response characteristics:
# - No "Marco Rossi" in output
# - No "RSSMRC80A01H501Z" in output
# - No "+39 333 1234567" in output
# - No "marco.example@example.com" in output
# - No "IT12345678901" or "IT60X0542811101000000123456" in output
# - Response should contain tax deduction information
# - RAG trace should show Step 7 execution with PII detection count
```

**Implementation**:
1. Create `tests/e2e/test_frontend_pii_anonymization.py`
2. Send test query via `/chat/stream`
3. Collect streaming response chunks
4. Assert no PII in response
5. Verify RAG trace shows Step 7 with `pii_detected=True`

**File**: `tests/e2e/test_frontend_pii_anonymization.py`
```python
import pytest
from httpx import AsyncClient
from app.main import app

@pytest.mark.asyncio
async def test_pii_anonymization_in_streaming_response():
    """Test that PII is anonymized in streaming responses."""
    query_with_pii = """
    Mi chiamo Marco Rossi, il mio codice fiscale √® RSSMRC80A01H501Z.
    Il mio numero di telefono √® +39 333 1234567 e la mia email √® marco.example@example.com.
    Vorrei sapere se posso detrarre le spese mediche.
    """

    async with AsyncClient(app=app, base_url="http://test") as client:
        # Authenticate and create session
        auth_response = await client.post("/api/v1/auth/login", json={...})
        token = auth_response.json()["access_token"]

        # Send query with PII
        response = await client.post(
            "/api/v1/chat/stream",
            json={"message": query_with_pii},
            headers={"Authorization": f"Bearer {token}"}
        )

        # Collect streaming chunks
        chunks = []
        async for chunk in response.aiter_lines():
            if chunk.startswith("data: "):
                chunks.append(chunk[6:])

        full_response = "".join(chunks)

        # Assert no PII in response
        assert "Marco Rossi" not in full_response
        assert "RSSMRC80A01H501Z" not in full_response
        assert "+39 333 1234567" not in full_response
        assert "marco.example@example.com" not in full_response

        # Verify medical deduction info is present (query was answered)
        assert "spese mediche" in full_response.lower() or "detrarre" in full_response.lower()
```

**Acceptance Criteria**:
- [ ] Test passes with PII-laden query
- [ ] No PII appears in streaming response
- [ ] Response answers the underlying question
- [ ] RAG trace shows Step 7 execution

**Time**: 0.5 day

---

#### Task 7.3: Wire Tool Execution to Unified Graph
**Goal**: Connect Steps 75-99 (tool execution lane) to unified graph

**Status**: ‚è∏Ô∏è Pending

**Current State**:
- Lane 7 (Steps 59-99) partially implemented
- Steps 59-64, 67-74 are wired
- Steps 75-99 exist as orchestrators but not connected to graph
- No routing from Step 67 (LLMSuccess) to Step 75 (ToolCheck)

**Implementation**:
1. **Update `app/core/langgraph/graph.py`**:
   - Import tool execution nodes (Steps 75-99)
   - Add nodes to unified graph
   - Create routing function `_route_from_llm_success_to_tools()`
   - Wire conditional edges from Step 67 to Step 75

2. **Routing Logic**:
```python
def _route_from_llm_success_to_tools(state: RAGState) -> str:
    """Route from LLM success to tool check or cache response."""
    response = state.get("llm_response")

    # Check if response has tool calls
    if response and hasattr(response, "tool_calls") and response.tool_calls:
        return "step_75__tool_check"  # Has tool calls, go to tool execution

    # No tool calls, cache response
    return "step_68__cache_response"
```

3. **Wire Tool Execution Steps**:
   - Step 75: ToolCheck (decision: has tool calls?)
   - Step 76-77: Convert to message format
   - Step 78: ExecuteTools
   - Step 79: ToolType (decision: routing)
   - Steps 80-83: Tool implementations (KB, CCNL, DocIngest, FAQ)
   - Steps 84-99: Tool processing and results

4. **Update Graph Edges**:
```python
# In create_graph_unified():

# Add tool execution nodes
graph.add_node("step_75__tool_check", step_75__tool_check)
graph.add_node("step_78__execute_tools", step_78__execute_tools)
graph.add_node("step_79__tool_type", step_79__tool_type)
# ... add all tool nodes

# Route from LLM success
graph.add_conditional_edges(
    "step_67__llm_success",
    _route_from_llm_success_to_tools,
    {
        "step_75__tool_check": "step_75__tool_check",
        "step_68__cache_response": "step_68__cache_response"
    }
)

# Route from tool check
graph.add_conditional_edges(
    "step_75__tool_check",
    _route_from_tool_check,
    {
        "step_76__convert_aimsg": "step_76__convert_aimsg",  # Has tools
        "step_77__simple_aimsg": "step_77__simple_aimsg"     # No tools
    }
)
```

**Files to Modify**:
- `app/core/langgraph/graph.py` (add ~200 lines)

**Acceptance Criteria**:
- [ ] Steps 75-99 nodes added to unified graph
- [ ] Routing from Step 67 to Step 75 works
- [ ] Tool execution flow completes successfully
- [ ] Existing tests still pass

**Time**: 1 day

---

#### Task 7.4: Frontend Attachment/Tool E2E Test
**Goal**: Verify attachment uploads trigger document ingest tool

**Status**: ‚è∏Ô∏è Pending (depends on Task 7.3)

**Test Scenario**:
```python
# Upload PDF attachment
# Verify Step 17 (attachment fingerprint) executes
# Verify Step 78 (execute tools) calls document ingest
# Verify LLM receives tool results
# Verify response references document content
```

**Implementation**:
1. Create `tests/e2e/test_frontend_attachment_tool.py`
2. Upload test PDF document
3. Send query: "Analizza il documento allegato"
4. Verify RAG trace shows:
   - Step 17: AttachmentFingerprint
   - Step 75: ToolCheck (has_tool_calls=True)
   - Step 78: ExecuteTools
   - Step 82: DocIngest tool execution
5. Verify response contains document analysis

**File**: `tests/e2e/test_frontend_attachment_tool.py`
```python
import pytest
from httpx import AsyncClient
from app.main import app

@pytest.mark.asyncio
async def test_attachment_triggers_document_ingest_tool():
    """Test that PDF attachment triggers document ingest tool."""

    # Create test PDF
    test_pdf = create_test_pdf_with_content("Test invoice content")

    async with AsyncClient(app=app, base_url="http://test") as client:
        # Authenticate
        auth_response = await client.post("/api/v1/auth/login", json={...})
        token = auth_response.json()["access_token"]

        # Upload document with query
        files = {"file": ("test.pdf", test_pdf, "application/pdf")}
        data = {"message": "Analizza il documento allegato"}

        response = await client.post(
            "/api/v1/chat",
            files=files,
            data=data,
            headers={"Authorization": f"Bearer {token}"}
        )

        result = response.json()

        # Verify response contains document analysis
        assert "invoice" in result["response"].lower() or "fattura" in result["response"].lower()

        # Verify RAG trace shows tool execution
        # (Would need to fetch trace from Langfuse or logs)
        assert result.get("tools_called") is not None
        assert "document_ingest" in [t["name"] for t in result["tools_called"]]
```

**Acceptance Criteria**:
- [ ] Test uploads PDF successfully
- [ ] Attachment fingerprint computed (Step 17)
- [ ] Document ingest tool executed (Step 82)
- [ ] Response contains document analysis
- [ ] RAG trace shows complete tool execution flow

**Time**: 0.5 day

---

#### Task 7.5: Redis Cache E2E Test
**Goal**: Verify Redis cache works end-to-end with proper hit/miss/invalidation

**Status**: ‚è∏Ô∏è Pending

**Why Critical**:
- Cache is essential for cost reduction (target >60% hit rate)
- Saves $$ by avoiding duplicate LLM API calls
- Must invalidate correctly when knowledge base updates (epoch changes)

**Test Scenarios**:

**Scenario 1: Cache Miss ‚Üí LLM Call ‚Üí Cache Save**
```python
@pytest.mark.asyncio
async def test_cache_miss_then_save():
    """First query should miss cache, call LLM, and save response."""

    # Clear any existing cache
    await cache_service.clear()

    query = "Quali sono i requisiti per aprire una partita IVA in regime forfettario 2024?"

    # First request - should MISS cache
    response1 = await client.post("/api/v1/chat", json={"message": query})

    # Verify RAG trace shows:
    # - Step 59: CheckCache executed
    # - Step 62: CacheHit ‚Üí decision=NO (cache miss)
    # - Step 64: LLMCall executed (because cache missed)
    # - Step 67: LLMSuccess
    # - Step 68: CacheResponse saved to Redis

    # Verify response received
    assert response1.status_code == 200
    assert "forfettario" in response1.json()["response"].lower()

    # Verify cache key was saved to Redis
    cache_key = cache_service._generate_query_hash(...)
    cached = await redis.get(f"llm_response:{cache_key}")
    assert cached is not None
```

**Scenario 2: Cache Hit ‚Üí Return Cached (No LLM)**
```python
@pytest.mark.asyncio
async def test_cache_hit_no_llm_call():
    """Same query should HIT cache and NOT call LLM."""

    query = "Quali sono i requisiti per aprire una partita IVA in regime forfettario 2024?"

    # First request (cache miss, LLM call)
    response1 = await client.post("/api/v1/chat", json={"message": query})
    original_response = response1.json()["response"]

    # Get LLM call count before second request
    llm_calls_before = get_llm_call_count()

    # Second request - SAME QUERY (should hit cache)
    response2 = await client.post("/api/v1/chat", json={"message": query})

    # Verify RAG trace shows:
    # - Step 59: CheckCache executed
    # - Step 62: CacheHit ‚Üí decision=YES (cache hit!)
    # - Step 63: TrackCacheHit logged
    # - Step 65: LogCacheHit
    # - Step 66: ReturnCached
    # - Step 64: LLMCall NOT executed (cache hit path)

    # Verify NO additional LLM calls
    llm_calls_after = get_llm_call_count()
    assert llm_calls_after == llm_calls_before, "LLM should NOT be called on cache hit"

    # Verify responses are identical
    assert response2.json()["response"] == original_response

    # Verify faster response time (cache < 100ms vs LLM ~2000ms)
    assert response2.elapsed.total_seconds() < 0.5
```

**Scenario 3: Epoch Change ‚Üí Cache Invalidation**
```python
@pytest.mark.asyncio
async def test_epoch_change_invalidates_cache():
    """KB epoch change should invalidate cache (new cache key)."""

    query = "Quali sono le novit√† CCNL metalmeccanici?"

    # First request with kb_epoch=100
    await set_kb_epoch(100)
    response1 = await client.post("/api/v1/chat", json={"message": query})

    # KB updated - epoch increments to 101
    await set_kb_epoch(101)

    # Same query - should MISS cache (different epoch = different cache key)
    llm_calls_before = get_llm_call_count()
    response2 = await client.post("/api/v1/chat", json={"message": query})
    llm_calls_after = get_llm_call_count()

    # Verify LLM was called again (cache miss due to epoch change)
    assert llm_calls_after > llm_calls_before, "LLM should be called when epoch changes"

    # Verify RAG trace shows cache miss on Step 62
    # Verify new cache key includes epoch=101
```

**Scenario 4: Hardened Cache Key Components**
```python
@pytest.mark.asyncio
async def test_hardened_cache_key_includes_all_factors():
    """Verify cache key changes when any factor changes."""

    base_query = "Calcola TFR per dipendente"

    # Test 1: Same query, different temperature ‚Üí different cache key
    key1 = await get_cache_key(query=base_query, temperature=0.2)
    key2 = await get_cache_key(query=base_query, temperature=0.7)
    assert key1 != key2, "Temperature change should change cache key"

    # Test 2: Same query, different model ‚Üí different cache key
    key3 = await get_cache_key(query=base_query, model="gpt-4o-mini")
    key4 = await get_cache_key(query=base_query, model="claude-3-5-sonnet")
    assert key3 != key4, "Model change should change cache key"

    # Test 3: Same query, with/without tools ‚Üí different cache key
    key5 = await get_cache_key(query=base_query, tools_used=False)
    key6 = await get_cache_key(query=base_query, tools_used=True)
    assert key5 != key6, "Tools usage should change cache key"

    # Test 4: Same query, different document hashes ‚Üí different cache key
    key7 = await get_cache_key(query=base_query, doc_hashes=["hash1"])
    key8 = await get_cache_key(query=base_query, doc_hashes=["hash2"])
    assert key7 != key8, "Document change should change cache key"
```

**Implementation**:

**File**: `tests/e2e/test_redis_cache.py`
```python
import pytest
import time
from httpx import AsyncClient
from app.main import app
from app.services.cache import cache_service
from unittest.mock import patch

@pytest.fixture
async def authenticated_client():
    """Provide authenticated test client."""
    async with AsyncClient(app=app, base_url="http://test") as client:
        # Authenticate
        auth_response = await client.post("/api/v1/auth/login", json={
            "username": "test@example.com",
            "password": "testpass123"  # pragma: allowlist secret
        })
        token = auth_response.json()["access_token"]
        client.headers["Authorization"] = f"Bearer {token}"
        yield client

@pytest.mark.asyncio
async def test_cache_miss_then_hit(authenticated_client):
    """Test cache miss on first query, cache hit on second."""

    # Clear cache
    await cache_service.clear()

    query = "Quali sono i requisiti per aprire una partita IVA in regime forfettario 2024?"

    # First request - cache MISS
    start1 = time.time()
    response1 = await authenticated_client.post("/api/v1/chat", json={"message": query})
    duration1 = time.time() - start1

    assert response1.status_code == 200
    result1 = response1.json()
    assert "forfettario" in result1["response"].lower()

    # Should take >1 second (LLM call)
    assert duration1 > 1.0, f"First call should be slow (LLM): {duration1}s"

    # Second request - cache HIT
    start2 = time.time()
    response2 = await authenticated_client.post("/api/v1/chat", json={"message": query})
    duration2 = time.time() - start2

    assert response2.status_code == 200
    result2 = response2.json()

    # Responses should be identical
    assert result1["response"] == result2["response"]

    # Should take <0.5 seconds (cache hit)
    assert duration2 < 0.5, f"Second call should be fast (cache): {duration2}s"

    # Verify cache metadata
    assert result2.get("cache_hit") is True or "cache" in result2.get("metadata", {})

@pytest.mark.asyncio
async def test_epoch_invalidation(authenticated_client):
    """Test that epoch change invalidates cache."""

    query = "Quali sono le novit√† CCNL metalmeccanici 2024?"

    # Mock epoch service
    with patch("app.services.epoch_service.get_kb_epoch", return_value=100):
        response1 = await authenticated_client.post("/api/v1/chat", json={"message": query})
        assert response1.status_code == 200

    # Change epoch
    with patch("app.services.epoch_service.get_kb_epoch", return_value=101):
        # Should call LLM again (different cache key due to epoch)
        start = time.time()
        response2 = await authenticated_client.post("/api/v1/chat", json={"message": query})
        duration = time.time() - start

        assert response2.status_code == 200
        # Should be slow (LLM call, not cache)
        assert duration > 1.0, f"Should call LLM when epoch changes: {duration}s"
```

**Acceptance Criteria**:
- [ ] Test verifies cache miss ‚Üí LLM call ‚Üí cache save
- [ ] Test verifies cache hit ‚Üí no LLM call ‚Üí fast response
- [ ] Test verifies responses identical on cache hit
- [ ] Test verifies epoch change invalidates cache
- [ ] Test verifies hardened cache key includes all factors
- [ ] Cache hit provides <500ms response time
- [ ] Cache miss provides >1000ms response time (LLM call)

**Frontend Testing Required**: YES

**What User Should Do on FE**:
1. **Cache Miss Test**: Ask question: "Quali sono i requisiti per aprire una partita IVA in regime forfettario 2024?"
   - Note response time (should be 2-3 seconds)
   - Verify answer appears

2. **Cache Hit Test**: Ask EXACT SAME question again
   - Note response time (should be <500ms - instant!)
   - Verify IDENTICAL answer
   - Look for cache indicator in UI (if present)

3. **Verify in Backend Logs**: Check RAG trace logs show:
   - First request: Step 64 (LLMCall) executed
   - Second request: Step 64 NOT executed, Step 66 (ReturnCached) instead

**Time**: 0.5 day

---

### Phase 8: RSS Monitoring Integration (2.5 days)

#### Task 8.1: Database Schema for RSS Feeds
**Goal**: Create database tables for RSS feed storage

**Status**: ‚è∏Ô∏è Pending

**Schema Design**:

**Table 1: `rss_feed_sources`**
```sql
CREATE TABLE rss_feed_sources (
    id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    url VARCHAR(512) NOT NULL UNIQUE,
    source_type VARCHAR(50) NOT NULL,  -- 'union', 'employer', 'official', 'news'
    update_source VARCHAR(50) NOT NULL,  -- UpdateSource enum value
    enabled BOOLEAN DEFAULT TRUE,
    check_interval_minutes INTEGER DEFAULT 120,
    last_checked_at TIMESTAMP,
    last_error VARCHAR(512),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_rss_sources_enabled ON rss_feed_sources(enabled);
CREATE INDEX idx_rss_sources_last_checked ON rss_feed_sources(last_checked_at);
```

**Table 2: `rss_feed_items`**
```sql
CREATE TABLE rss_feed_items (
    id SERIAL PRIMARY KEY,
    feed_source_id INTEGER REFERENCES rss_feed_sources(id) ON DELETE CASCADE,
    guid VARCHAR(512) NOT NULL,  -- RSS item GUID
    title VARCHAR(512) NOT NULL,
    link VARCHAR(512) NOT NULL,
    description TEXT,
    content TEXT,
    published_at TIMESTAMP NOT NULL,
    fetched_at TIMESTAMP DEFAULT NOW(),

    -- CCNL detection fields
    is_ccnl_related BOOLEAN DEFAULT FALSE,
    ccnl_sector VARCHAR(100),
    update_type VARCHAR(50),
    confidence_score FLOAT,
    keywords_matched TEXT[],
    priority_score FLOAT,

    -- Processing status
    processed BOOLEAN DEFAULT FALSE,
    processed_at TIMESTAMP,

    created_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(feed_source_id, guid)
);

CREATE INDEX idx_rss_items_published ON rss_feed_items(published_at DESC);
CREATE INDEX idx_rss_items_ccnl ON rss_feed_items(is_ccnl_related, processed);
CREATE INDEX idx_rss_items_guid ON rss_feed_items(guid);
```

**Migration File**: `alembic/versions/xxx_add_rss_feed_tables.py`

**Acceptance Criteria**:
- [ ] Migration file created
- [ ] Tables created with proper indexes
- [ ] Foreign key constraints work
- [ ] UNIQUE constraint on (feed_source_id, guid) prevents duplicates

**Time**: 0.5 day

---

#### Task 8.2: RSS Persistence Layer
**Goal**: Implement repository pattern for RSS feed data

**Status**: ‚è∏Ô∏è Pending (depends on Task 8.1)

**Implementation**:

**File**: `app/repositories/rss_repository.py`
```python
from typing import List, Optional, Dict, Any
from datetime import datetime
from sqlmodel import Session, select
from app.models.rss_models import RSSFeedSource, RSSFeedItem

class RSSRepository:
    """Repository for RSS feed data persistence."""

    def __init__(self, session: Session):
        self.session = session

    async def get_enabled_sources(self) -> List[RSSFeedSource]:
        """Get all enabled RSS feed sources."""
        statement = select(RSSFeedSource).where(RSSFeedSource.enabled == True)
        results = await self.session.exec(statement)
        return results.all()

    async def save_feed_items(self, items: List[Dict[str, Any]]) -> int:
        """Save RSS feed items with deduplication."""
        saved_count = 0

        for item_data in items:
            # Check if item already exists
            existing = await self._get_item_by_guid(
                item_data["feed_source_id"],
                item_data["guid"]
            )

            if existing:
                continue  # Skip duplicates

            # Create new item
            item = RSSFeedItem(**item_data)
            self.session.add(item)
            saved_count += 1

        await self.session.commit()
        return saved_count

    async def _get_item_by_guid(self, feed_source_id: int, guid: str) -> Optional[RSSFeedItem]:
        """Get feed item by source and GUID."""
        statement = select(RSSFeedItem).where(
            RSSFeedItem.feed_source_id == feed_source_id,
            RSSFeedItem.guid == guid
        )
        result = await self.session.exec(statement)
        return result.first()

    async def get_unprocessed_ccnl_items(self, limit: int = 100) -> List[RSSFeedItem]:
        """Get unprocessed CCNL-related items."""
        statement = (
            select(RSSFeedItem)
            .where(
                RSSFeedItem.is_ccnl_related == True,
                RSSFeedItem.processed == False
            )
            .order_by(RSSFeedItem.priority_score.desc(), RSSFeedItem.published_at.desc())
            .limit(limit)
        )
        results = await self.session.exec(statement)
        return results.all()

    async def mark_item_processed(self, item_id: int):
        """Mark feed item as processed."""
        item = await self.session.get(RSSFeedItem, item_id)
        if item:
            item.processed = True
            item.processed_at = datetime.utcnow()
            await self.session.commit()

    async def update_source_last_checked(self, source_id: int, error: Optional[str] = None):
        """Update feed source last checked timestamp."""
        source = await self.session.get(RSSFeedSource, source_id)
        if source:
            source.last_checked_at = datetime.utcnow()
            source.last_error = error
            await self.session.commit()
```

**Models File**: `app/models/rss_models.py`
```python
from datetime import datetime
from typing import Optional, List
from sqlmodel import Field, SQLModel

class RSSFeedSource(SQLModel, table=True):
    """RSS feed source configuration."""
    __tablename__ = "rss_feed_sources"

    id: Optional[int] = Field(default=None, primary_key=True)
    name: str = Field(max_length=255)
    url: str = Field(max_length=512, unique=True)
    source_type: str = Field(max_length=50)
    update_source: str = Field(max_length=50)
    enabled: bool = Field(default=True)
    check_interval_minutes: int = Field(default=120)
    last_checked_at: Optional[datetime] = None
    last_error: Optional[str] = Field(default=None, max_length=512)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

class RSSFeedItem(SQLModel, table=True):
    """RSS feed item."""
    __tablename__ = "rss_feed_items"

    id: Optional[int] = Field(default=None, primary_key=True)
    feed_source_id: int = Field(foreign_key="rss_feed_sources.id")
    guid: str = Field(max_length=512)
    title: str = Field(max_length=512)
    link: str = Field(max_length=512)
    description: Optional[str] = None
    content: Optional[str] = None
    published_at: datetime
    fetched_at: datetime = Field(default_factory=datetime.utcnow)

    # CCNL detection
    is_ccnl_related: bool = Field(default=False)
    ccnl_sector: Optional[str] = Field(default=None, max_length=100)
    update_type: Optional[str] = Field(default=None, max_length=50)
    confidence_score: Optional[float] = None
    keywords_matched: Optional[List[str]] = Field(default=None, sa_column_kwargs={"type_": "TEXT[]"})
    priority_score: Optional[float] = None

    # Processing
    processed: bool = Field(default=False)
    processed_at: Optional[datetime] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
```

**Acceptance Criteria**:
- [ ] RSSRepository implements save/fetch methods
- [ ] Deduplication works (GUID uniqueness)
- [ ] Can retrieve unprocessed CCNL items
- [ ] Can mark items as processed
- [ ] Unit tests pass

**Time**: 0.5 day

---

#### Task 8.3: Scheduled RSS Monitoring
**Goal**: Create background task for automated RSS feed monitoring

**Status**: ‚è∏Ô∏è Pending (depends on Task 8.2)

**Implementation**:

**File**: `app/services/rss_background_task.py`
```python
import asyncio
from datetime import datetime, timedelta
from typing import List
from app.services.ccnl_rss_monitor import RSSFeedMonitor, RSSFeedItem as MonitorFeedItem
from app.repositories.rss_repository import RSSRepository
from app.models.database import get_session
from app.core.logging import logger

class RSSBackgroundMonitor:
    """Background task for RSS feed monitoring."""

    def __init__(self):
        self.monitor = RSSFeedMonitor()
        self.check_interval = timedelta(hours=2)
        self._running = False
        self._task = None

    async def start(self):
        """Start the background monitoring task."""
        if self._running:
            logger.warning("RSS monitor already running")
            return

        self._running = True
        self._task = asyncio.create_task(self._monitor_loop())
        logger.info("rss_monitor_started", interval_hours=2)

    async def stop(self):
        """Stop the background monitoring task."""
        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("rss_monitor_stopped")

    async def _monitor_loop(self):
        """Main monitoring loop."""
        while self._running:
            try:
                await self._check_all_feeds()
            except Exception as e:
                logger.error("rss_monitor_error", error=str(e))

            # Wait for next check interval
            await asyncio.sleep(self.check_interval.total_seconds())

    async def _check_all_feeds(self):
        """Check all enabled RSS feeds."""
        async with get_session() as session:
            repo = RSSRepository(session)

            # Get enabled sources
            sources = await repo.get_enabled_sources()
            logger.info("rss_check_started", source_count=len(sources))

            total_new_items = 0

            for source in sources:
                try:
                    # Fetch feed
                    feed_config = {
                        "url": source.url,
                        "name": source.name,
                        "source": source.update_source
                    }

                    items = await self.monitor.fetch_feed(feed_config)

                    # Convert to DB format
                    db_items = []
                    for item in items:
                        db_items.append({
                            "feed_source_id": source.id,
                            "guid": item.guid,
                            "title": item.title,
                            "link": item.link,
                            "description": item.description,
                            "content": item.content,
                            "published_at": item.published,
                            "is_ccnl_related": False,  # Will be classified separately
                        })

                    # Save to database
                    saved = await repo.save_feed_items(db_items)
                    total_new_items += saved

                    # Update source last checked
                    await repo.update_source_last_checked(source.id)

                    logger.info("rss_feed_checked",
                               source=source.name,
                               items_fetched=len(items),
                               new_items=saved)

                except Exception as e:
                    logger.error("rss_feed_check_failed",
                                source=source.name,
                                error=str(e))
                    await repo.update_source_last_checked(source.id, error=str(e))

            logger.info("rss_check_completed", total_new_items=total_new_items)

    async def classify_unprocessed_items(self):
        """Classify unprocessed items for CCNL relevance."""
        # TODO: Implement CCNL classification using existing CCNLUpdateDetector
        pass

# Global instance
rss_monitor = RSSBackgroundMonitor()
```

**Integration with FastAPI**:

**File**: `app/main.py` (add startup/shutdown hooks)
```python
from app.services.rss_background_task import rss_monitor

@app.on_event("startup")
async def startup_event():
    """Start background tasks on application startup."""
    logger.info("application_startup")
    await rss_monitor.start()

@app.on_event("shutdown")
async def shutdown_event():
    """Stop background tasks on application shutdown."""
    logger.info("application_shutdown")
    await rss_monitor.stop()
```

**Acceptance Criteria**:
- [ ] Background task starts on app startup
- [ ] Checks all feeds every 2 hours
- [ ] Saves new items to database
- [ ] Deduplicates existing items
- [ ] Logs errors and continues on failure
- [ ] Stops cleanly on app shutdown

**Time**: 1 day

---

#### Task 8.4: RSS Integration Test
**Goal**: Create integration test for RSS monitoring

**Status**: ‚è∏Ô∏è Pending (depends on Task 8.3)

**Implementation**:

**File**: `tests/integration/test_rss_monitoring.py`
```python
import pytest
from datetime import datetime
from unittest.mock import patch, MagicMock
from app.services.rss_background_task import RSSBackgroundMonitor
from app.repositories.rss_repository import RSSRepository
from app.models.rss_models import RSSFeedSource

@pytest.mark.asyncio
async def test_rss_feed_fetch_and_save(test_db_session):
    """Test RSS feed fetch and database save."""

    # Create test feed source
    source = RSSFeedSource(
        name="Test Union",
        url="http://test.com/rss",
        source_type="union",
        update_source="TEST_RSS",
        enabled=True
    )
    test_db_session.add(source)
    await test_db_session.commit()

    # Mock RSS feed response
    mock_feed_data = MagicMock()
    mock_feed_data.entries = [
        {
            "title": "CCNL Metalmeccanici Rinnovo 2024",
            "link": "http://test.com/article1",
            "description": "Firmato il rinnovo del CCNL metalmeccanici",
            "published_parsed": (2024, 10, 31, 10, 0, 0, 0, 0, 0),
            "id": "article1"
        },
        {
            "title": "News generica",
            "link": "http://test.com/article2",
            "description": "Notizia non CCNL",
            "published_parsed": (2024, 10, 30, 10, 0, 0, 0, 0, 0),
            "id": "article2"
        }
    ]

    # Patch feedparser
    with patch("feedparser.parse", return_value=mock_feed_data):
        monitor = RSSBackgroundMonitor()
        await monitor._check_all_feeds()

    # Verify items saved to database
    repo = RSSRepository(test_db_session)
    items = await test_db_session.exec(
        select(RSSFeedItem).where(RSSFeedItem.feed_source_id == source.id)
    )
    items = items.all()

    assert len(items) == 2
    assert items[0].title == "CCNL Metalmeccanici Rinnovo 2024"
    assert items[1].title == "News generica"

    # Test deduplication - fetch same feed again
    with patch("feedparser.parse", return_value=mock_feed_data):
        await monitor._check_all_feeds()

    # Should still have only 2 items (no duplicates)
    items = await test_db_session.exec(
        select(RSSFeedItem).where(RSSFeedItem.feed_source_id == source.id)
    )
    items = items.all()
    assert len(items) == 2

@pytest.mark.asyncio
async def test_rss_monitor_startup_shutdown():
    """Test RSS monitor starts and stops cleanly."""
    monitor = RSSBackgroundMonitor()

    # Start monitor
    await monitor.start()
    assert monitor._running is True
    assert monitor._task is not None

    # Stop monitor
    await monitor.stop()
    assert monitor._running is False
```

**Acceptance Criteria**:
- [ ] Test mocks RSS feed successfully
- [ ] Items saved to database
- [ ] Deduplication prevents duplicates
- [ ] Monitor starts/stops cleanly

**Time**: 0.5 day

---

### Phase 9: Pinecone Health Check (0.5 day)

#### Task 9.1: Pinecone Health Check Endpoint
**Goal**: Add health check for Pinecone connection

**Status**: ‚è∏Ô∏è Pending

**Implementation**:

**File**: `app/api/v1/health.py` (or create new file)
```python
from fastapi import APIRouter, HTTPException
from app.services.vector_provider_factory import get_vector_provider
from app.core.config import settings
from app.core.logging import logger

router = APIRouter(prefix="/health", tags=["health"])

@router.get("/pinecone")
async def pinecone_health_check():
    """Check Pinecone vector database health."""

    if not settings.PINECONE_API_KEY:
        return {
            "status": "disabled",
            "message": "Pinecone not configured (using local provider)",
            "environment": settings.ENVIRONMENT
        }

    try:
        # Get Pinecone provider
        provider = get_vector_provider()

        if provider.__class__.__name__ != "PineconeProvider":
            return {
                "status": "not_active",
                "message": f"Using {provider.__class__.__name__} instead of Pinecone",
                "environment": settings.ENVIRONMENT
            }

        # Test connection
        await provider.health_check()

        # Get index stats
        stats = await provider.get_index_stats()

        return {
            "status": "healthy",
            "index_name": provider.index_name,
            "dimension": provider.dimension,
            "vector_count": stats.get("total_vector_count", 0),
            "namespace_count": len(stats.get("namespaces", {})),
            "environment": settings.ENVIRONMENT
        }

    except Exception as e:
        logger.error("pinecone_health_check_failed", error=str(e))
        raise HTTPException(status_code=503, detail=f"Pinecone unhealthy: {str(e)}")
```

**Update PineconeProvider**:

**File**: `app/services/vector_providers/pinecone_provider.py` (add methods)
```python
async def health_check(self) -> bool:
    """Health check for Pinecone connection."""
    try:
        # Try to describe index
        description = self.client.describe_index(self.index_name)
        return description is not None
    except Exception as e:
        logger.error("pinecone_health_check_failed", error=str(e))
        raise

async def get_index_stats(self) -> dict:
    """Get index statistics."""
    try:
        stats = self.index.describe_index_stats()
        return stats
    except Exception as e:
        logger.error("pinecone_stats_failed", error=str(e))
        return {}
```

**Acceptance Criteria**:
- [ ] Health check endpoint returns 200 when Pinecone healthy
- [ ] Returns index stats (vector count, namespaces)
- [ ] Returns 503 when Pinecone unavailable
- [ ] Returns appropriate message when using local provider

**Time**: 0.25 day

---

#### Task 9.2: Pinecone Index Verification
**Goal**: Verify Pinecone index exists and test operations

**Status**: ‚è∏Ô∏è Pending (depends on Task 9.1)

**Implementation**:

**File**: `tests/integration/test_pinecone_verification.py`
```python
import pytest
from app.services.vector_provider_factory import get_vector_provider
from app.core.config import settings

@pytest.mark.skipif(
    not settings.PINECONE_API_KEY,
    reason="Pinecone not configured"
)
@pytest.mark.asyncio
async def test_pinecone_index_exists():
    """Test that Pinecone index exists and is accessible."""
    provider = get_vector_provider()

    # Skip if not using Pinecone
    if provider.__class__.__name__ != "PineconeProvider":
        pytest.skip("Not using Pinecone provider")

    # Verify index exists
    health = await provider.health_check()
    assert health is True

    # Verify index name
    assert provider.index_name == f"pratikoai-embed-{provider.dimension}"

    # Get stats
    stats = await provider.get_index_stats()
    assert stats is not None
    assert "total_vector_count" in stats or "namespaces" in stats

@pytest.mark.skipif(
    not settings.PINECONE_API_KEY,
    reason="Pinecone not configured"
)
@pytest.mark.asyncio
async def test_pinecone_upsert_query():
    """Test upsert and query operations."""
    provider = get_vector_provider()

    if provider.__class__.__name__ != "PineconeProvider":
        pytest.skip("Not using Pinecone provider")

    # Test vector
    test_vector = [0.1] * provider.dimension
    test_id = "test_vector_123"
    test_namespace = f"env={settings.ENVIRONMENT}:domain=test:tenant=test123"

    # Upsert
    await provider.upsert(
        vectors=[(test_id, test_vector, {"test": "data"})],
        namespace=test_namespace
    )

    # Query
    results = await provider.query(
        vector=test_vector,
        top_k=5,
        namespace=test_namespace
    )

    assert len(results) > 0
    assert results[0]["id"] == test_id

    # Cleanup
    await provider.delete(ids=[test_id], namespace=test_namespace)
```

**Acceptance Criteria**:
- [ ] Test verifies index exists
- [ ] Test verifies index name format
- [ ] Test successfully upserts vector
- [ ] Test successfully queries vector
- [ ] Test cleans up test data

**Time**: 0.25 day

---

## ‚úÖ ACCEPTANCE CRITERIA

### Phase 7: Trace Verification & E2E Testing
- [ ] RAG trace shows all 135 steps executing in diagram order
- [ ] No duplicate step logging
- [ ] No missing steps
- [ ] Frontend test: Query with PII returns anonymized response
- [ ] Frontend test: No PII visible in streaming chunks
- [ ] Steps 75-99 wired to unified graph
- [ ] Tool execution flow completes successfully
- [ ] Frontend test: PDF attachment triggers document ingest tool
- [ ] Frontend test: Response contains document analysis

### Phase 8: RSS Monitoring Integration
- [ ] Database tables `rss_feed_sources` and `rss_feed_items` created
- [ ] Migration runs successfully
- [ ] `RSSRepository` saves/fetches items correctly
- [ ] Deduplication prevents duplicate feed items
- [ ] Background task starts on app startup
- [ ] RSS feeds checked every 2 hours
- [ ] New items saved to database
- [ ] Integration test mocks RSS feed and verifies save

### Phase 9: Pinecone Health Check
- [ ] Health check endpoint returns Pinecone status
- [ ] Index stats visible (vector count, namespaces)
- [ ] Integration test verifies index exists
- [ ] Integration test successfully upserts and queries vectors

---

## üìä PROGRESS TRACKING

| Phase | Task | Status | Time Est. | Time Actual |
|-------|------|--------|-----------|-------------|
| 7.1 | Trace Verification | ‚è∏Ô∏è Pending | 1 day | - |
| 7.2 | PII Anonymization Test | ‚è∏Ô∏è Pending | 0.5 day | - |
| 7.3 | Wire Tool Execution | ‚è∏Ô∏è Pending | 1 day | - |
| 7.4 | Attachment/Tool Test | ‚è∏Ô∏è Pending | 0.5 day | - |
| 8.1 | RSS Database Schema | ‚è∏Ô∏è Pending | 0.5 day | - |
| 8.2 | RSS Persistence Layer | ‚è∏Ô∏è Pending | 0.5 day | - |
| 8.3 | Scheduled RSS Monitoring | ‚è∏Ô∏è Pending | 1 day | - |
| 8.4 | RSS Integration Test | ‚è∏Ô∏è Pending | 0.5 day | - |
| 9.1 | Pinecone Health Check | ‚è∏Ô∏è Pending | 0.25 day | - |
| 9.2 | Pinecone Index Verification | ‚è∏Ô∏è Pending | 0.25 day | - |
| **TOTAL** | | | **6 days** | **0 days** |

---

## üìÅ FILES TO CREATE/MODIFY

### New Files
1. `docs/architecture/RAG_FLOW_IMPLEMENTATION_03_verification_testing.md` - This document
2. `tests/e2e/test_frontend_pii_anonymization.py` - PII anonymization E2E test
3. `tests/e2e/test_frontend_attachment_tool.py` - Attachment/tool E2E test
4. `app/models/rss_models.py` - RSS feed database models
5. `app/repositories/rss_repository.py` - RSS repository
6. `app/services/rss_background_task.py` - Background monitoring task
7. `tests/integration/test_rss_monitoring.py` - RSS integration test
8. `tests/integration/test_pinecone_verification.py` - Pinecone verification test
9. `alembic/versions/xxx_add_rss_feed_tables.py` - Database migration

### Modified Files
1. `app/core/langgraph/graph.py` - Wire Steps 75-99 to unified graph (~200 lines)
2. `app/services/vector_providers/pinecone_provider.py` - Add health_check() and get_index_stats()
3. `app/api/v1/health.py` - Add /health/pinecone endpoint
4. `app/main.py` - Add RSS monitor startup/shutdown hooks

---

## üîÑ DEPENDENCIES

```
Phase 7.1 (Trace Verification)
    ‚Üì
Phase 7.2 (PII Test) ‚Üê Can run in parallel
    ‚Üì
Phase 7.3 (Wire Tool Execution)
    ‚Üì
Phase 7.4 (Attachment Test) ‚Üê Depends on 7.3

Phase 8.1 (RSS Schema)
    ‚Üì
Phase 8.2 (RSS Persistence) ‚Üê Depends on 8.1
    ‚Üì
Phase 8.3 (RSS Monitoring) ‚Üê Depends on 8.2
    ‚Üì
Phase 8.4 (RSS Test) ‚Üê Depends on 8.3

Phase 9.1 (Pinecone Health) ‚Üê Can run anytime
    ‚Üì
Phase 9.2 (Pinecone Verification) ‚Üê Depends on 9.1
```

**Phases 7, 8, 9 can run in parallel** after their internal dependencies are met.

---

## üìù NOTES

### Atomic Facts Persistence Decision
**Decision**: Do NOT persist atomic facts to database

**Rationale**:
1. Atomic facts are ephemeral query-specific context, not domain knowledge
2. Already used for cache key generation (Step 18: query signature)
3. Langfuse traces capture facts for observability
4. Persisting would create unnecessary DB storage overhead
5. Not persisting reduces PII exposure risk

**Alternative**: If analytics needed in the future, export facts to data warehouse via Langfuse

### RSS Monitoring Strategy
- Default check interval: 2 hours
- 10 RSS feeds monitored (unions, employer associations, official sources)
- CCNL detection via keyword matching
- Items classified separately after fetch (not in real-time)

### Pinecone Configuration
- Environment-aware: prod/preprod use Pinecone, dev uses local fallback
- Index naming: `pratikoai-embed-{dimension}` (e.g., `pratikoai-embed-384`)
- Namespace structure: `env={environment}:domain={domain}:tenant={tenant_id}`

---

---

## üé≠ FRONTEND TESTING REQUIREMENTS

**Important**: Several tasks require frontend interaction to validate end-to-end flows. Below are the specific actions you (the user) need to perform on the frontend to help verify the backend implementation.

### Test 1: PII Anonymization (Task 7.2)

**What to Test**: Verify that Personally Identifiable Information is anonymized in responses

**Steps**:
1. Open the frontend chat interface
2. **Ask this EXACT question** (contains Italian PII):
   ```
   Mi chiamo Marco Rossi, il mio codice fiscale √® RSSMRC80A01H501Z.
   Il mio numero di telefono √® +39 333 1234567 e la mia email √® marco.example@example.com.
   La mia partita IVA √® IT12345678901 e il mio IBAN √® IT60X0542811101000000123456.
   Vorrei sapere se posso detrarre le spese mediche del 2024.
   ```

3. **Verify in response**:
   - ‚úÖ Response contains tax deduction information (query was answered)
   - ‚ùå NO "Marco Rossi" appears
   - ‚ùå NO "RSSMRC80A01H501Z" appears
   - ‚ùå NO "+39 333 1234567" appears
   - ‚ùå NO "marco.example@example.com" appears
   - ‚ùå NO "IT12345678901" appears
   - ‚ùå NO "IT60X0542811101000000123456" appears

4. **Tell me**: Did any PII leak through? If yes, which ones?

---

### Test 2: Redis Cache Hit/Miss (Task 7.5)

**What to Test**: Verify Redis cache works and speeds up responses

**Steps**:

**Part A: Cache Miss (First Request)**
1. Open frontend chat interface
2. **Ask this EXACT question**:
   ```
   Quali sono i requisiti per aprire una partita IVA in regime forfettario 2024?
   ```
3. **Note the response time** (should be 2-3 seconds - slow because it's calling the LLM)
4. **Verify** answer appears with information about "forfettario"

**Part B: Cache Hit (Second Request)**
1. **Ask the EXACT SAME question again** (word-for-word):
   ```
   Quali sono i requisiti per aprire una partita IVA in regime forfettario 2024?
   ```
2. **Note the response time** (should be <500ms - instant! This is the cache hit)
3. **Verify** answer is IDENTICAL to first response

**Tell me**:
- First request time: ___ seconds
- Second request time: ___ seconds
- Were the responses identical? (Yes/No)
- Did you see any "cache" indicator in the UI?

---

### Test 3: Document Attachment & Tool Execution (Task 7.4)

**What to Test**: Verify PDF attachment triggers document ingest tool

**Steps**:
1. Create a simple test PDF with text: "Test Invoice - Total Amount: ‚Ç¨1,500"
2. Upload the PDF to chat interface
3. **Ask this question**:
   ```
   Analizza il documento allegato
   ```
4. **Verify in response**:
   - ‚úÖ Response mentions the document was analyzed
   - ‚úÖ Response contains information from the PDF (e.g., "invoice", "‚Ç¨1,500", or Italian "fattura")
   - ‚úÖ Response is NOT a generic "I can't see attachments" error

**Tell me**: Did the backend analyze the document? What did it say?

---

### Test 4: Trace Verification (Task 7.1)

**What to Test**: Comprehensive query that exercises full RAG flow

**Steps**:
1. **Ask this comprehensive question**:
   ```
   Quali sono le ultime novit√† CCNL metalmeccanici 2024?
   Devo calcolare il TFR per un dipendente assunto nel 2020 con RAL ‚Ç¨35,000.
   ```
2. Wait for response
3. **Tell me** if you received a complete answer

**I will check backend**: RAG trace logs to verify all 135 steps executed in correct order

---

### Test 5: Epoch Invalidation (Task 7.5 - Advanced)

**What to Test**: Cache invalidates when knowledge base updates

**Steps**:
1. **Ask question about CCNL**:
   ```
   Quali sono le novit√† CCNL metalmeccanici?
   ```
2. Note response time (first = slow, LLM call)
3. **I will update KB epoch on backend** (simulating new knowledge added)
4. **Ask SAME question again**:
   ```
   Quali sono le novit√† CCNL metalmeccanici?
   ```
5. Note response time (should be slow again - cache invalidated!)

**Tell me**: Second response time - was it slow again (>2s) or fast (<500ms)?

---

## üìû How to Report Results

For each test, please tell me:
1. **Test number** (e.g., "Test 2 - Cache")
2. **What happened** (response time, content, errors)
3. **Pass/Fail** - Did it work as expected?
4. **Screenshots** (if helpful)

**Example Report**:
```
Test 2 - Redis Cache:
- First request: 2.3 seconds ‚úÖ
- Second request: 0.15 seconds ‚úÖ
- Responses identical: Yes ‚úÖ
- Cache indicator: No indicator visible in UI
Result: PASS
```

---

**Document created**: 2025-10-31
**Author**: RAG Implementation Team
**Status**: ‚è∏Ô∏è Pending - Ready to start Phase 7
