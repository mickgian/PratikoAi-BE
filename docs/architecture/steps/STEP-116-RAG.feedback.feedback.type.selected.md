# RAG STEP 116 â€” Feedback type selected (RAG.feedback.feedback.type.selected)

**Type:** process  
**Category:** feedback  
**Node ID:** `FeedbackTypeSel`

## Intent (Blueprint)
Process orchestrator that routes feedback to appropriate processing endpoints based on feedback type and context. Takes input from Step 114 (when user provides feedback) and routes to Step 117 (FAQ feedback), Step 118 (KB feedback), or Step 119 (Expert feedback collector) based on priority-based logic. Implements thin orchestration pattern with no business logic, focusing on routing decisions and context preservation.

## Current Implementation (Repo)
- **Role:** Internal
- **Paths / classes:** `app/orchestrators/feedback.py:466` - `step_116__feedback_type_sel()`
- **Helper function:** `app/orchestrators/feedback.py:445` - `_determine_feedback_routing()`
- **Test suite:** `tests/test_rag_step_116_feedback_type_sel.py` (17 comprehensive tests)
- **Status:** âœ… Implemented (async process orchestrator with priority-based routing)
- **Behavior notes:**
  - Routes feedback to Step 117 (FAQ), Step 118 (KB), or Step 119 (Expert) based on priority logic
  - Expert user/feedback gets highest priority routing to Step 119
  - Explicit feedback type routing to appropriate endpoints (FAQâ†’117, KBâ†’118)
  - Contextual detection for implicit feedback type classification
  - Fallback routing to expert feedback collector (Step 119) for unmatched cases
  - Preserves all context data while adding routing metadata

## Differences (Blueprint vs Current)
- âœ… Fully implemented as async process orchestrator following thin orchestration pattern
- âœ… Added priority-based routing logic (expert > explicit > contextual > fallback)
- âœ… Added comprehensive context preservation and routing metadata
- âœ… Added error handling with graceful fallback to expert feedback collector
- âœ… Added performance tracking with routing decision timing
- âœ… Added comprehensive observability with structured logging

## Risks / Impact
- **Low Risk:** Well-tested routing logic with comprehensive test coverage (17 tests)
- **Performance:** Minimal latency impact - routing decisions are fast priority checks
- **Error Handling:** Graceful fallback to expert feedback collector on any routing failures
- **Backwards Compatibility:** Preserves all existing context data while adding routing metadata

## TDD Task List
- [x] Unit tests: 11 comprehensive test cases covering all routing scenarios and edge cases
- [x] Integration tests: 5 integration tests covering Step 114â†’116, 116â†’117, 116â†’119, and full pipeline
- [x] Implementation changes:
  - [x] Converted sync stub to async process orchestrator
  - [x] Added `_determine_feedback_routing()` helper function with priority-based logic
  - [x] Added expert user/feedback priority handling (highest priority)
  - [x] Added explicit feedback type routing (FAQâ†’117, KBâ†’118)
  - [x] Added contextual feedback detection for implicit classification
  - [x] Added fallback routing to expert feedback collector (Step 119)
  - [x] Added comprehensive context preservation and error handling
- [x] Observability: added structured log lines
  `RAG STEP 116 (RAG.feedback.feedback.type.selected): Feedback type selected | attrs={...}`
- [x] Feature flag / config: Uses existing context-based routing logic
- [x] Rollout plan: No rollout needed - routing enhancement to existing feedback pipeline

## Done When
- [x] Tests pass (17/17 tests passing);
- [x] metrics/latency acceptable (minimal routing decision time);
- [x] feature behind flag if risky (routing logic with graceful fallback).

## Links
- RAG Diagram: `docs/architecture/diagrams/pratikoai_rag.mmd`
- Step registry: `docs/architecture/rag_steps.yml`


<!-- AUTO-AUDIT:BEGIN -->
Status: ðŸ”Œ  |  Confidence: 0.49

Top candidates:
1) app/services/expert_feedback_collector.py:31 â€” app.services.expert_feedback_collector.ExpertFeedbackCollector (score 0.49)
   Evidence: Score 0.49, Service for collecting and processing expert feedback on AI responses.

Features...
2) app/services/automatic_improvement_engine.py:1 â€” app.services.automatic_improvement_engine (score 0.49)
   Evidence: Score 0.49, Automatic Improvement Engine for Quality Analysis System.

Automatically generat...
3) app/orchestrators/feedback.py:783 â€” app.orchestrators.feedback._create_expert_feedback_record (score 0.46)
   Evidence: Score 0.46, Helper function to create expert feedback record using ExpertFeedbackCollector s...
4) app/services/expert_feedback_collector.py:1 â€” app.services.expert_feedback_collector (score 0.45)
   Evidence: Score 0.45, Expert Feedback Collection Service for Quality Analysis System.

Handles collect...
5) app/services/expert_feedback_collector.py:149 â€” app.services.expert_feedback_collector.ExpertFeedbackCollector._validate_feedback_data (score 0.45)
   Evidence: Score 0.45, Validate feedback data structure and content

Notes:
- Implementation exists but may not be wired correctly
- Low confidence in symbol matching

Suggested next TDD actions:
- Connect existing implementation to RAG workflow
- Add integration tests for end-to-end flow
- Verify error handling and edge cases
<!-- AUTO-AUDIT:END -->