# PratikoAi Backend - Development Roadmap

**Last Updated:** 2024-11-24
**Status:** Active Development
**Next Task ID:** DEV-BE-94

---

## Overview

This roadmap tracks planned architectural improvements and enhancements for the PratikoAi backend system. Each task follows the DEV-XX numbering scheme matching our development workflow.

**Current Architecture:** See `docs/DATABASE_ARCHITECTURE.md` for detailed documentation of the production system.

**Recent Completed Work:**
- DEV-BE-92: Test Coverage to 49% Threshold (2024-11-24)
- DEV-BE-71: Disable Emoji in LLM Responses (2024-11-24)
- DEV-BE-68: Remove Pinecone Integration Code (2024-11-24)
- DEV-BE-67: Sprint 0: Multi-Agent System Setup (2024-11-17)
- DEV-BE-66: RSS feed setup and initial monitoring (2024-11-13)

**Deployment Timeline Estimates:**

üìÖ **Time to QA Environment (DEV-BE-75):**
- **Optimistic (parallel work):** ~7-8 weeks (26 Nov - 21 Gen)
- **Conservative (sequential):** ~9-10 weeks (26 Nov - 5 Feb)
- **Prerequisites:** DEV-BE-70, DEV-BE-69, DEV-BE-67, DEV-BE-71, DEV-BE-72...
- **Total effort (sequential):** 49 days (7.0 weeks)

üìÖ **Time to Preprod Environment (DEV-BE-88):**
- **Optimistic:** ~15-17 weeks from now (26 Nov - 28 Mar)
- **Conservative:** ~22-24 weeks from now (26 Nov - 10 Mag)
- **Prerequisites:** Path to QA + DEV-BE-75, DEV-BE-87, DEV-BE-68
- **Total effort (sequential):** 108 days (15.4 weeks)

üìÖ **Time to Production Environment (DEV-BE-90):**
- **Optimistic:** ~17-18.3 weeks from now (26 Nov - 3 Apr)
- **Conservative:** ~25-28 weeks from now (26 Nov - 11 Giu)
- **Prerequisites:** Path to Preprod + DEV-BE-68, DEV-BE-91, DEV-BE-88
- **Total effort (sequential):** 118 days (16.8 weeks)
- **Note:** Production launch requires full GDPR compliance and payment system validation

**Key Dependencies:**
- ‚ö†Ô∏è **DEV-BE-72** - Implement Expert Feedback System: Blocks QA deployment (longest task)
- ‚ö†Ô∏è **GDPR Audits** - DEV-74, DEV-89, DEV-91: Required before each environment launch

---

## Development Standards

**All tasks in this roadmap must follow these requirements:**

### Test-Driven Development (TDD)
- **Write tests FIRST, then implement features**
- Follow the Red-Green-Refactor cycle:
  1. üî¥ Write failing test
  2. üü¢ Write minimal code to pass test
  3. üîµ Refactor while keeping tests green

### Code Coverage Requirements
- **Minimum coverage:** ‚â•49% (configured in `pyproject.toml`)
- **Pre-commit enforcement:** Commits will be blocked if coverage falls below threshold
- **Test command:** `uv run pytest --cov=app --cov-report=html`
- **Coverage report:** Generated to `htmlcov/index.html`

### Code Quality & Style
- **Linting:** All code must pass Ruff linter checks
  - Ruff replaces: `flake8`, `pylint`, `isort`, `pyupgrade`, `black`
  - Run: `uv run ruff check . --fix`
- **Formatting:** Use Ruff formatter (enforces consistent style)
  - Run: `uv run ruff format .`
- **Type Hints:** Add type hints to all new functions
  - Checked by MyPy: `uv run mypy app/`
  - Start with function signatures, gradually increase strictness
- **Import Management:** Unused imports automatically removed by Ruff
- **Commented Code:** Eradicate commented-out code (detected by Ruff ERA rules)
- **Pre-commit Hooks:** All checks run automatically before commits
  - Ruff linter + formatter
  - MyPy type checker
  - Tests with coverage
  - Security checks
- **Manual Quality Check:** Run before creating PRs:
  ```bash
  ./scripts/check_code.sh          # Run all checks
  ./scripts/check_code.sh --fix    # Auto-fix issues
  ./scripts/check_code.sh --no-test # Skip tests (faster)
  ```

### Quality Commands Reference
```bash
# Fix all auto-fixable issues (run before commit)
uv run ruff check . --fix
uv run ruff format .

# Type check (non-blocking warnings)
uv run mypy app/

# Run all checks + tests
./scripts/check_code.sh

# Fix and format in one command
./scripts/check_code.sh --fix
```

### Why These Standards?
- **Prevent compilation errors** - MyPy catches type errors before runtime
- **Eliminate unused imports** - Ruff automatically removes them
- **Consistent code style** - Ruff formatter enforces uniform formatting
- **Catch bugs early** - Ruff detects common Python anti-patterns
- **Save commit time** - Pre-commit hooks prevent bad code from being committed
- **Code quality metrics** - Configured in `pyproject.toml` with detailed comments

---

## Q4 2024 (October - December)

### ‚úÖ Completed Tasks

<details>
<summary>
<h3>DEV-BE-94: Dual Metadata Registry Migration (CRITICAL)</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 60 days (Actual: ~3 days) | <strong>Status:</strong> ‚úÖ COMPLETED (2024-12-01)<br>
Consolidated all 44 models to SQLModel, eliminating dual metadata registry and enabling proper Alembic migrations.
</summary>

### DEV-BE-94: Dual Metadata Registry Migration (CRITICAL)

**Status:** ‚úÖ COMPLETED (2024-12-01)
**Priority:** HIGH - Affects maintainability and Alembic migrations
**Estimated Effort:** 60 days (8-10 weeks) - REVISED from 37 days
**Actual Effort:** ~3 days
**Created:** 2025-11-28
**Completed:** 2024-12-01
**Phase 0 Complete:** 2025-11-28 (FK fixes, type fixes, tests, audit, architectural review)
**Phases 1-4 Complete:** 2024-12-01 (all models converted, merged into DEV-BE-72)
**Architectural Review:** ADR-014 CONDITIONALLY ACCEPTED by @Egidio

#### Problem Solved

The codebase had **TWO separate SQLAlchemy metadata registries** that didn't communicate:

1. **SQLModel.metadata** (CORRECT ‚úÖ) - Used by User model
2. **Base.metadata** (LEGACY ‚ö†Ô∏è) - Used by 44 models across 7 files

This caused Alembic migration failures, mapper initialization errors, and database schema drift.

#### Solution Implemented

Consolidated ALL 44 models to use `SQLModel.metadata`:

- `app/models/ccnl_database.py` (9 models)
- `app/models/ccnl_update_models.py` (5 models)
- `app/models/faq_automation.py` (5 models)
- `app/models/quality_analysis.py` (9 models)
- `app/models/regional_taxes.py` (4 models)
- `app/models/subscription.py` (4 models)
- `app/models/data_export.py` (8 models)

#### Migration Phases Completed

- ‚úÖ Phase 0: Preparation (FK fixes, type fixes, tests, audit)
- ‚úÖ Phase 0.5: Alembic Behavior Testing
- ‚úÖ Phase 1: Simple Models (regional_taxes.py - 4 models)
- ‚úÖ Phase 2: Core CCNL Models (14 models)
- ‚úÖ Phase 3: User-Dependent Models (14 models with User FKs)
- ‚úÖ Phase 4: Complex Business Models (12 models)

#### Key Technical Challenges Resolved

1. **pgvector Vector columns** - Used `sa_column=Column(Vector(1536))`
2. **PostgreSQL ARRAY columns** - Used `sa_column=Column(ARRAY(String))`
3. **JSONB columns** - Used `sa_column=Column(JSONB))`
4. **Foreign key consistency** - Standardized to `user.id`

#### Benefits Achieved

- ‚úÖ Single metadata registry (single source of truth)
- ‚úÖ Alembic works correctly for all models
- ‚úÖ No more mapper initialization errors
- ‚úÖ Consistent codebase patterns
- ‚úÖ Better FastAPI integration
- ‚úÖ Less code duplication (DB model = API schema)

#### References

- **SQLModel Docs:** https://sqlmodel.tiangolo.com/
- **Related Issues:** Golden set retrieval workflow blocker, Alembic migration failures

</details>

---

<details>
<summary>
<h3>DEV-BE-67: Sprint 0: Multi-Agent System Setup</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 1 day (2024-11-17) | <strong>Actual:</strong> 1 day | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚úÖ COMPLETED<br>
Established foundational multi-agent development system with 8 specialized subagents and Italian name mappings.
</summary>

### DEV-BE-67: Sprint 0: Multi-Agent System Setup
**Priority:** HIGH | **Effort:** 1 day (2024-11-17) | **Actual:** 1 day | **Dependencies:** None | **Status:** ‚úÖ COMPLETED

**Original Task:** Migrate FAQ Embeddings from Pinecone to pgvector

**NOTE:** The original FAQ migration work has been **deferred to Sprint 1**. Sprint 0 focused on establishing the foundational multi-agent development system.

**What Was Actually Completed (Sprint 0):**

**1. Multi-Agent System Architecture**
- ‚úÖ Documented complete subagent architecture in `.claude/decisions.md`
- ‚úÖ Established Scrum Master role (@Ottavio) for sprint coordination
- ‚úÖ Defined clear responsibilities for each specialized subagent
- ‚úÖ Created systematic approach to complex development tasks

**2. Subagent Configurations Created (8 Total)**
- ‚úÖ `.claude/subagents/architect.md` - System architecture and design decisions
- ‚úÖ `.claude/subagents/backend-dev.md` - Backend implementation specialist
- ‚úÖ `.claude/subagents/data-engineer.md` - Database and data pipeline work
- ‚úÖ `.claude/subagents/devops.md` - Infrastructure and deployment automation
- ‚úÖ `.claude/subagents/frontend-dev.md` - Frontend implementation specialist
- ‚úÖ `.claude/subagents/qa-engineer.md` - Testing and quality assurance
- ‚úÖ `.claude/subagents/scrum-master.md` - Sprint planning and coordination
- ‚úÖ `.claude/subagents/tech-lead.md` - Technical leadership and code review

**3. Italian Name Mapping (@mentions)**
- ‚úÖ Configured Italian name aliases for natural team interaction:
  - @Ottavio (Scrum Master)
  - @Marco (Architect)
  - @Luigi (Backend Dev)
  - @Giovanni (Data Engineer)
  - @Alessandro (DevOps)
  - @Francesca (Frontend Dev)
  - @Sofia (QA Engineer)
  - @Roberto (Tech Lead)

**4. Slack Integration (Two-Webhook Architecture)**
- ‚úÖ Implemented dual-webhook system for team communication
- ‚úÖ Main webhook: General team notifications and updates
- ‚úÖ Scrum Master webhook: Sprint planning, task assignments, progress reports
- ‚úÖ Automated notification system for development milestones

**5. Context Files Structure**
- ‚úÖ Created `.claude/sprint-plan.md` for current sprint tracking
- ‚úÖ Created `.claude/subagent-assignments.md` for task distribution
- ‚úÖ Established single source of truth for sprint progress
- ‚úÖ Enabled parallel work coordination across subagents

**Impact & Value Delivered:**
- **Development Efficiency:** Specialized subagents reduce context switching
- **Code Quality:** Clear ownership and review processes
- **Team Coordination:** Systematic sprint planning with @Ottavio
- **Communication:** Automated Slack notifications keep stakeholders informed
- **Scalability:** Framework supports growing team and complexity

**Deferred Work (Sprint 1):**
The original FAQ migration from Pinecone to pgvector will be addressed in Sprint 1 as part of the broader infrastructure optimization efforts. This includes:
- Creating `faq_embeddings` table in PostgreSQL
- Migrating FAQ data from Pinecone to pgvector
- Refactoring `app/orchestrators/golden.py` to use pgvector
- Removing Pinecone dependencies (cost savings: $150-330/month)

**Acceptance Criteria (All Met):**
- ‚úÖ All 8 subagent configurations operational
- ‚úÖ Italian name mappings working correctly
- ‚úÖ Slack integration delivering notifications
- ‚úÖ Sprint planning framework established
- ‚úÖ Context files structure in place
- ‚úÖ Team ready for Sprint 1 development work

**Completion Date:** 2024-11-17

</details>

---

<details>
<summary>
<h3>DEV-BE-68: Remove Pinecone Integration Code</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 1-2 days (with Claude Code) | <strong>Dependencies:</strong> DEV-BE-67 ‚úÖ (must complete first) | <strong>Status:</strong> ‚úÖ COMPLETED (2024-11-24)<br>
Removed 600+ lines of dead Pinecone code after FAQ migration, reducing maintenance burden and costs.
</summary>

### DEV-BE-68: Remove Pinecone Integration Code
**Priority:** HIGH | **Effort:** 1-2 days (with Claude Code) | **Dependencies:** DEV-BE-67 ‚úÖ (must complete first)

**Problem:**
Pinecone integration code (600+ lines) adds maintenance burden and confuses developers. After FAQ migration (DEV-BE-67), all Pinecone code is dead code.

**Implementation Tasks:**

**Week 1: Code Removal**
- [x] Delete `app/services/vector_providers/pinecone_provider.py` (349 lines)
- [x] Delete `app/services/vector_config.py` (205 lines)
- [x] Delete `app/services/vector_provider_factory.py`
- [x] Delete `app/services/embedding_management.py` (Pinecone-based)
- [x] Delete `app/services/hybrid_search_engine.py` (Pinecone-based)
- [x] Delete `app/services/query_expansion_service.py`
- [x] Delete `app/services/semantic_faq_matcher.py`
- [x] Delete `app/services/context_builder.py` (check if used elsewhere first!)
- [x] Remove from `requirements.txt` or `pyproject.toml`: `pinecone-client>=2.2.0`
- [x] Delete tests: `tests/test_vector_search.py`
- [x] Remove Pinecone env vars from `.env.example`:
  - `PINECONE_API_KEY`
  - `PINECONE_ENVIRONMENT`
  - `PINECONE_INDEX_NAME`
- [x] Update `app/core/config.py` - remove Pinecone settings
- [x] Search codebase for "pinecone" (case-insensitive) and clean up all references

**Documentation Cleanup:**
- [x] Delete `docs/pinecone-guardrails.md` (262 lines)
- [x] Delete `docs/architecture/vector-search.md` (261 lines)
- [x] Update README.md to remove Pinecone references
- [x] Update `docs/DATABASE_ARCHITECTURE.md` if needed

**Acceptance Criteria:**
- ‚úÖ `grep -ri "pinecone" .` returns no results (excluding git history)
- ‚úÖ All tests pass without Pinecone dependencies
- ‚úÖ Production deployment successful with no errors
- ‚úÖ No Pinecone costs on billing dashboard

**Validation:**
- [x] Run full test suite: `pytest`
- [x] Deploy to QA, test FAQ lookup functionality

</details>

---

<details>
<summary>
<h3>DEV-BE-71: Disable Emoji in LLM Responses</h3>
<strong>Priority:</strong> MEDIUM | <strong>Effort:</strong> 1-2 days (with Claude Code) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚úÖ COMPLETED (2024-11-24)<br>
Disabled emojis in all LLM responses for professional Italian tax and legal advisory context.
</summary>

### DEV-BE-71: Disable Emoji in LLM Responses
**Priority:** MEDIUM | **Effort:** 1-2 days (with Claude Code) | **Dependencies:** None | **Status:** ‚úÖ COMPLETED (2024-11-24)

**Problem:**
LLMs (especially ChatGPT) frequently include emojis in responses (‚úÖ, üìä, üí°, etc.), which looks unprofessional for Italian tax and legal advisory context. Users expect formal, professional language without decorative elements.

**Solution:**
Add explicit instruction to system prompts to disable emoji usage. Update all prompt templates.

**Implementation Tasks:**

**Day 1: Prompt Updates**
- [x] Update `SYSTEM_PROMPT` in `app/core/langgraph/prompt_policy.py`
- [x] Update all domain-specific prompts in `app/services/prompt_template_manager.py`
- [x] Update FAQ generation prompt in `app/services/auto_faq_generator.py`
- [x] Update expert feedback prompts if LLM-generated

**Day 2: Testing & Validation**
- [x] Test 50 diverse queries and verify no emojis in responses
- [x] Check streaming responses (emojis sometimes appear in chunks)
- [x] Test with different LLM providers (OpenAI, Claude fallback)
- [x] Document emoji-free response requirement in `docs/PROMPT_ENGINEERING.md`

**Acceptance Criteria:**
- ‚úÖ No emojis in LLM responses across 100 test queries
- ‚úÖ Professional tone maintained in all responses
- ‚úÖ Bullet points and numbered lists work correctly
- ‚úÖ All prompt templates updated
- ‚úÖ Documentation updated

</details>

---

<details>
<summary>
<h3>DEV-BE-92: Increase Test Coverage to 49% Threshold</h3>
<strong>Priority:</strong> CRITICAL | <strong>Effort:</strong> 7-10 days | <strong>Dependencies:</strong> None | <strong>Sprint:</strong> Sprint 1 | <strong>Status:</strong> ‚úÖ COMPLETED (2024-11-24)<br>
Increased test coverage from 4% to 49% threshold, removing broken tests and adding comprehensive test suites.
</summary>

### DEV-BE-92: Increase Test Coverage to 49% Threshold
**Priority:** CRITICAL | **Effort:** 7-10 days | **Dependencies:** None | **Sprint:** Sprint 1 | **Status:** ‚úÖ COMPLETED (2024-11-24)

**Problem:**
Pre-commit hooks block all commits when test coverage is below 49% threshold. Current coverage is only 4%, preventing any code from being committed to the repository. This is a critical workflow blocker that halts all development activity.

**Root Cause:**
- 20 broken test files were blocking pytest execution
- Many core modules have zero test coverage
- Pre-commit hook enforces minimum 49% coverage (configured in `pyproject.toml`)
- No work should ever proceed without a proper DEV-BE task in ARCHITECTURE_ROADMAP.md

**Solution:**
Systematically remove broken tests, measure baseline coverage, and generate comprehensive tests for all uncovered modules until 49% threshold is achieved.

**Implementation Tasks (All Completed):**

- [x] Remove 20 broken test files blocking pytest
- [x] Create comprehensive tests for tax_constants.py (100% coverage achieved)
- [x] Fix Slack mobile formatting issues
- [x] Establish baseline coverage measurement
- [x] Generate tests for high-impact modules
- [x] Verify coverage reaches 49%
- [x] Pre-commit hooks pass successfully

**Acceptance Criteria (All Met):**
- ‚úÖ All 20 broken tests removed
- ‚úÖ tax_constants.py has 100% coverage
- ‚úÖ Slack mobile formatting fixed
- ‚úÖ `pytest --cov=app` shows coverage ‚â•49%
- ‚úÖ All tests pass without errors
- ‚úÖ Pre-commit hooks pass successfully
- ‚úÖ HTML coverage report generated to `htmlcov/index.html`

</details>

<details>
<summary>
<h3>DEV-BE-72: Implement Expert Feedback System</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 2 weeks (Actual: 1.5 weeks) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚úÖ COMPLETED<br>
<strong>Completion Date:</strong> 2024-11-25<br>
Simplified SUPER_USER-only expert feedback system with automatic task generation for improvement tracking.
</summary>

### DEV-BE-72: Implement Expert Feedback System
**Priority:** HIGH | **Effort:** 2 weeks (Actual: 1.5 weeks) | **Dependencies:** None | **Status:** ‚úÖ COMPLETED
**Completion Date:** 2024-11-25

**Frontend Integration:**
This backend task is linked to **DEV-BE-004** in frontend roadmap:
- **Frontend Task:** DEV-BE-004: Implement Super Users Feedback on Answers (Expert Feedback System)
- **Location:** `/Users/micky/WebstormProjects/PratikoAiWebApp/ARCHITECTURE_ROADMAP.md`
- **Coordination Required:** Backend APIs must be completed BEFORE frontend implementation
- **API Endpoints:** Frontend will consume `/api/v1/expert-feedback/*` endpoints created in this task

**Problem:**
Expert feedback system was designed in architecture diagram (steps S113-S130) with complex trust scoring and auto-approval workflows. However, for MVP launch, a simpler SUPER_USER-only approach was needed to:
1. Collect structured feedback from verified experts
2. Track answer quality issues (correct/incomplete/incorrect)
3. Automatically generate improvement tasks for content team
4. Integrate with Golden Set workflow for correct answers

**Architectural Decision:**
The original plan included a 3-tier trust scoring system (0.7-0.79, 0.80-0.89, 0.90-1.00) with graduated auto-approval privileges. This was **intentionally simplified away** to a binary SUPER_USER role approach because:
- All experts are manually vetted before receiving SUPER_USER role
- Trust scoring adds complexity without value when experts are pre-trusted
- Admin approval queue creates unnecessary friction for trusted experts
- Reduces database queries (4-5 fewer per feedback submission)

**Reference:** See "Not Approved Features" section for full rationale and future implementation path.

---

## Implementation Completed

### 1. Database Schema & Migrations

**Migration Files:**
- `alembic/versions/20251121_add_expert_feedback_system.py` - 339 lines
- `alembic/versions/20251124_add_user_role.py` - 60 lines

### 2. Backend API Endpoints

**File:** `app/api/v1/expert_feedback.py` - 558 lines

**Endpoints Implemented:**
1. **POST `/api/v1/expert-feedback/submit`** - Submit expert feedback on an AI response
2. **GET `/api/v1/expert-feedback/history`** - Retrieve paginated feedback history
3. **GET `/api/v1/expert-feedback/{feedback_id}`** - Retrieve detailed feedback record
4. **GET `/api/v1/expert-feedback/profile`** - Retrieve expert profile with credentials

### 3. Business Logic Services

- `app/services/expert_feedback_collector.py` - 706 lines
- `app/services/task_generator_service.py` - 349 lines
- `app/services/task_digest_email_service.py` - 206 lines

### 4. Request/Response Schemas

**File:** `app/schemas/expert_feedback.py` - 282 lines

### 5. Testing

- `tests/api/test_expert_feedback.py` - 804 lines (17 API tests passing)
- `tests/services/test_task_generator_service.py` - 354 lines (18 service tests passing)
- **Total Tests:** 35 tests passing

---

**Acceptance Criteria (All Met):**
- ‚úÖ Expert feedback collected via POST `/api/v1/expert-feedback/submit`
- ‚úÖ SUPER_USER role validation enforced (403 Forbidden for non-experts)
- ‚úÖ Authentication required (JWT Bearer token)
- ‚úÖ Feedback stored in `expert_feedback` table with all fields
- ‚úÖ Automatic task generation for "incomplete" and "incorrect" feedback
- ‚úÖ Tasks written to SUPER_USER_TASKS.md with structured format
- ‚úÖ Golden Set workflow integration for "correct" feedback (background task)
- ‚úÖ Feedback history API endpoint with pagination
- ‚úÖ Comprehensive test coverage (35 tests, ‚â•69.5% coverage)

**Total Lines:** 4,221 lines of implementation + 1,158 lines of tests = **5,379 lines**

</details>

---

<details>
<summary>
<h3>DEV-BE-69: Expand RSS Feed Sources</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 1.5 weeks | <strong>Dependencies:</strong> DEV-BE-66 ‚úÖ | <strong>Status:</strong> ‚úÖ COMPLETED<br>
<strong>Completion Date:</strong> 2024-12-10<br>
Expanded knowledge base with 11 new RSS feeds (4-hour schedule) + 2 web scrapers (daily schedule) for comprehensive Italian regulatory coverage.
</summary>

### DEV-BE-69: Expand RSS Feed Sources
**Priority:** HIGH | **Effort:** 1.5 weeks | **Dependencies:** DEV-BE-66 ‚úÖ (RSS infrastructure complete) | **Status:** ‚úÖ COMPLETED
**Completion Date:** 2024-12-10

**Git:** Branch from `develop` ‚Üí `DEV-BE-69-Expand-RSS-Feed-Sources`

**Problem:**
Currently only 2 RSS feeds configured: Agenzia delle Entrate (Normativa e prassi, News). Missing coverage of INPS, Ministero del Lavoro, MEF, INAIL, Gazzetta Ufficiale, and Corte di Cassazione.

**Solution:**
Expanded knowledge base with 10 new RSS feeds (4-hour schedule) + 2 web scrapers (daily schedule).

**Sources Added:**
- **INPS** (5 feeds): News, Comunicati stampa, Circolari, Messaggi, Sentenze
- **Ministero del Lavoro** (1 feed): RSS feed
- **MEF** (2 feeds): Documenti, Aggiornamenti
- **INAIL** (2 feeds): Notizie, Eventi
- **Gazzetta Ufficiale** (4 feeds + scraper): Serie Generale, Corte Costituzionale, Unione Europea, Regioni
- **Corte di Cassazione** (scraper): Tax section (Tributaria) + Labor section (Lavoro)

**Key Implementations:**
- Rate limiting with semaphore (max 5 concurrent feeds) + stagger delay (1-3s)
- Content deduplication via SHA256 hashing
- Gazzetta Ufficiale scraper with robots.txt compliance
- Corte di Cassazione scraper extension

**Acceptance Criteria (All Met):**
- [x] 11 RSS feeds configured and ingesting (2 existing + 9 new, >0 docs per source)
- [x] 2 scrapers operational (Gazzetta + Cassazione)
- [x] Rate limiting active (max 5 concurrent, 1-3s delay)
- [x] Deduplication working (no cross-source duplicates)
- [x] Document quality maintained (junk rate <15%)
- [x] Code coverage >=70% for new code
- [x] E2E tests passing
- [x] Security audit passed (@severino)

</details>

---

## Q1 2025 (January - March)

### üìã Planned Tasks

<details>
<summary>
<h3>DEV-BE-93: Unified Input Security Hardening (Chat + Expert Feedback)</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 2-3 days (with Claude Code) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Implement comprehensive input sanitization across chat flow and expert feedback to prevent XSS, markdown injection, prompt injection, and data exfiltration attacks.
</summary>

### DEV-BE-93: Unified Input Security Hardening (Chat + Expert Feedback)
**Priority:** HIGH | **Effort:** 2-3 days (with Claude Code) | **Dependencies:** None

**Problem:**
Security audit revealed critical vulnerabilities allowing malicious payloads from compromised super user laptops or malicious chat inputs:
- **V-001:** Markdown injection in task generation (unescaped user input ‚Üí `.md` files)
- **V-002:** Missing field-level length limits (50KB message limit too broad)
- **V-003:** Prompt injection in chat flow (user input directly to LLM)
- **V-004:** XSS in data exports (malicious payloads exported in JSON/CSV)
- **V-005:** Log injection (unescaped newlines in logs)
- **V-006:** Unvalidated improvement suggestions in expert feedback

**Files to Create/Modify:**

**New Files:**
- `app/utils/security/__init__.py`
- `app/utils/security/markdown_escaper.py`
- `app/utils/security/prompt_guard.py`
- `app/utils/security/validators.py`
- `tests/utils/security/test_markdown_escaper.py`
- `tests/utils/security/test_prompt_guard.py`
- `tests/services/test_task_generator_security.py`
- `tests/api/test_chat_security.py`
- `tests/api/test_expert_feedback_security.py`
- `docs/security/TESTING.md`

**Modified Files:**
- `app/services/task_generator_service.py` (lines 213-298: apply markdown escaping)
- `app/schemas/expert_feedback.py` (add max_length to fields)
- `app/schemas/chat.py` (add prompt injection detection)
- `app/api/v1/data_export.py` (sanitize before export)
- `app/core/logging.py` (escape log messages)
- `pyproject.toml` (add markupsafe dependency)

**Acceptance Criteria:**
- [ ] All 27 security tests passing
- [ ] All user inputs escaped before markdown file writes
- [ ] Max length limits enforced (query_text: 2000, original_answer: 5000, additional_details: 2000)
- [ ] Prompt injection patterns detected and logged (not blocked, for monitoring)
- [ ] Data exports sanitized (no XSS in JSON/CSV files)
- [ ] Log injection prevented (newlines/control chars escaped)
- [ ] Coverage ‚â•95% for `app/utils/security/` modules
- [ ] Performance overhead <5ms per request

</details>

---

<details>
<summary>
<h3>DEV-BE-70: Daily RSS Collection Email Report</h3>
<strong>Priority:</strong> MEDIUM | <strong>Effort:</strong> 2-3 days (with Claude Code) | <strong>Dependencies:</strong> DEV-BE-69 ‚úÖ (RSS feeds expanded) | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Automated daily email report showing RSS collection activity and DB additions.
</summary>

### DEV-BE-70: Daily RSS Collection Email Report
**Priority:** MEDIUM | **Effort:** 2-3 days (with Claude Code) | **Dependencies:** DEV-BE-69 ‚úÖ (RSS feeds expanded)

**Problem:**
No visibility into daily RSS feed ingestion. Team doesn't know if feeds are working, how many documents were added, or if there are errors.

**Solution:**
Automated daily email report showing RSS collection activity and DB additions.

**Report Contents:**

**1. Daily Summary**
- Total documents collected (last 24 hours)
- Total documents added to DB (after deduplication)
- Duplicate rate (% of collected docs already in DB)
- Error rate (% of feeds with parsing errors)

**2. Per-Feed Breakdown**
- Source name (e.g., "Agenzia delle Entrate - Normativa")
- Documents collected
- Documents added to DB
- Parse errors (if any)
- Last successful check time

**3. Data Quality Metrics**
- Average text quality score for new documents
- Junk detection rate (% marked as junk)
- Top 5 new documents by title (preview)

**4. Alerts**
- Feeds down (HTTP errors, timeouts)
- Feeds stale (no new items in 7+ days)
- High junk rate (>25% for a specific feed)

**Implementation Tasks:**

**Week 1: Email Service + Report Generation**
- [ ] Add email service to config (use AWS SES or SendGrid)
- [ ] Create `app/services/rss_report_service.py`
- [ ] Create HTML email template: `templates/email/daily_rss_report.html`
- [ ] Add cron job to `docker-compose.yml` or use Celery Beat
- [ ] Write tests: `tests/test_rss_report_service.py`

**Acceptance Criteria:**
- ‚úÖ Daily email sent at 8am (configurable time)
- ‚úÖ Email includes all report sections (summary, per-feed, quality, alerts)
- ‚úÖ Email recipients configurable in environment variables
- ‚úÖ HTML email renders correctly in Gmail/Outlook
- ‚úÖ Email sent even if no new documents (shows "0 documents collected")

**Recipients Configuration:**
```env
RSS_REPORT_RECIPIENTS=dev-team@pratikoai.com,stakeholders@pratikoai.com
RSS_REPORT_TIME=08:00  # Daily at 8am
```

</details>

---

<details>
<summary>
<h3>DEV-BE-74: GDPR Compliance Audit (QA Environment)</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 3-4 days (with Claude Code generating checklists/docs) | <strong>Dependencies:</strong> DEV-BE-75 ‚úÖ (QA environment live) | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Comprehensive GDPR compliance audit on QA environment to validate all required features.
</summary>

### DEV-BE-74: GDPR Compliance Audit (QA Environment)
**Priority:** HIGH | **Effort:** 3-4 days (with Claude Code generating checklists/docs) | **Dependencies:** DEV-BE-75 ‚úÖ (QA environment live)

**Problem:**
Must ensure GDPR compliance before any production launch. QA environment is the first place to validate compliance features.

**Solution:**
Comprehensive GDPR compliance audit on QA environment to validate all required features.

**Audit Checklist:**

**1. Right to Access (Data Export)**
- [ ] Test user data export functionality
- [ ] Verify exported data includes all user information
- [ ] Validate export format (JSON/PDF)
- [ ] Verify export completes within 30 days (GDPR requirement)

**2. Right to Erasure (Data Deletion)**
- [ ] Test user account deletion
- [ ] Verify complete data removal
- [ ] Validate deletion completes within 30 days

**3. Consent Management**
- [ ] Verify cookie consent banner functionality
- [ ] Test opt-in/opt-out mechanisms
- [ ] Validate consent records are stored

**4. Data Retention Policies**
- [ ] Verify automatic data deletion after retention period
- [ ] Test conversation data retention (default: 90 days)
- [ ] Validate log data retention (default: 30 days)

**5. Privacy by Design**
- [ ] Verify minimal data collection
- [ ] Test data encryption at rest (PostgreSQL, Redis)
- [ ] Validate data encryption in transit (HTTPS/TLS)

**Acceptance Criteria:**
- ‚úÖ All 8 audit categories pass on QA
- ‚úÖ Data export works correctly
- ‚úÖ Data deletion works completely
- ‚úÖ Documentation complete

</details>

---

<details>
<summary>
<h3>DEV-BE-75: Deploy QA Environment (Hetzner VPS)</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 1 week (mostly waiting for Hetzner approval) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Deploy complete PratikoAI backend to Hetzner VPS using existing docker-compose.yml configuration.
</summary>

### DEV-BE-75: Deploy QA Environment (Hetzner VPS)
**Priority:** HIGH | **Effort:** 1 week (mostly waiting for Hetzner approval) | **Dependencies:** None

**‚ö†Ô∏è IMPORTANT:** Contact Hetzner support first - they have a strict onboarding process for new clients.

**Problem:**
Currently testing only in local Docker environment. No QA environment for integration testing, performance validation, or stakeholder demos before production deployment.

**Solution:**
Deploy complete PratikoAI backend to Hetzner VPS using existing docker-compose.yml configuration.

**Implementation Tasks:**

**Week 1: Hetzner Account & VPS Setup**
- [ ] **Contact Hetzner support** for account approval (can take 1-3 days)
- [ ] Provision Hetzner CX21 VPS (2 vCPU, 4GB RAM, 40GB SSD)
  - Region: Germany (Falkenstein or Nuremberg)
  - OS: Ubuntu 22.04 LTS
  - Cost: ~‚Ç¨6.50/month (~$7/month)
- [ ] Configure SSH access with key-based authentication
- [ ] Set up firewall rules
- [ ] Install Docker and Docker Compose on VPS

**Week 2: Deployment & Configuration**
- [ ] Copy docker-compose.yml to VPS
- [ ] Create `.env.qa` with QA-specific configuration
- [ ] Deploy stack: `docker-compose --env-file .env.qa up -d`
- [ ] Run database migrations
- [ ] Set up DNS: `api-qa.pratikoai.com`
- [ ] Configure SSL with Let's Encrypt
- [ ] Set up automated backups (Hetzner snapshots)

**Acceptance Criteria:**
- ‚úÖ QA environment accessible at `https://api-qa.pratikoai.com`
- ‚úÖ All services running (PostgreSQL, Redis, Backend, Prometheus, Grafana)
- ‚úÖ Database migrations run successfully
- ‚úÖ All API endpoints responding (health check passes)
- ‚úÖ Automated daily backups configured

**Infrastructure Cost (QA):**
- Hetzner CX21 VPS: ~$7/month
- Snapshots/backups: ~$1/month
- **Total: ~$8/month**

</details>

---

<details>
<summary>
<h3>DEV-BE-76: Fix Cache Key + Add Semantic Layer</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 1 week (with Claude Code) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Fix broken Redis cache key (too strict) and add semantic similarity for 60-70% hit rate.
</summary>

### DEV-BE-76: Fix Cache Key + Add Semantic Layer
**Priority:** HIGH | **Effort:** 1 week (with Claude Code) | **Dependencies:** None

**Problem:**
Current Redis cache is **implemented but broken**. The cache key is TOO STRICT - includes `doc_hashes` which changes if retrieved docs differ even slightly.

**Why it fails:**
- Same question ‚Üí Slightly different retrieved documents ‚Üí Different `doc_hashes` ‚Üí Cache miss
- **Result:** Effective hit rate ~0-5% (not 20-30% as assumed)

**Solution (Two-Phase Fix):**

**Phase 1: Fix Current Cache Key (1 week)**
- Remove `doc_hashes` from cache key (too volatile)
- Simplified key: `sha256(query_hash + model + temperature + kb_epoch)`
- Expected improvement: 0-5% ‚Üí 20-30% hit rate

**Phase 2: Add Semantic Layer (1-2 weeks)**
- Add embedding similarity search for near-miss queries
- Expected improvement: 20-30% ‚Üí 60-70% hit rate

**Implementation Tasks:**

**Phase 1: Fix Cache Key**
- [ ] **Audit current cache key generation** in `app/orchestrators/cache.py`
- [ ] **Remove `doc_hashes`** from cache key
- [ ] **Simplify cache key** to: `sha256(query_hash + model + temperature + kb_epoch)`
- [ ] **Add cache hit/miss logging**
- [ ] **Test on QA:** Ask same question 10 times, verify cache hit after first call

**Phase 2: Add Semantic Layer**
- [ ] Create `query_cache_embeddings` table
- [ ] Implement `app/services/semantic_cache_service.py`
- [ ] Update cache orchestrator to check semantic similarity
- [ ] Write tests: `tests/test_semantic_cache.py`

**Expected Impact:**
- **Before:** 0-5% hit rate (broken cache)
- **After Phase 1:** 20-30% hit rate
- **After Phase 2:** 60-70% hit rate
- **Cost savings:** At 60% hit rate, save $1,500-1,800/month in LLM costs

</details>

---

<details>
<summary>
<h3>DEV-BE-77: Implement Prometheus + Grafana Monitoring</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 1-2 weeks (dashboards already in docker-compose.yml) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Industry-standard observability stack: Prometheus (metrics collection) + Grafana (visualization/alerting).
</summary>

### DEV-BE-77: Implement Prometheus + Grafana Monitoring
**Priority:** HIGH | **Effort:** 1-2 weeks | **Dependencies:** None

**Problem:**
Current monitoring relies on basic logs and periodic REST API metrics calls. No real-time visibility into RAG performance, cache hit rates, or automatic alerting on degradation.

**Solution:**
Industry-standard observability stack: Prometheus (metrics collection) + Grafana (visualization/alerting)

**Implementation Tasks:**

**Phase 1: Prometheus Setup**
- [ ] Add Prometheus to `docker-compose.yml`
- [ ] Create `prometheus.yml` configuration
- [ ] Add `prometheus-fastapi-instrumentator` to requirements
- [ ] Instrument FastAPI app with Prometheus metrics
- [ ] Add custom metrics (rag_query_duration, cache_hit_rate, etc.)

**Phase 2: Grafana Dashboards**
- [ ] Configure Prometheus as data source in Grafana
- [ ] Create **Dashboard 1: RAG Performance**
- [ ] Create **Dashboard 2: System Health**
- [ ] Create **Dashboard 3: Cost & Usage**
- [ ] Create **Dashboard 4: Data Quality**

**Phase 3: Alerting**
- [ ] Define alert rules in Grafana
- [ ] Set up Slack webhook integration
- [ ] Test alert firing and notification delivery

**Acceptance Criteria:**
- ‚úÖ Prometheus scraping metrics from FastAPI, PostgreSQL, Redis
- ‚úÖ 4 Grafana dashboards live with real-time data
- ‚úÖ Alert rules configured and tested
- ‚úÖ Documentation complete

**Cost:**
- Grafana Cloud Free Tier: **$0/month** (14-day retention, 10K metrics)

</details>

---

<details>
<summary>
<h3>DEV-BE-78: Cross-Encoder Reranking</h3>
<strong>Priority:</strong> MEDIUM | <strong>Effort:</strong> 1 week (with Claude Code) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Two-stage retrieval with cross-encoder reranking for +10-15% precision improvement.
</summary>

### DEV-BE-78: Cross-Encoder Reranking
**Priority:** MEDIUM | **Effort:** 1 week (with Claude Code) | **Dependencies:** None

**Problem:**
Hybrid retrieval returns top-14 candidates, but ranking may not be optimal. Adding a reranking stage can improve precision by 10-15%.

**Solution:**
Two-stage retrieval:
1. **Stage 1 (Broad):** Hybrid retrieval ‚Üí top 30 candidates
2. **Stage 2 (Precision):** Cross-encoder reranks top 30 ‚Üí final top 14

**Implementation Tasks:**

**Week 1: Model Selection**
- [ ] Evaluate cross-encoder models (multilingual support)
- [ ] Benchmark latency: Target <100ms for reranking 30 candidates
- [ ] Choose model based on Italian performance + latency

**Week 2: Implementation**
- [ ] Add `sentence-transformers` to requirements
- [ ] Create `app/retrieval/reranker.py`
- [ ] Update hybrid retrieval to support reranking
- [ ] Write tests and A/B test

**Acceptance Criteria:**
- ‚úÖ Precision@14 improvement: +10-15%
- ‚úÖ Latency increase: <100ms (p95)
- ‚úÖ Fallback to hybrid-only if reranking fails

</details>

---

<details>
<summary>
<h3>DEV-BE-85: Configure PostgreSQL High Availability</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 1 day (with Claude Code generating configs) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚ùå NOT STARTED<br>
PostgreSQL streaming replication with automatic failover for production readiness.
</summary>

### DEV-BE-85: Configure PostgreSQL High Availability
**Priority:** HIGH (production readiness) | **Effort:** 1 day (with Claude Code generating configs) | **Dependencies:** None

**Problem:**
Current deployment has single PostgreSQL instance. If it fails, database goes down.

**Solution:**
Set up PostgreSQL streaming replication with automatic failover using Patroni or similar.

**Tasks:**
- [ ] Set up PostgreSQL streaming replication
- [ ] Configure automatic failover (using Patroni/repmgr)
- [ ] Test failover procedure
- [ ] Document failover behavior

**Cost:** Requires additional VPS (~$7-15/month for standby)

**Trigger:** Before production launch with paying customers

</details>

---

<details>
<summary>
<h3>DEV-BE-86: Automated Index Health Monitoring + Rebuild</h3>
<strong>Priority:</strong> MEDIUM | <strong>Effort:</strong> 2-3 days (with Claude Code generating scripts/dashboards) | <strong>Dependencies:</strong> DEV-BE-77 ‚úÖ | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Automated monitoring + alerts + rebuild scripts for FTS and pgvector indexes.
</summary>

### DEV-BE-86: Automated Index Health Monitoring + Rebuild
**Priority:** MEDIUM | **Effort:** 2-3 days | **Dependencies:** DEV-BE-77 ‚úÖ (Prometheus/Grafana monitoring)

**Problem:** If FTS (GIN) or pgvector (IVFFlat) indexes become corrupted, queries become extremely slow (10-100x slower). Currently requires manual detection + rebuild.

**Solution:** Automated monitoring + alerts + rebuild scripts.

**Tasks:**
- [ ] Add Prometheus metric: `pg_index_health`
- [ ] Create Grafana alert: "Index scan ratio <50%"
- [ ] Create automated rebuild script: `scripts/ops/rebuild_indexes.sh`
- [ ] Schedule weekly index health check (cron job)
- [ ] Document manual rebuild procedure in runbook

**Acceptance Criteria:**
- ‚úÖ Grafana dashboard shows index health metrics
- ‚úÖ Alert fires when index scan ratio drops below 50%
- ‚úÖ Automated rebuild script tested on QA

</details>

---

<details>
<summary>
<h3>DEV-BE-87: User Subscription & Payment Management</h3>
<strong>Priority:</strong> CRITICAL | <strong>Effort:</strong> 2-3 weeks (with Claude Code) | <strong>Dependencies:</strong> DEV-BE-75 ‚úÖ (QA environment for testing) | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Complete subscription management system with Stripe integration, usage tracking, and automated billing.
</summary>

### DEV-BE-87: User Subscription & Payment Management
**Priority:** CRITICAL | **Effort:** 2-3 weeks (with Claude Code) | **Dependencies:** DEV-BE-75 ‚úÖ (QA environment for testing)

**Frontend Integration:**
This backend task is linked to **DEV-BE-009** in frontend roadmap:
- **Frontend Task:** DEV-BE-009: Integrate User Subscription Payment (Stripe)
- **Location:** `/Users/micky/WebstormProjects/PratikoAiWebApp/ARCHITECTURE_ROADMAP.md`
- **Coordination Required:** Backend APIs must be completed BEFORE frontend implementation

**Problem:**
No payment system implemented. Cannot accept paying customers or manage subscriptions.

**Solution:**
Implement complete subscription management system with Stripe integration.

**Payment Provider:** Stripe (2.9% + ‚Ç¨0.25 per transaction, excellent EU support, PSD2 compliant)

**Subscription Tiers:**

**1. Free Tier**
- 10 questions/month
- Basic responses
- No payment required

**2. Professional (‚Ç¨29/month or ‚Ç¨290/year)**
- 500 questions/month
- Priority responses
- Email support

**3. Business (‚Ç¨99/month or ‚Ç¨990/year)**
- Unlimited questions
- Fastest responses
- Phone + email support

**Implementation Tasks:**

**Week 1: Stripe Integration & Database Schema**
- [ ] Create Stripe account
- [ ] Add Stripe SDK: `pip install stripe`
- [ ] Create database tables (subscriptions, usage_tracking, payment_history)
- [ ] Create Alembic migration

**Week 2: Payment & Subscription APIs**
- [ ] Create `app/services/stripe_service.py`
- [ ] Create API endpoints in `app/api/v1/subscriptions.py`
- [ ] Create Stripe webhook endpoint
- [ ] Implement usage tracking middleware

**Week 3: Billing Reminders & Frontend Integration**
- [ ] Create `app/services/billing_reminder_service.py`
- [ ] Create background job for reminder emails
- [ ] Create email templates
- [ ] Write tests

**Acceptance Criteria:**
- ‚úÖ Stripe integration working (test mode)
- ‚úÖ Users can subscribe to Professional/Business plans
- ‚úÖ Usage tracking accurate
- ‚úÖ Rate limiting enforces subscription limits
- ‚úÖ Payment failures trigger email reminders
- ‚úÖ GDPR compliant (payment data via Stripe, not stored locally)

**Expected Revenue (Year 1):**
- 50 Professional subscribers: ‚Ç¨1,450/month
- 10 Business subscribers: ‚Ç¨990/month
- **Total: ‚Ç¨2,440/month** (~‚Ç¨29,280/year)

</details>

---

<details>
<summary>
<h3>DEV-BE-88: Deploy Preprod Environment (Hetzner VPS)</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 3-4 days (QA is template) | <strong>Dependencies:</strong> DEV-BE-75 ‚úÖ + DEV-BE-87 ‚úÖ | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Production-like environment for final testing before deploying to production.
</summary>

### DEV-BE-88: Deploy Preprod Environment (Hetzner VPS)
**Priority:** HIGH | **Effort:** 3-4 days (QA is template) | **Dependencies:** DEV-BE-75 ‚úÖ (QA deployment complete) + DEV-BE-87 ‚úÖ (payment system testable)

**Problem:**
Need production-like environment for final testing before deploying to production.

**Solution:**
Deploy complete PratikoAI backend to separate Hetzner VPS with production-like configuration.

**Implementation Tasks:**
- [ ] Provision Hetzner CX21 VPS for Preprod (2 vCPU, 4GB RAM)
- [ ] Configure firewall rules (same as QA)
- [ ] Install Docker and Docker Compose
- [ ] Create `.env.preprod` with production-like configuration
- [ ] Deploy stack
- [ ] Set up DNS: `api-preprod.pratikoai.com`
- [ ] Configure SSL with Let's Encrypt
- [ ] Set up automated backups

**Acceptance Criteria:**
- ‚úÖ Preprod environment accessible at `https://api-preprod.pratikoai.com`
- ‚úÖ All services running with production-like configuration
- ‚úÖ Stripe test mode working

**Infrastructure Cost (Preprod):**
- Hetzner CX21 VPS: ~$7/month
- Snapshots/backups: ~$1/month
- **Total: ~$8/month**

</details>

---

<details>
<summary>
<h3>DEV-BE-89: GDPR Compliance Audit (Preprod Environment)</h3>
<strong>Priority:</strong> HIGH | <strong>Effort:</strong> 2-3 days (QA audit is template) | <strong>Dependencies:</strong> DEV-BE-88 ‚úÖ + DEV-BE-74 ‚úÖ | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Validate GDPR compliance with production-like configuration and data before production launch.
</summary>

### DEV-BE-89: GDPR Compliance Audit (Preprod Environment)
**Priority:** HIGH | **Effort:** 2-3 days | **Dependencies:** DEV-BE-88 ‚úÖ (Preprod live) + DEV-BE-74 ‚úÖ (QA audit complete)

**Problem:**
Need to validate GDPR compliance with production-like configuration and data before production launch.

**Solution:**
Run same GDPR audit on Preprod environment with production-like data and configuration.

**Audit Focus Areas:**

**1. Production-Like Data Testing**
- [ ] Test data export with realistic data volume
- [ ] Test data deletion with production-like database size
- [ ] Validate performance of GDPR operations at scale

**2. Configuration Validation**
- [ ] Verify production log levels don't expose PII
- [ ] Test production monitoring doesn't log sensitive data

**3. Third-Party Integrations**
- [ ] Verify OpenAI API usage is GDPR compliant
- [ ] Verify Stripe payment data handling is GDPR compliant

**Acceptance Criteria:**
- ‚úÖ All GDPR features work at production scale
- ‚úÖ Payment data handling is GDPR compliant
- ‚úÖ Stakeholder approval obtained

</details>

---

<details>
<summary>
<h3>DEV-BE-90: Deploy Production Environment (Hetzner VPS)</h3>
<strong>Priority:</strong> CRITICAL | <strong>Effort:</strong> 1 week (with production hardening) | <strong>Dependencies:</strong> DEV-BE-88 ‚úÖ + DEV-BE-89 ‚úÖ | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Production environment for paying customers. Must be reliable, performant, and cost-effective.
</summary>

### DEV-BE-90: Deploy Production Environment (Hetzner VPS)
**Priority:** CRITICAL | **Effort:** 1 week (with production hardening) | **Dependencies:** DEV-BE-88 ‚úÖ (Preprod validated) + DEV-BE-89 ‚úÖ (Preprod GDPR audit passed)

**Problem:**
Need production environment for paying customers. Must be reliable, performant, and cost-effective.

**Solution:**
Deploy complete PratikoAI backend to Hetzner VPS with production configuration and enhanced resources.

**Implementation Tasks:**

**Week 1: VPS Setup**
- [ ] Provision Hetzner CX31 VPS (2 vCPU, 8GB RAM, 80GB SSD)
- [ ] Configure strict firewall rules
- [ ] Set up fail2ban for SSH brute force protection
- [ ] Configure automatic security updates

**Week 2: Deployment & Hardening**
- [ ] Create `.env.production` with secure configuration
- [ ] Deploy stack
- [ ] Set up DNS: `api.pratikoai.com`
- [ ] Configure SSL with Let's Encrypt (with auto-renewal)
- [ ] Set up automated daily backups
- [ ] Configure monitoring alerts
- [ ] Configure Stripe webhook endpoint with live keys

**Security Hardening:**
- [ ] Disable root SSH login
- [ ] Enable UFW firewall
- [ ] Configure fail2ban
- [ ] Enable PostgreSQL SSL connections
- [ ] Configure Redis password authentication

**Acceptance Criteria:**
- ‚úÖ Production environment accessible at `https://api.pratikoai.com`
- ‚úÖ SSL certificate valid and auto-renewing
- ‚úÖ All API endpoints responding with <100ms latency (p95)
- ‚úÖ Stripe live mode working
- ‚úÖ Security hardening complete
- ‚úÖ Zero downtime deployment process documented

**Infrastructure Cost (Production):**
- Hetzner CX31 VPS: ~$15/month
- Snapshots/backups: ~$2/month
- **Total: ~$17/month**

</details>

---

<details>
<summary>
<h3>DEV-BE-91: GDPR Compliance Audit (Production Environment)</h3>
<strong>Priority:</strong> CRITICAL | <strong>Effort:</strong> 4-5 days (requires legal review) | <strong>Dependencies:</strong> DEV-BE-90 ‚úÖ + DEV-BE-89 ‚úÖ | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Final GDPR compliance validation required before accepting real user data in production.
</summary>

### DEV-BE-91: GDPR Compliance Audit (Production Environment)
**Priority:** CRITICAL | **Effort:** 4-5 days (requires legal review) | **Dependencies:** DEV-BE-90 ‚úÖ (Production live) + DEV-BE-89 ‚úÖ (Preprod audit complete)

**Problem:**
Final GDPR compliance validation required before accepting real user data in production.

**Solution:**
Comprehensive production GDPR audit with security hardening and compliance documentation.

**Audit Activities:**

**1. Production GDPR Feature Validation**
- [ ] Test data export on production
- [ ] Test data deletion on production
- [ ] Test payment data export (Stripe + local subscription records)

**2. Security Audit**
- [ ] Test SSL/TLS configuration (A+ rating on SSL Labs)
- [ ] Verify firewall rules
- [ ] Test API authentication and rate limiting

**3. Data Protection Impact Assessment (DPIA)**
- [ ] Document all data processing activities
- [ ] Identify and assess privacy risks
- [ ] Get legal/compliance team sign-off

**4. Vendor Compliance**
- [ ] Verify Hetzner GDPR compliance (Data Processing Agreement)
- [ ] Validate OpenAI GDPR compliance
- [ ] Validate Stripe GDPR compliance (DPA signed)

**Documentation Deliverables:**
- [ ] `docs/compliance/GDPR_AUDIT_PRODUCTION.md`
- [ ] `docs/compliance/DATA_PROTECTION_IMPACT_ASSESSMENT.md`
- [ ] `docs/compliance/INCIDENT_RESPONSE_PLAN.md`
- [ ] Privacy Policy (published on website)
- [ ] Cookie Policy (published on website)

**Acceptance Criteria:**
- ‚úÖ All GDPR features functional on production
- ‚úÖ Security audit passed
- ‚úÖ GDPR compliance certification obtained
- ‚úÖ Legal/compliance team approval

</details>

---

## Q2 2025 (April - June)

<details>
<summary>
<h3>DEV-BE-79: Upgrade to HNSW Index</h3>
<strong>Priority:</strong> MEDIUM | <strong>Effort:</strong> 3-5 days (with Claude Code) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚ùå NOT STARTED<br>
HNSW provides 90-95% recall and 20-30% faster queries vs IVFFlat.
</summary>

### DEV-BE-79: Upgrade to HNSW Index
**Priority:** MEDIUM | **Effort:** 3-5 days (with Claude Code) | **Dependencies:** None

**Problem:**
Current IVFFlat index has 85-90% recall. HNSW (Hierarchical Navigable Small World) provides 90-95% recall and 20-30% faster queries.

**Implementation:**
```sql
-- Drop existing IVFFlat
DROP INDEX idx_kc_embedding_ivfflat_1536;

-- Create HNSW (requires pgvector 0.5.0+)
CREATE INDEX CONCURRENTLY idx_kc_embedding_hnsw_1536
ON knowledge_chunks
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

**Tasks:**
- [ ] Test HNSW build time on QA (expect 2-4 hours for 500K vectors)
- [ ] Benchmark query performance (HNSW vs IVFFlat)
- [ ] Plan production migration: Create index CONCURRENTLY during low-traffic window
- [ ] Document rollback procedure

**Acceptance Criteria:**
- ‚úÖ Vector search latency reduced by 20-30% (30-40ms ‚Üí 20-30ms)
- ‚úÖ Recall improved from 85-90% to 90-95%
- ‚úÖ Zero downtime during migration

</details>

---

<details>
<summary>
<h3>DEV-BE-80: Italian Financial Dictionary</h3>
<strong>Priority:</strong> LOW | <strong>Effort:</strong> 1 week (with Claude Code generating dictionary) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Custom Italian financial dictionary with synonym mappings for better FTS recall.
</summary>

### DEV-BE-80: Italian Financial Dictionary
**Priority:** LOW | **Effort:** 1 week (with Claude Code generating dictionary) | **Dependencies:** None

**Problem:**
PostgreSQL `italian` dictionary handles general Italian well, but misses domain-specific acronyms and synonyms common in tax/legal documents.

**Solution:**
Custom Italian financial dictionary with synonym mappings:

**Tasks:**
- [ ] Compile synonym list (100-200 terms):
  - "IVA" ‚Üí "imposta sul valore aggiunto"
  - "IRPEF" ‚Üí "imposta sul reddito delle persone fisiche"
  - "cedolare secca" ‚Üí "regime di tassazione sostitutiva"
- [ ] Create PostgreSQL synonym dictionary
- [ ] Update search configuration
- [ ] Test FTS recall improvement on domain queries

**Acceptance Criteria:**
- ‚úÖ +5-10% FTS recall on tax-specific queries
- ‚úÖ Better handling of Italian acronyms
- ‚úÖ Backward compatible

</details>

---

<details>
<summary>
<h3>DEV-BE-81: Expand Monitoring Dashboards</h3>
<strong>Priority:</strong> LOW | <strong>Effort:</strong> 2-3 days (with Claude Code) | <strong>Dependencies:</strong> DEV-BE-77 ‚úÖ | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Additional Grafana dashboards for document ingestion and user behavior metrics.
</summary>

### DEV-BE-81: Expand Monitoring Dashboards
**Priority:** LOW | **Effort:** 2-3 days (with Claude Code) | **Dependencies:** DEV-BE-77 ‚úÖ

**Tasks:**
- [ ] Add "Document Ingestion" dashboard
- [ ] Add "User Behavior" dashboard (query patterns, popular categories)
- [ ] Set up Grafana data source for PostgreSQL (direct DB queries)
- [ ] Create weekly email reports (Grafana Cloud feature)

</details>

---

## Backlog (Q3+ or Deferred)

<details>
<summary>
<h3>DEV-BE-82: LLM Fallback to Claude/Gemini</h3>
<strong>Priority:</strong> MEDIUM | <strong>Effort:</strong> 1 week (with Claude Code) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Multi-LLM fallback strategy to reduce single point of failure on OpenAI.
</summary>

### DEV-BE-82: LLM Fallback to Claude/Gemini
**Priority:** MEDIUM (reduces SPOF) | **Effort:** 1 week (with Claude Code) | **Dependencies:** None

**Problem:** If OpenAI API fails (outage, rate limits, account issues), entire RAG system stops working.

**Solution:** Multi-LLM fallback strategy:
1. Primary: OpenAI (gpt-4-turbo)
2. Fallback 1: Anthropic Claude (claude-3-sonnet)
3. Fallback 2: Google Gemini (gemini-1.5-pro)

**Tasks:**
- [ ] Create `app/services/llm_provider.py` with fallback logic
- [ ] Add Anthropic + Google API keys to config
- [ ] Implement retry logic with exponential backoff
- [ ] Add metrics: LLM provider used per request, fallback rate
- [ ] Test all three providers with identical prompts

**Acceptance Criteria:**
- ‚úÖ System stays online during OpenAI outage
- ‚úÖ Automatic fallback within 5 seconds
- ‚úÖ Quality comparable across providers

</details>

---

<details>
<summary>
<h3>DEV-BE-84: Multi-Tenancy Support</h3>
<strong>Priority:</strong> LOW | <strong>Effort:</strong> 3-4 weeks (with Claude Code) | <strong>Dependencies:</strong> None | <strong>Status:</strong> ‚ùå NOT STARTED<br>
Multi-tenant architecture for white-label deployment.
</summary>

### DEV-BE-84: Multi-Tenancy Support
**Priority:** LOW (only if white-label deployment needed) | **Effort:** 3-4 weeks (with Claude Code) | **Dependencies:** None

**Trigger:** White-label product requirement

</details>

---

## Not Approved Features

This section documents features that were fully designed in the architecture but were **intentionally simplified away** during the MVP implementation. The decision to implement DEV-BE-72 (Expert Feedback System) with a **SUPER_USER-only approach** eliminated the need for these three complex features.

**Last Updated:** 2024-11-24
**Decision Authority:** Michele Giannone

---

### 1. Expert Trust Score System (Graduated Validation)

**Original Architectural Design:**
A sophisticated 3-tier trust scoring system to validate expert feedback quality over time before granting auto-approval privileges.

**Why Not Implemented:**
The SUPER_USER role provides binary trust (trusted/not trusted), which is sufficient for MVP launch. The graduated system was designed for a marketplace scenario where external experts register with unknown credentials. Current reality: all experts are manually vetted before receiving SUPER_USER role.

**Technical Impact:**
- Simplified from 3 conditional branches to 1 boolean check
- Removed need for `trust_score` recalculation background jobs
- Eliminated admin notification system for level upgrades
- Saved 4-5 database queries per feedback submission

**Estimated Effort:** 3-4 days to implement if needed later.

---

### 2. Admin Dashboard for FAQ Approval Queue

**Original Architectural Design:**
A comprehensive admin interface for reviewing, approving, or rejecting FAQ candidates submitted by experts with trust scores below 0.9.

**Why Not Implemented:**
When experts have SUPER_USER role, they are pre-trusted by definition. Manual review would add unnecessary friction to trusted experts and create administrative overhead.

**Technical Impact:**
- Eliminated entire admin approval workflow (6 API endpoints not built)
- No `faq_approval_queue` table in database schema
- Feedback goes directly from submission ‚Üí Golden Set

**Estimated Effort:** 5-6 days to implement if needed later.

---

### 3. FAQ Obsolescence Tracking System

**Original Architectural Design:**
An automated system to detect when FAQs become outdated due to changes in source documents and flag them for expert review or automatic retirement.

**Why Not Implemented:**
This is a low-priority optimization for MVP. Small FAQ volume initially (<100 FAQs in first 3 months) makes manual review feasible.

**Estimated Effort:** 7-10 days to implement if needed later.

---

### Trade-offs Summary

| Feature | Complexity Avoided | When to Revisit |
|---------|-------------------|-----------------|
| **Trust Score System** | 3-tier validation logic, trust score recalculation jobs | External expert registration opens |
| **Admin Dashboard** | 6 API endpoints, full frontend dashboard, email notifications | Quality issues emerge with SUPER_USER FAQs |
| **Obsolescence Tracking** | Background jobs, staleness algorithm, 4 new DB fields | FAQ volume exceeds 500+ entries |

**Total Development Time Saved:** 15-20 days (3-4 weeks)

---

## Success Metrics

### Q1 2025 Targets

- [x] **Sprint 0 Complete:** Multi-agent system operational (DEV-BE-67) ‚úÖ
- [x] **Pinecone removed:** Zero Pinecone costs on billing (DEV-BE-68) ‚úÖ
- [x] **No emojis in responses:** Professional, formal tone (DEV-BE-71) ‚úÖ
- [x] **Test coverage 49%:** Pre-commit hooks passing (DEV-BE-92) ‚úÖ
- [x] **Expert feedback system:** Complete S113-S130 flow (DEV-BE-72) ‚úÖ
- [ ] **RSS feeds expanded:** 8+ sources configured (DEV-BE-69)
- [ ] **Daily RSS email reports:** Automated feed monitoring (DEV-BE-70)
- [ ] **QA environment deployed:** On Hetzner (DEV-BE-75)

---

## Decision Log

| Date | Decision | Rationale | Task |
|------|----------|-----------|------|
| 2024-11-24 | Simplify DEV-BE-72 to SUPER_USER-only | All experts manually vetted, trust scoring adds complexity without value for MVP | DEV-BE-72 |
| 2024-11-17 | Complete Sprint 0: Multi-Agent System | Establish subagent framework before major development work | DEV-BE-67 |
| 2024-11-14 | Remove Pinecone entirely | Over-engineered for current scale, pgvector sufficient | DEV-BE-68 |
| 2024-11-14 | Deploy to Hetzner (not AWS) | Cost: $33/month (all 3 envs) vs $330+/month AWS | DEV-BE-75, DEV-BE-88, DEV-BE-90 |

---

## Infrastructure Cost Summary

**All 3 Environments (Hetzner):**
- QA: $8/month (CX21 + backups)
- Preprod: $8/month (CX21 + backups)
- Production: $17/month (CX31 + backups)
- **Total: ~$33/month** for complete multi-environment setup

**vs AWS Alternative:**
- AWS QA alone: $110-135/month
- **Savings: ~$297-402/month** (90% cost reduction)

---

**Roadmap Maintained By:** Engineering Team
**Review Cycle:** Monthly sprint planning
**Next Review:** 2025-12-01
