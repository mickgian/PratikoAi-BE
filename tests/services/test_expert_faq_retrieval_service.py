"""Unit tests for ExpertFAQRetrievalService - Golden Set Retrieval Bug Detection.

RED PHASE: These tests FAIL to expose the bug where:
- Expert FAQs ARE saved to database when marked as "correct"
- But they are NEVER retrieved because Step 24 only has mock code
- LLM is called for identical questions instead of serving cached answers

Test Coverage:
1. Exact question matching with approved FAQs
2. Semantic similarity matching (similar but not identical questions)
3. Approval status filtering (only return approved FAQs)
4. Similarity threshold enforcement (reject low-similarity matches)
5. Embedding generation during FAQ creation
6. Signature-based exact lookup optimization

All 6 tests will FAIL because ExpertFAQRetrievalService doesn't exist yet.
"""

from uuid import uuid4

import numpy as np
import pytest
import pytest_asyncio
from sqlalchemy.ext.asyncio import AsyncSession

# Import will fail - service doesn't exist yet
try:
    from app.services.expert_faq_retrieval_service import ExpertFAQRetrievalService
except ImportError:
    ExpertFAQRetrievalService = None

from datetime import UTC

from app.models.faq_automation import FAQCandidate, QueryCluster


@pytest_asyncio.fixture
async def faq_retrieval_service(db_session: AsyncSession):
    """Create ExpertFAQRetrievalService instance.

    Expected to FAIL: Service doesn't exist yet.
    """
    if ExpertFAQRetrievalService is None:
        pytest.skip("ExpertFAQRetrievalService not implemented yet")
    return ExpertFAQRetrievalService(db_session)


@pytest_asyncio.fixture
async def insert_test_faq(db_session: AsyncSession):
    """Helper fixture to insert test FAQ with embedding.

    Returns callable that inserts FAQ with specified parameters.
    """

    async def _insert(
        question: str,
        answer: str,
        approval_status: str = "auto_approved",
        similarity_score: float = 0.95,
    ):
        """Insert a test FAQ candidate with mock embedding.

        Args:
            question: FAQ question text
            answer: FAQ answer text
            approval_status: Approval status (auto_approved, pending_review, etc.)
            similarity_score: Similarity score for testing

        Returns:
            FAQCandidate: Created FAQ candidate
        """
        from datetime import datetime, timezone
        from decimal import Decimal

        # Generate mock embedding (1536 dimensions for OpenAI ada-002)
        # In production, this would be generated by OpenAI API
        mock_embedding = np.random.rand(1536).tolist()

        # First create the required QueryCluster (FK constraint)
        cluster_id = uuid4()
        cluster = QueryCluster(
            id=cluster_id,
            canonical_query=question,
            normalized_form=question.lower(),
            query_count=10,
            first_seen=datetime.now(UTC),
            last_seen=datetime.now(UTC),
        )
        db_session.add(cluster)
        await db_session.flush()  # Flush to ensure cluster exists before FK reference

        # Create FAQ candidate with valid cluster_id
        faq = FAQCandidate(
            id=uuid4(),
            cluster_id=cluster_id,  # Use the cluster we just created
            suggested_question=question,
            best_response_content=answer,
            suggested_category="fiscale",
            frequency=10,
            estimated_monthly_savings=Decimal("15.50"),
            roi_score=Decimal("8.5"),
            priority_score=Decimal("85.0"),
            status=approval_status,
            generation_attempts=1,
            created_at=datetime.now(UTC),
        )

        db_session.add(faq)
        await db_session.commit()
        await db_session.refresh(faq)

        return faq

    return _insert


@pytest.mark.asyncio
@pytest.mark.skip(reason="RED phase tests - question_embedding column not yet implemented in FAQCandidate model")
class TestExpertFAQRetrievalService:
    """Unit tests for Expert FAQ retrieval service.

    All tests expected to FAIL (RED phase).
    NOTE: Skipped until question_embedding column is added to FAQCandidate model.
    """

    async def test_find_matching_faqs_exact_question_match(
        self,
        db_session: AsyncSession,
        insert_test_faq,
        faq_retrieval_service,
    ):
        """TEST 1: Verify exact question matching works.

        Setup:
            - Insert approved FAQ: "Cos'è l'IVA?" with answer "L'IVA è l'Imposta..."
            - Query with exact same question: "Cos'è l'IVA?"

        Assert:
            - FAQ is found
            - Similarity score >= 0.95 (exact match)
            - Answer matches stored answer
            - approval_status is "auto_approved"

        Expected: FAIL - ExpertFAQRetrievalService doesn't exist yet
        """
        # Arrange: Insert approved FAQ
        question = "Cos'è l'IVA?"
        answer = "L'IVA è l'Imposta sul Valore Aggiunto, una tassa sui consumi."

        inserted_faq = await insert_test_faq(
            question=question,
            answer=answer,
            approval_status="auto_approved",
        )

        # Act: Search for exact same question
        results = await faq_retrieval_service.find_matching_faqs(
            query=question,
            min_similarity=0.85,
            max_results=1,
        )

        # Assert: Exact match found
        assert len(results) == 1, "Expected exactly one match for exact question"

        match = results[0]
        assert match["faq_id"] == str(inserted_faq.id)
        assert match["similarity_score"] >= 0.95, "Exact match should have similarity >= 0.95"
        assert match["answer"] == answer
        assert match["approval_status"] == "auto_approved"
        assert match["question"] == question

    async def test_find_matching_faqs_semantic_similarity(
        self,
        db_session: AsyncSession,
        insert_test_faq,
        faq_retrieval_service,
    ):
        """TEST 2: Verify semantic matching works for similar questions.

        Setup:
            - Insert approved FAQ: "Come funziona l'IVA in Italia?"
            - Query with similar question: "Puoi spiegarmi il funzionamento dell'IVA italiana?"

        Assert:
            - FAQ is found despite different wording
            - Similarity score >= 0.85 (semantic match)
            - Correct answer is returned

        Expected: FAIL - Semantic search not implemented
        """
        # Arrange: Insert approved FAQ
        original_question = "Come funziona l'IVA in Italia?"
        answer = "L'IVA italiana funziona come imposta indiretta sui consumi..."

        await insert_test_faq(
            question=original_question,
            answer=answer,
            approval_status="auto_approved",
        )

        # Act: Search with semantically similar but different question
        similar_question = "Puoi spiegarmi il funzionamento dell'IVA italiana?"
        results = await faq_retrieval_service.find_matching_faqs(
            query=similar_question,
            min_similarity=0.85,
            max_results=1,
        )

        # Assert: Semantic match found
        assert len(results) >= 1, "Expected at least one semantic match"

        match = results[0]
        assert match["similarity_score"] >= 0.85, "Semantic match should have similarity >= 0.85"
        assert match["answer"] == answer
        assert match["approval_status"] == "auto_approved"
        # Question text will differ, but should be related
        assert "IVA" in match["question"]

    async def test_respects_approval_status_filter(
        self,
        db_session: AsyncSession,
        insert_test_faq,
        faq_retrieval_service,
    ):
        """TEST 3: Only approved FAQs should be returned.

        Setup:
            - Insert FAQ with status="pending_review": "Question A"
            - Insert FAQ with status="auto_approved": "Question B"
            - Query for "Question A"

        Assert:
            - Pending FAQ is NOT returned (filtered out)
            - Only approved FAQs are searchable

        Expected: FAIL - No approval status filtering
        """
        # Arrange: Insert FAQs with different approval statuses
        pending_question = "Come calcolo le detrazioni fiscali?"
        pending_answer = "Le detrazioni fiscali si calcolano..."

        approved_question = "Cos'è il modello 730?"
        approved_answer = "Il modello 730 è una dichiarazione..."

        pending_faq = await insert_test_faq(
            question=pending_question,
            answer=pending_answer,
            approval_status="pending_review",  # NOT APPROVED
        )

        approved_faq = await insert_test_faq(
            question=approved_question,
            answer=approved_answer,
            approval_status="auto_approved",  # APPROVED
        )

        # Act: Search for pending question
        results = await faq_retrieval_service.find_matching_faqs(
            query=pending_question,
            min_similarity=0.85,
            max_results=5,
        )

        # Assert: Pending FAQ should NOT be in results
        faq_ids_in_results = [r["faq_id"] for r in results]
        assert str(pending_faq.id) not in faq_ids_in_results, "Pending FAQs should not be returned in search results"

        # Act: Search for approved question
        results = await faq_retrieval_service.find_matching_faqs(
            query=approved_question,
            min_similarity=0.85,
            max_results=5,
        )

        # Assert: Approved FAQ should be in results
        faq_ids_in_results = [r["faq_id"] for r in results]
        assert str(approved_faq.id) in faq_ids_in_results, "Approved FAQs should be returned in search results"

    async def test_min_similarity_threshold_respected(
        self,
        db_session: AsyncSession,
        insert_test_faq,
        faq_retrieval_service,
    ):
        """TEST 4: Low-similarity matches should be rejected.

        Setup:
            - Insert approved FAQ: "Cos'è l'IVA?"
            - Query with unrelated question: "Come calcolo le tasse sugli immobili?"

        Assert:
            - No match returned (similarity < 0.85 threshold)
            - Returns empty list

        Expected: FAIL - No similarity checking
        """
        # Arrange: Insert FAQ about IVA
        iva_question = "Cos'è l'IVA?"
        iva_answer = "L'IVA è l'Imposta sul Valore Aggiunto..."

        await insert_test_faq(
            question=iva_question,
            answer=iva_answer,
            approval_status="auto_approved",
        )

        # Act: Search with completely unrelated question
        unrelated_question = "Come calcolo le tasse sugli immobili?"
        results = await faq_retrieval_service.find_matching_faqs(
            query=unrelated_question,
            min_similarity=0.85,
            max_results=5,
        )

        # Assert: No matches should be returned (low similarity)
        assert len(results) == 0, "Unrelated questions should return no matches (similarity < 0.85)"

    async def test_embedding_generation_for_faq_questions(
        self,
        db_session: AsyncSession,
        faq_retrieval_service,
    ):
        """TEST 5: Verify embeddings are generated when FAQ is created.

        Setup:
            - Create FAQ candidate with question "Test question"

        Assert:
            - question_embedding column is populated (or equivalent)
            - Embedding is a vector of length 1536 (OpenAI ada-002)
            - Embedding contains non-zero values

        Expected: FAIL - question_embedding column doesn't exist or isn't populated

        NOTE: This test may need to be updated based on how embeddings are stored.
        FAQCandidate model may not have question_embedding column yet.
        """
        # Arrange: Create FAQ with embedding generation
        from datetime import datetime, timezone
        from decimal import Decimal

        question = "Test question for embedding generation"
        answer = "Test answer for embedding verification"

        # First create the required QueryCluster (FK constraint)
        cluster_id = uuid4()
        cluster = QueryCluster(
            id=cluster_id,
            canonical_query=question,
            normalized_form=question.lower(),
            query_count=5,
            first_seen=datetime.now(UTC),
            last_seen=datetime.now(UTC),
        )
        db_session.add(cluster)
        await db_session.flush()

        # Act: Create FAQ (should trigger embedding generation)
        faq = FAQCandidate(
            id=uuid4(),
            cluster_id=cluster_id,  # Use the cluster we just created
            suggested_question=question,
            best_response_content=answer,
            suggested_category="test",
            frequency=5,
            estimated_monthly_savings=Decimal("10.00"),
            roi_score=Decimal("5.0"),
            priority_score=Decimal("50.0"),
            status="auto_approved",
            generation_attempts=1,
            created_at=datetime.now(UTC),
        )

        db_session.add(faq)
        await db_session.commit()
        await db_session.refresh(faq)

        # Assert: Check if embedding exists
        # NOTE: This will fail if question_embedding column doesn't exist
        # TODO: Update assertion based on actual column name
        assert hasattr(faq, "question_embedding"), "FAQCandidate should have question_embedding column"

        # If embedding column exists, verify it's populated
        if hasattr(faq, "question_embedding"):
            embedding = faq.question_embedding
            assert embedding is not None, "Embedding should be generated"
            assert len(embedding) == 1536, "Embedding should be 1536 dimensions (OpenAI ada-002)"
            assert any(val != 0 for val in embedding), "Embedding should contain non-zero values"

    async def test_get_by_signature_exact_lookup(
        self,
        db_session: AsyncSession,
        insert_test_faq,
        faq_retrieval_service,
    ):
        """TEST 6: Verify signature-based exact matching works.

        Setup:
            - Insert FAQ with computed query_signature
            - Query using same signature

        Assert:
            - FAQ is found instantly (no vector search needed)
            - Exact match performance <10ms

        Expected: FAIL - Signature lookup not implemented

        NOTE: query_signature is computed in Step 18 (orchestrators/preflight.py)
        as a hash of the normalized query. This test verifies the optimization
        path where identical queries can be matched without vector search.
        """
        # Arrange: Insert FAQ with query signature
        question = "Cos'è la risoluzione 62?"
        answer = "La risoluzione 62 è un documento..."

        # Compute query signature (same as Step 18)
        import hashlib

        query_signature = hashlib.sha256(question.lower().encode()).hexdigest()

        faq = await insert_test_faq(
            question=question,
            answer=answer,
            approval_status="auto_approved",
        )

        # Act: Lookup by signature (should be instant)
        import time

        start_time = time.perf_counter()

        result = await faq_retrieval_service.get_by_signature(query_signature=query_signature)

        elapsed_ms = (time.perf_counter() - start_time) * 1000

        # Assert: Exact match found quickly
        assert result is not None, "Signature lookup should find exact match"
        assert result["faq_id"] == str(faq.id)
        assert result["similarity_score"] == 1.0, "Signature match is exact (similarity = 1.0)"
        assert elapsed_ms < 10, f"Signature lookup should be <10ms (was {elapsed_ms:.2f}ms)"
        assert result["answer"] == answer


@pytest.mark.asyncio
@pytest.mark.skip(reason="Edge case tests have event loop issues in CI - skipped until db_session fixture is fixed")
class TestExpertFAQRetrievalServiceEdgeCases:
    """Edge case tests for FAQ retrieval service.

    Additional tests for error handling and edge cases.
    """

    async def test_empty_query_returns_empty_results(
        self,
        faq_retrieval_service,
    ):
        """Test that empty query returns empty results.

        Expected: FAIL - Service doesn't handle empty queries
        """
        if faq_retrieval_service is None:
            pytest.skip("ExpertFAQRetrievalService not implemented yet")

        results = await faq_retrieval_service.find_matching_faqs(
            query="",
            min_similarity=0.85,
            max_results=5,
        )

        assert len(results) == 0, "Empty query should return no results"

    async def test_max_results_limit_respected(
        self,
        db_session: AsyncSession,
        insert_test_faq,
        faq_retrieval_service,
    ):
        """Test that max_results parameter is respected.

        Expected: FAIL - Service doesn't limit results
        """
        if faq_retrieval_service is None:
            pytest.skip("ExpertFAQRetrievalService not implemented yet")

        # Insert 5 similar FAQs
        base_question = "Cos'è l'IVA"
        for i in range(5):
            await insert_test_faq(
                question=f"{base_question} {i}?",
                answer=f"Risposta {i}",
                approval_status="auto_approved",
            )

        # Request only 3 results
        results = await faq_retrieval_service.find_matching_faqs(
            query=base_question,
            min_similarity=0.80,
            max_results=3,
        )

        assert len(results) <= 3, "Should return at most max_results FAQs"
