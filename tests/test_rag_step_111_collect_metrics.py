#!/usr/bin/env python3
"""
Tests for RAG STEP 111 â€” Collect usage metrics

This step collects usage metrics for completed queries and aggregates system-wide metrics.
"""

from datetime import datetime, timedelta
from unittest.mock import MagicMock, patch, AsyncMock

import pytest

from app.orchestrators.metrics import step_111__collect_metrics
from app.services.metrics_service import Environment, MetricResult, MetricsReport, MetricStatus


class TestRAGStep111CollectMetrics:
    """Test suite for RAG STEP 111 - Collect usage metrics"""

    @pytest.mark.asyncio
    @patch('app.orchestrators.metrics.rag_step_log')
    @patch('app.core.logging.logger')
    @patch('app.services.metrics_service.MetricsService')
    @patch('app.services.usage_tracker.usage_tracker')
    async def test_step_111_collect_successful_metrics(self, mock_usage_tracker, mock_metrics_service_class, mock_logger, mock_rag_log):
        """Test Step 111: Successful metrics collection"""

        # Mock user metrics
        mock_user_metrics = MagicMock()
        mock_user_metrics.total_requests = 25
        mock_user_metrics.total_cost_eur = 0.15
        mock_user_metrics.cache_hit_rate = 0.75
        mock_usage_tracker.get_user_metrics = AsyncMock(return_value=mock_user_metrics)

        # Mock system metrics
        mock_system_metrics = MagicMock()
        mock_system_metrics.total_requests = 1500
        mock_system_metrics.avg_response_time_ms = 320.5
        mock_system_metrics.error_rate = 0.02
        mock_usage_tracker.get_system_metrics = AsyncMock(return_value=mock_system_metrics)

        # Mock metrics report
        mock_metrics_report = MetricsReport(
            environment=Environment.DEVELOPMENT,
            timestamp=datetime.utcnow(),
            technical_metrics=[
                MetricResult(
                    name="API Response Time",
                    value=320.5,
                    target=500.0,
                    status=MetricStatus.PASS,
                    unit="ms",
                    description="API response time",
                    timestamp=datetime.utcnow(),
                    environment=Environment.DEVELOPMENT
                )
            ],
            business_metrics=[],
            overall_health_score=0.92,
            alerts=[],
            recommendations=[]
        )

        mock_metrics_service = MagicMock()
        mock_metrics_service.generate_metrics_report = AsyncMock(return_value=mock_metrics_report)
        mock_metrics_service_class.return_value = mock_metrics_service

        ctx = {
            'user_id': 'user_123',
            'session_id': 'session_456',
            'response_time_ms': 320,
            'cache_hit': True,
            'provider': 'openai',
            'model': 'gpt-4',
            'total_tokens': 150,
            'cost': 0.003,
            'environment': 'development'
        }

        # Call the orchestrator function
        result = await step_111__collect_metrics(ctx=ctx)

        # Verify the result structure
        assert isinstance(result, dict)
        assert result['metrics_collected'] is True
        assert result['user_id'] == 'user_123'
        assert result['session_id'] == 'session_456'
        assert result['environment'] == 'development'
        assert result['user_metrics_available'] is True
        assert result['system_metrics_available'] is True
        assert result['metrics_report_available'] is True
        assert result['health_score'] == 0.92
        assert result['alerts_count'] == 0
        assert 'timestamp' in result

        # Verify user metrics summary
        assert 'user_metrics_summary' in result
        assert result['user_metrics_summary']['total_requests'] == 25
        assert result['user_metrics_summary']['total_cost_eur'] == 0.15
        assert result['user_metrics_summary']['cache_hit_rate'] == 0.75

        # Verify system metrics summary
        assert 'system_metrics_summary' in result
        assert result['system_metrics_summary']['total_requests'] == 1500
        assert result['system_metrics_summary']['avg_response_time_ms'] == 320.5
        assert result['system_metrics_summary']['error_rate'] == 0.02

        # Verify services were called correctly
        mock_usage_tracker.get_user_metrics.assert_called_once()
        user_call_args = mock_usage_tracker.get_user_metrics.call_args
        assert user_call_args[1]['user_id'] == 'user_123'
        assert isinstance(user_call_args[1]['start_date'], datetime)
        assert isinstance(user_call_args[1]['end_date'], datetime)

        mock_usage_tracker.get_system_metrics.assert_called_once()
        system_call_args = mock_usage_tracker.get_system_metrics.call_args
        assert isinstance(system_call_args[1]['start_date'], datetime)
        assert isinstance(system_call_args[1]['end_date'], datetime)

        mock_metrics_service.generate_metrics_report.assert_called_once_with(Environment.DEVELOPMENT)

        # Verify logging was called
        mock_logger.info.assert_called_once()
        log_call = mock_logger.info.call_args
        assert 'Metrics collected successfully' in log_call[0][0]
        assert log_call[1]['extra']['metrics_event'] == 'collection_successful'

    @pytest.mark.asyncio
    @patch('app.orchestrators.metrics.rag_step_log')
    @patch('app.core.logging.logger')
    @patch('app.services.metrics_service.MetricsService')
    @patch('app.services.usage_tracker.usage_tracker')
    async def test_step_111_collect_without_user_id(self, mock_usage_tracker, mock_metrics_service_class, mock_logger, mock_rag_log):
        """Test Step 111: Metrics collection without user ID"""

        # Mock system metrics only (no user metrics)
        mock_system_metrics = MagicMock()
        mock_system_metrics.total_requests = 500
        mock_system_metrics.avg_response_time_ms = 280.0
        mock_system_metrics.error_rate = 0.01
        mock_usage_tracker.get_system_metrics = AsyncMock(return_value=mock_system_metrics)

        # Mock metrics report
        mock_metrics_report = MagicMock()
        mock_metrics_report.overall_health_score = 0.88
        mock_metrics_report.alerts = ['Warning: High response time']

        mock_metrics_service = MagicMock()
        mock_metrics_service.generate_metrics_report = AsyncMock(return_value=mock_metrics_report)
        mock_metrics_service_class.return_value = mock_metrics_service

        ctx = {
            'session_id': 'session_789',
            'response_time_ms': 450,
            'cache_hit': False,
            'environment': 'production'
        }

        result = await step_111__collect_metrics(ctx=ctx)

        assert result['metrics_collected'] is True
        assert result['user_id'] is None
        assert result['user_metrics_available'] is False
        assert result['system_metrics_available'] is True
        assert result['metrics_report_available'] is True
        assert result['health_score'] == 0.88
        assert result['alerts_count'] == 1

        # Should not have user metrics summary
        assert 'user_metrics_summary' not in result

        # Should have system metrics summary
        assert 'system_metrics_summary' in result
        assert result['system_metrics_summary']['total_requests'] == 500

        # Verify user metrics not called
        mock_usage_tracker.get_user_metrics.assert_not_called()
        # System metrics should still be called
        mock_usage_tracker.get_system_metrics.assert_called_once()

    @pytest.mark.asyncio
    @patch('app.orchestrators.metrics.rag_step_log')
    @patch('app.core.logging.logger')
    @patch('app.services.metrics_service.MetricsService')
    @patch('app.services.usage_tracker.usage_tracker')
    async def test_step_111_environment_handling(self, mock_usage_tracker, mock_metrics_service_class, mock_logger, mock_rag_log):
        """Test Step 111: Different environment handling"""

        mock_usage_tracker.get_system_metrics = AsyncMock(return_value=MagicMock())
        mock_metrics_service = MagicMock()
        mock_metrics_service.generate_metrics_report = AsyncMock(return_value=MagicMock())
        mock_metrics_service_class.return_value = mock_metrics_service

        # Test production environment
        result = await step_111__collect_metrics(environment='production')
        mock_metrics_service.generate_metrics_report.assert_called_with(Environment.PRODUCTION)

        # Test staging environment
        result = await step_111__collect_metrics(environment='staging')
        mock_metrics_service.generate_metrics_report.assert_called_with(Environment.STAGING)

        # Test default to development
        result = await step_111__collect_metrics(environment='unknown')
        mock_metrics_service.generate_metrics_report.assert_called_with(Environment.DEVELOPMENT)

    @pytest.mark.asyncio
    @patch('app.orchestrators.metrics.rag_step_log')
    @patch('app.core.logging.logger')
    @patch('app.services.metrics_service.MetricsService')
    @patch('app.services.usage_tracker.usage_tracker')
    async def test_step_111_usage_tracker_error(self, mock_usage_tracker, mock_metrics_service_class, mock_logger, mock_rag_log):
        """Test Step 111: Handle usage tracker service error"""

        mock_usage_tracker.get_user_metrics = AsyncMock(side_effect=Exception("Database connection error"))
        mock_usage_tracker.get_system_metrics = AsyncMock(side_effect=Exception("Database connection error"))

        ctx = {
            'user_id': 'user_123',
            'environment': 'development'
        }

        result = await step_111__collect_metrics(ctx=ctx)

        # Should return error result
        assert result['metrics_collected'] is False
        assert result['error'] == 'Database connection error'
        assert result['user_metrics_available'] is False
        assert result['system_metrics_available'] is False

        # Verify error was logged
        mock_logger.error.assert_called_once()
        error_call = mock_logger.error.call_args
        assert 'Metrics collection failed' in error_call[0][0]

    @pytest.mark.asyncio
    @patch('app.orchestrators.metrics.rag_step_log')
    @patch('app.core.logging.logger')
    @patch('app.services.metrics_service.MetricsService')
    @patch('app.services.usage_tracker.usage_tracker')
    async def test_step_111_metrics_service_error(self, mock_usage_tracker, mock_metrics_service_class, mock_logger, mock_rag_log):
        """Test Step 111: Handle metrics service error"""

        mock_usage_tracker.get_system_metrics = AsyncMock(return_value=MagicMock())

        mock_metrics_service = MagicMock()
        mock_metrics_service.generate_metrics_report = AsyncMock(side_effect=Exception("Metrics service error"))
        mock_metrics_service_class.return_value = mock_metrics_service

        ctx = {'environment': 'development'}

        result = await step_111__collect_metrics(ctx=ctx)

        assert result['metrics_collected'] is False
        assert result['error'] == 'Metrics service error'

    @pytest.mark.asyncio
    @patch('app.orchestrators.metrics.rag_step_log')
    @patch('app.core.logging.logger')
    @patch('app.services.metrics_service.MetricsService')
    @patch('app.services.usage_tracker.usage_tracker')
    async def test_step_111_kwargs_parameters(self, mock_usage_tracker, mock_metrics_service_class, mock_logger, mock_rag_log):
        """Test Step 111: Parameters passed via kwargs"""

        mock_usage_tracker.get_system_metrics = AsyncMock(return_value=MagicMock())
        mock_metrics_service = MagicMock()
        mock_metrics_service.generate_metrics_report = AsyncMock(return_value=MagicMock())
        mock_metrics_service_class.return_value = mock_metrics_service

        # Call with kwargs instead of ctx
        result = await step_111__collect_metrics(
            user_id='user_456',
            session_id='session_789',
            response_time_ms=200,
            cache_hit=True,
            environment='staging'
        )

        # Verify kwargs are processed correctly
        assert result['user_id'] == 'user_456'
        assert result['session_id'] == 'session_789'
        assert result['response_time_ms'] == 200
        assert result['cache_hit'] is True
        assert result['environment'] == 'staging'

    @pytest.mark.asyncio
    @patch('app.orchestrators.metrics.rag_step_log')
    @patch('app.core.logging.logger')
    @patch('app.services.metrics_service.MetricsService')
    @patch('app.services.usage_tracker.usage_tracker')
    async def test_step_111_performance_tracking(self, mock_usage_tracker, mock_metrics_service_class, mock_logger, mock_rag_log):
        """Test Step 111: Performance tracking with timer"""

        with patch('app.orchestrators.metrics.rag_step_timer') as mock_timer:
            # Mock the timer context manager
            mock_timer.return_value.__enter__ = MagicMock()
            mock_timer.return_value.__exit__ = MagicMock()

            mock_usage_tracker.get_system_metrics = AsyncMock(return_value=MagicMock())
            mock_metrics_service = MagicMock()
            mock_metrics_service.generate_metrics_report = AsyncMock(return_value=MagicMock())
            mock_metrics_service_class.return_value = mock_metrics_service

            # Call the orchestrator function
            await step_111__collect_metrics(ctx={'environment': 'development'})

            # Verify timer was used
            mock_timer.assert_called_with(
                111,
                'RAG.metrics.collect.usage.metrics',
                'CollectMetrics',
                stage='start'
            )

    @pytest.mark.asyncio
    @patch('app.orchestrators.metrics.rag_step_log')
    @patch('app.core.logging.logger')
    @patch('app.services.metrics_service.MetricsService')
    @patch('app.services.usage_tracker.usage_tracker')
    async def test_step_111_comprehensive_logging_format(self, mock_usage_tracker, mock_metrics_service_class, mock_logger, mock_rag_log):
        """Test Step 111: Verify comprehensive logging format"""

        mock_usage_tracker.get_user_metrics = AsyncMock(return_value=MagicMock())
        mock_usage_tracker.get_system_metrics = AsyncMock(return_value=MagicMock())
        mock_metrics_service = MagicMock()
        mock_metrics_service.generate_metrics_report = AsyncMock(return_value=MagicMock())
        mock_metrics_service_class.return_value = mock_metrics_service

        ctx = {
            'user_id': 'user_123',
            'environment': 'development'
        }

        # Call the orchestrator function
        await step_111__collect_metrics(ctx=ctx)

        # Verify RAG step logging format
        completed_logs = [
            call for call in mock_rag_log.call_args_list
            if call[1].get('processing_stage') == 'completed'
        ]

        assert len(completed_logs) > 0
        log_call = completed_logs[0]

        # Check required fields
        required_fields = [
            'step', 'step_id', 'node_label', 'metrics_event',
            'metrics_collected', 'user_id', 'environment',
            'user_metrics_available', 'system_metrics_available',
            'metrics_report_available', 'processing_stage'
        ]

        for field in required_fields:
            assert field in log_call[1], f"Missing required field: {field}"

        # Verify specific values
        assert log_call[1]['step'] == 111
        assert log_call[1]['step_id'] == 'RAG.metrics.collect.usage.metrics'
        assert log_call[1]['node_label'] == 'CollectMetrics'
        assert log_call[1]['metrics_event'] == 'collection_successful'

    @pytest.mark.asyncio
    @patch('app.orchestrators.metrics.rag_step_log')
    @patch('app.core.logging.logger')
    @patch('app.services.metrics_service.MetricsService')
    @patch('app.services.usage_tracker.usage_tracker')
    async def test_step_111_metrics_data_structure(self, mock_usage_tracker, mock_metrics_service_class, mock_logger, mock_rag_log):
        """Test Step 111: Verify metrics data structure"""

        mock_usage_tracker.get_user_metrics = AsyncMock(return_value=MagicMock())
        mock_usage_tracker.get_system_metrics = AsyncMock(return_value=MagicMock())
        mock_metrics_service = MagicMock()
        mock_metrics_service.generate_metrics_report = AsyncMock(return_value=MagicMock())
        mock_metrics_service_class.return_value = mock_metrics_service

        ctx = {
            'user_id': 'user_123',
            'session_id': 'session_456',
            'environment': 'development'
        }

        # Call the orchestrator function
        result = await step_111__collect_metrics(ctx=ctx)

        # Verify all expected fields in result
        expected_fields = [
            'timestamp', 'metrics_collected', 'user_id', 'session_id',
            'response_time_ms', 'cache_hit', 'provider', 'model',
            'total_tokens', 'cost', 'environment', 'user_metrics_available',
            'system_metrics_available', 'metrics_report_available', 'error'
        ]

        for field in expected_fields:
            assert field in result, f"Missing field in metrics data: {field}"

        # Verify data types
        assert isinstance(result['timestamp'], str)
        assert isinstance(result['metrics_collected'], bool)
        assert isinstance(result['user_id'], str) or result['user_id'] is None
        assert isinstance(result['session_id'], str) or result['session_id'] is None
        assert isinstance(result['response_time_ms'], int) or result['response_time_ms'] is None
        assert isinstance(result['cache_hit'], bool)
        assert isinstance(result['environment'], str)
        assert isinstance(result['user_metrics_available'], bool)
        assert isinstance(result['system_metrics_available'], bool)
        assert isinstance(result['metrics_report_available'], bool)

        # Verify timestamp format (ISO format)
        datetime.fromisoformat(result['timestamp'].replace('Z', '+00:00'))

    @pytest.mark.asyncio
    @patch('app.orchestrators.metrics.rag_step_log')
    @patch('app.core.logging.logger')
    @patch('app.services.metrics_service.MetricsService')
    @patch('app.services.usage_tracker.usage_tracker')
    async def test_step_111_partial_failures(self, mock_usage_tracker, mock_metrics_service_class, mock_logger, mock_rag_log):
        """Test Step 111: Handle partial service failures gracefully"""

        # User metrics succeeds, system metrics fails
        mock_user_metrics = MagicMock()
        mock_usage_tracker.get_user_metrics = AsyncMock(return_value=mock_user_metrics)
        mock_usage_tracker.get_system_metrics = AsyncMock(side_effect=Exception("System metrics error"))

        ctx = {
            'user_id': 'user_123',
            'environment': 'development'
        }

        result = await step_111__collect_metrics(ctx=ctx)

        # Should fail overall due to exception
        assert result['metrics_collected'] is False
        assert result['error'] == 'System metrics error'